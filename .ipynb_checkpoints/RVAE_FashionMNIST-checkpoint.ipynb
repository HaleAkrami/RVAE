{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply RVAE for FashionMNIST data set\n",
    "* <b>Objective:</b> In this problem, the purpose is trian a robust varational autoencoder when the training is polluted with outliers. Here we chose shoes and sneakers as inliers classes and samples from other categories as outliers. Since these images contain a significant range of gray scales, we chose the Gaussian model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import math\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import fashion_mnist\n",
    "import scipy.io as spio\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters\n",
    "seed:random seed \n",
    "epochs:number of epochs\n",
    "CODE_SIZE: z dimention\n",
    "SIGMA:constant variance for Guassian loss function\n",
    "batch_size:batch size for training\n",
    "log_interval:how many batches to wait before logging training status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10004\n",
    "epochs = 150 \n",
    "batch_size = 120\n",
    "log_interval = 10\n",
    "CODE_SIZE = 20\n",
    "SIGMA = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creat data for test loader and train loader\n",
    "input: anomoly percentage\n",
    "output: train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(frac_anom):\n",
    "\n",
    "    torch.manual_seed(seed=seed)\n",
    "    np.random.seed(seed=seed)\n",
    "\n",
    "    (X, X_lab), (_test_images, _test_lab) = fashion_mnist.load_data()\n",
    "    X_lab = np.array(X_lab)\n",
    "\n",
    "    # find other categories\n",
    "    ind = np.isin(X_lab, (0, 1, 2, 3, 4, 5, 6, 8))  #(1, 5, 7, 9)\n",
    "    X_lab_outliers = X_lab[ind]\n",
    "    X_outliers = X[ind]\n",
    "\n",
    "    # find sneaker and ankle boots\n",
    "    ind = np.isin(X_lab, (7, 9))  # (0, 2, 3, 4, 6))  #\n",
    "    X_lab = X_lab[ind]\n",
    "    X = X[ind]\n",
    "\n",
    "    #normalize the data\n",
    "    X = X / 255.0\n",
    "    X_outliers = X_outliers / 255.0\n",
    "\n",
    "    # add ouliers to the data the label for outliers is 10\n",
    "    Nsamp = np.int(np.rint(len(X) * frac_anom)) + 1\n",
    "    X[:Nsamp, :, :] = X_outliers[:Nsamp, :, :]\n",
    "    X_lab[:Nsamp] = 10\n",
    "\n",
    "    #split data to train and test\n",
    "    X_train, X_test, X_lab_train, X_lab_test = train_test_split(\n",
    "        X, X_lab, test_size=0.33, random_state=10003)\n",
    "    X_train = np.expand_dims(X_train, axis=1)\n",
    "    X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "\n",
    "    \n",
    "    #append samples and labels\n",
    "    train_data = []\n",
    "    for i in range(len(X_train)):\n",
    "        train_data.append(\n",
    "            [torch.from_numpy(X_train[i]).float(), X_lab_train[i]])\n",
    "\n",
    "    test_data = []\n",
    "    for i in range(len(X_test)):\n",
    "        test_data.append(\n",
    "            [torch.from_numpy(X_test[i]).float(), X_lab_test[i]])\n",
    "        \n",
    "        \n",
    "    #generate train loader and test loader\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                              batch_size=len(test_data),\n",
    "                                              shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define MSE loss and beta loss for Guassian posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE loss\n",
    "def MSE_loss(Y, X):\n",
    "    ret = (X - Y)**2\n",
    "    ret = torch.sum(ret,1)\n",
    "    return ret\n",
    "\n",
    "#beta loss\n",
    "def Gaussian_CE_loss(Y, X, beta, sigma=SIGMA):  # 784 for mnist\n",
    "    Dim = Y.shape[1]\n",
    "    const1 = -((1 + beta) / beta)\n",
    "    const2 = 1 / pow((2 * math.pi * (sigma**2)), (beta * Dim / 2))\n",
    "    MSE = MSE_loss(Y, X)\n",
    "    term1 = torch.exp(-(beta / (2 * (sigma**2))) * MSE)\n",
    "    loss = torch.sum(const1 * (const2* term1 - 1))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def beta_loss_function(recon_x, x, mu, logvar, beta):\n",
    "\n",
    "    if beta > 0:\n",
    "        # If beta is nonzero, use the beta entropy\n",
    "        BBCE = Gaussian_CE_loss(recon_x.view(-1, 784), x.view(-1, 784), beta)\n",
    "    else:\n",
    "        # if beta is zero use binary cross entropy\n",
    "        BBCE = torch.sum(MSE_loss(recon_x.view(-1, 784), x.view(-1, 784)))\n",
    "\n",
    "    # compute KL divergence\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BBCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RVAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, CODE_SIZE)\n",
    "        self.fc22 = nn.Linear(400, CODE_SIZE)\n",
    "        self.fc3 = nn.Linear(CODE_SIZE, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "    \n",
    "    # for reseting network weights\n",
    "    def weight_reset(self):\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                m.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model reset\n",
    "This function calls weight_reset from the network class and reset the weights of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_reset():\n",
    "    model.weight_reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RVAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function\n",
    "input: number of epochs, value of beta\n",
    "\n",
    "Prints loss after each log intervalof bathces and after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, beta_val):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, data_lab) in enumerate(train_loader):\n",
    "   \n",
    "        data = (data).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = beta_loss_function(recon_batch, data, mu, logvar, beta=beta_val)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing function\n",
    "input: number of epochs, value of beta\n",
    "out put: total loss for the test samples, loss for the inlier samples in the test, loss for the out lier samples in the test\n",
    "Saves the resconstruction of 8 random samples(4 inliers, 4 outliers)\n",
    "Saves data, reconstruction and anomolies lables in a npz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(frac_anom, beta_val):\n",
    "    model.eval()\n",
    "    test_loss_total = 0\n",
    "    test_loss_anom = 0\n",
    "    num_anom = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, data_lab) in enumerate(test_loader):\n",
    "        \n",
    "            data = (data).to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            anom_lab = data_lab == 10\n",
    "            num_anom += np.sum(anom_lab.numpy())  # count number of anomalies\n",
    "            anom_lab = (anom_lab[:, None].float()).to(device)\n",
    "\n",
    "            test_loss_anom += torch.sum(MSE_loss(recon_batch * anom_lab,\n",
    "                                      data * anom_lab)).item()\n",
    "            test_loss_total += torch.sum(MSE_loss(recon_batch, data)).item()\n",
    "\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 100)\n",
    "                samp=[4, 14, 50, 60, 25, 29, 32, 65]\n",
    "                comparison = torch.cat([\n",
    "                    data.view(len(recon_batch), 1, 28, 28)[samp],\n",
    "                    recon_batch.view(len(recon_batch), 1, 28, 28)[samp]\n",
    "                ])\n",
    "                save_image(comparison.cpu(),\n",
    "                           'results/fashion_mnist_recon_shallow_' +\n",
    "                           str(beta_val) + '_' + str(frac_anom) + '.png',\n",
    "                           nrow=n)\n",
    "\n",
    "        np.savez('results/fashion_mnist_' + str(beta_val) + '_' +\n",
    "                 str(frac_anom) + '.npz',\n",
    "                 recon=recon_batch.cpu(),\n",
    "                 data=data.cpu(),\n",
    "                 anom_lab=anom_lab.cpu())\n",
    "\n",
    "    test_loss_normals = (test_loss_total - test_loss_anom) / (\n",
    "        len(test_loader.dataset) - num_anom)\n",
    "    test_loss_anom /= num_anom\n",
    "    test_loss_total /= len(test_loader.dataset)\n",
    "\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss_total))\n",
    "\n",
    "    return test_loss_total, test_loss_anom, test_loss_normals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function\n",
    "\n",
    "Runs training and testing for a givern values of beta and percentage of anomolies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/8040 (0%)]\tLoss: 148.044954\n",
      "Train Epoch: 1 [1200/8040 (15%)]\tLoss: 45.567753\n",
      "Train Epoch: 1 [2400/8040 (30%)]\tLoss: 37.132817\n",
      "Train Epoch: 1 [3600/8040 (45%)]\tLoss: 33.979333\n",
      "Train Epoch: 1 [4800/8040 (60%)]\tLoss: 31.240161\n",
      "Train Epoch: 1 [6000/8040 (75%)]\tLoss: 31.424642\n",
      "Train Epoch: 1 [7200/8040 (90%)]\tLoss: 29.571454\n",
      "====> Epoch: 1 Average loss: 39.2259\n",
      "epoch: 1, beta=0, frac_anom=0.01\n",
      "Train Epoch: 2 [0/8040 (0%)]\tLoss: 29.706140\n",
      "Train Epoch: 2 [1200/8040 (15%)]\tLoss: 29.206207\n",
      "Train Epoch: 2 [2400/8040 (30%)]\tLoss: 30.272343\n",
      "Train Epoch: 2 [3600/8040 (45%)]\tLoss: 26.317039\n",
      "Train Epoch: 2 [4800/8040 (60%)]\tLoss: 29.363403\n",
      "Train Epoch: 2 [6000/8040 (75%)]\tLoss: 27.558669\n",
      "Train Epoch: 2 [7200/8040 (90%)]\tLoss: 28.041359\n",
      "====> Epoch: 2 Average loss: 28.3701\n",
      "epoch: 2, beta=0, frac_anom=0.01\n",
      "Train Epoch: 3 [0/8040 (0%)]\tLoss: 29.901971\n",
      "Train Epoch: 3 [1200/8040 (15%)]\tLoss: 28.600098\n",
      "Train Epoch: 3 [2400/8040 (30%)]\tLoss: 26.931057\n",
      "Train Epoch: 3 [3600/8040 (45%)]\tLoss: 27.563391\n",
      "Train Epoch: 3 [4800/8040 (60%)]\tLoss: 28.141516\n",
      "Train Epoch: 3 [6000/8040 (75%)]\tLoss: 26.036151\n",
      "Train Epoch: 3 [7200/8040 (90%)]\tLoss: 25.483167\n",
      "====> Epoch: 3 Average loss: 26.8836\n",
      "epoch: 3, beta=0, frac_anom=0.01\n",
      "Train Epoch: 4 [0/8040 (0%)]\tLoss: 25.813501\n",
      "Train Epoch: 4 [1200/8040 (15%)]\tLoss: 23.153276\n",
      "Train Epoch: 4 [2400/8040 (30%)]\tLoss: 25.519562\n",
      "Train Epoch: 4 [3600/8040 (45%)]\tLoss: 23.488961\n",
      "Train Epoch: 4 [4800/8040 (60%)]\tLoss: 24.967743\n",
      "Train Epoch: 4 [6000/8040 (75%)]\tLoss: 27.172249\n",
      "Train Epoch: 4 [7200/8040 (90%)]\tLoss: 22.722292\n",
      "====> Epoch: 4 Average loss: 25.4160\n",
      "epoch: 4, beta=0, frac_anom=0.01\n",
      "Train Epoch: 5 [0/8040 (0%)]\tLoss: 24.693461\n",
      "Train Epoch: 5 [1200/8040 (15%)]\tLoss: 23.181486\n",
      "Train Epoch: 5 [2400/8040 (30%)]\tLoss: 23.567814\n",
      "Train Epoch: 5 [3600/8040 (45%)]\tLoss: 25.262392\n",
      "Train Epoch: 5 [4800/8040 (60%)]\tLoss: 26.414945\n",
      "Train Epoch: 5 [6000/8040 (75%)]\tLoss: 25.083653\n",
      "Train Epoch: 5 [7200/8040 (90%)]\tLoss: 24.266536\n",
      "====> Epoch: 5 Average loss: 24.3417\n",
      "epoch: 5, beta=0, frac_anom=0.01\n",
      "Train Epoch: 6 [0/8040 (0%)]\tLoss: 23.017739\n",
      "Train Epoch: 6 [1200/8040 (15%)]\tLoss: 22.891711\n",
      "Train Epoch: 6 [2400/8040 (30%)]\tLoss: 24.141064\n",
      "Train Epoch: 6 [3600/8040 (45%)]\tLoss: 22.988656\n",
      "Train Epoch: 6 [4800/8040 (60%)]\tLoss: 21.742013\n",
      "Train Epoch: 6 [6000/8040 (75%)]\tLoss: 23.205037\n",
      "Train Epoch: 6 [7200/8040 (90%)]\tLoss: 23.599961\n",
      "====> Epoch: 6 Average loss: 23.6293\n",
      "epoch: 6, beta=0, frac_anom=0.01\n",
      "Train Epoch: 7 [0/8040 (0%)]\tLoss: 23.144535\n",
      "Train Epoch: 7 [1200/8040 (15%)]\tLoss: 22.267983\n",
      "Train Epoch: 7 [2400/8040 (30%)]\tLoss: 25.448474\n",
      "Train Epoch: 7 [3600/8040 (45%)]\tLoss: 23.985050\n",
      "Train Epoch: 7 [4800/8040 (60%)]\tLoss: 23.796891\n",
      "Train Epoch: 7 [6000/8040 (75%)]\tLoss: 24.472174\n",
      "Train Epoch: 7 [7200/8040 (90%)]\tLoss: 22.817749\n",
      "====> Epoch: 7 Average loss: 23.0973\n",
      "epoch: 7, beta=0, frac_anom=0.01\n",
      "Train Epoch: 8 [0/8040 (0%)]\tLoss: 21.202091\n",
      "Train Epoch: 8 [1200/8040 (15%)]\tLoss: 22.421665\n",
      "Train Epoch: 8 [2400/8040 (30%)]\tLoss: 25.025956\n",
      "Train Epoch: 8 [3600/8040 (45%)]\tLoss: 24.771364\n",
      "Train Epoch: 8 [4800/8040 (60%)]\tLoss: 22.198712\n",
      "Train Epoch: 8 [6000/8040 (75%)]\tLoss: 22.725586\n",
      "Train Epoch: 8 [7200/8040 (90%)]\tLoss: 21.895093\n",
      "====> Epoch: 8 Average loss: 22.6937\n",
      "epoch: 8, beta=0, frac_anom=0.01\n",
      "Train Epoch: 9 [0/8040 (0%)]\tLoss: 20.821375\n",
      "Train Epoch: 9 [1200/8040 (15%)]\tLoss: 22.338293\n",
      "Train Epoch: 9 [2400/8040 (30%)]\tLoss: 22.153813\n",
      "Train Epoch: 9 [3600/8040 (45%)]\tLoss: 22.386395\n",
      "Train Epoch: 9 [4800/8040 (60%)]\tLoss: 20.943817\n",
      "Train Epoch: 9 [6000/8040 (75%)]\tLoss: 22.730273\n",
      "Train Epoch: 9 [7200/8040 (90%)]\tLoss: 22.238420\n",
      "====> Epoch: 9 Average loss: 22.4618\n",
      "epoch: 9, beta=0, frac_anom=0.01\n",
      "Train Epoch: 10 [0/8040 (0%)]\tLoss: 22.612415\n",
      "Train Epoch: 10 [1200/8040 (15%)]\tLoss: 22.780611\n",
      "Train Epoch: 10 [2400/8040 (30%)]\tLoss: 21.890047\n",
      "Train Epoch: 10 [3600/8040 (45%)]\tLoss: 22.023275\n",
      "Train Epoch: 10 [4800/8040 (60%)]\tLoss: 22.478601\n",
      "Train Epoch: 10 [6000/8040 (75%)]\tLoss: 21.866532\n",
      "Train Epoch: 10 [7200/8040 (90%)]\tLoss: 21.523541\n",
      "====> Epoch: 10 Average loss: 22.0700\n",
      "epoch: 10, beta=0, frac_anom=0.01\n",
      "Train Epoch: 11 [0/8040 (0%)]\tLoss: 21.693079\n",
      "Train Epoch: 11 [1200/8040 (15%)]\tLoss: 22.571340\n",
      "Train Epoch: 11 [2400/8040 (30%)]\tLoss: 21.014176\n",
      "Train Epoch: 11 [3600/8040 (45%)]\tLoss: 23.019061\n",
      "Train Epoch: 11 [4800/8040 (60%)]\tLoss: 21.592039\n",
      "Train Epoch: 11 [6000/8040 (75%)]\tLoss: 21.972068\n",
      "Train Epoch: 11 [7200/8040 (90%)]\tLoss: 22.529126\n",
      "====> Epoch: 11 Average loss: 21.8762\n",
      "epoch: 11, beta=0, frac_anom=0.01\n",
      "Train Epoch: 12 [0/8040 (0%)]\tLoss: 21.317310\n",
      "Train Epoch: 12 [1200/8040 (15%)]\tLoss: 21.541622\n",
      "Train Epoch: 12 [2400/8040 (30%)]\tLoss: 20.956584\n",
      "Train Epoch: 12 [3600/8040 (45%)]\tLoss: 20.948558\n",
      "Train Epoch: 12 [4800/8040 (60%)]\tLoss: 22.004720\n",
      "Train Epoch: 12 [6000/8040 (75%)]\tLoss: 20.776152\n",
      "Train Epoch: 12 [7200/8040 (90%)]\tLoss: 22.295799\n",
      "====> Epoch: 12 Average loss: 21.6056\n",
      "epoch: 12, beta=0, frac_anom=0.01\n",
      "Train Epoch: 13 [0/8040 (0%)]\tLoss: 21.268955\n",
      "Train Epoch: 13 [1200/8040 (15%)]\tLoss: 21.282819\n",
      "Train Epoch: 13 [2400/8040 (30%)]\tLoss: 22.306252\n",
      "Train Epoch: 13 [3600/8040 (45%)]\tLoss: 21.769596\n",
      "Train Epoch: 13 [4800/8040 (60%)]\tLoss: 20.922744\n",
      "Train Epoch: 13 [6000/8040 (75%)]\tLoss: 20.938873\n",
      "Train Epoch: 13 [7200/8040 (90%)]\tLoss: 21.078939\n",
      "====> Epoch: 13 Average loss: 21.3931\n",
      "epoch: 13, beta=0, frac_anom=0.01\n",
      "Train Epoch: 14 [0/8040 (0%)]\tLoss: 21.949300\n",
      "Train Epoch: 14 [1200/8040 (15%)]\tLoss: 21.087764\n",
      "Train Epoch: 14 [2400/8040 (30%)]\tLoss: 22.352468\n",
      "Train Epoch: 14 [3600/8040 (45%)]\tLoss: 22.147717\n",
      "Train Epoch: 14 [4800/8040 (60%)]\tLoss: 20.719773\n",
      "Train Epoch: 14 [6000/8040 (75%)]\tLoss: 20.032817\n",
      "Train Epoch: 14 [7200/8040 (90%)]\tLoss: 20.296694\n",
      "====> Epoch: 14 Average loss: 21.2843\n",
      "epoch: 14, beta=0, frac_anom=0.01\n",
      "Train Epoch: 15 [0/8040 (0%)]\tLoss: 20.329118\n",
      "Train Epoch: 15 [1200/8040 (15%)]\tLoss: 21.074729\n",
      "Train Epoch: 15 [2400/8040 (30%)]\tLoss: 21.525545\n",
      "Train Epoch: 15 [3600/8040 (45%)]\tLoss: 19.992830\n",
      "Train Epoch: 15 [4800/8040 (60%)]\tLoss: 23.419027\n",
      "Train Epoch: 15 [6000/8040 (75%)]\tLoss: 22.155585\n",
      "Train Epoch: 15 [7200/8040 (90%)]\tLoss: 20.889172\n",
      "====> Epoch: 15 Average loss: 21.1012\n",
      "epoch: 15, beta=0, frac_anom=0.01\n",
      "Train Epoch: 16 [0/8040 (0%)]\tLoss: 20.539768\n",
      "Train Epoch: 16 [1200/8040 (15%)]\tLoss: 21.497963\n",
      "Train Epoch: 16 [2400/8040 (30%)]\tLoss: 21.064311\n",
      "Train Epoch: 16 [3600/8040 (45%)]\tLoss: 21.741846\n",
      "Train Epoch: 16 [4800/8040 (60%)]\tLoss: 20.351510\n",
      "Train Epoch: 16 [6000/8040 (75%)]\tLoss: 22.104940\n",
      "Train Epoch: 16 [7200/8040 (90%)]\tLoss: 20.539624\n",
      "====> Epoch: 16 Average loss: 21.0109\n",
      "epoch: 16, beta=0, frac_anom=0.01\n",
      "Train Epoch: 17 [0/8040 (0%)]\tLoss: 21.166750\n",
      "Train Epoch: 17 [1200/8040 (15%)]\tLoss: 20.868547\n",
      "Train Epoch: 17 [2400/8040 (30%)]\tLoss: 20.220097\n",
      "Train Epoch: 17 [3600/8040 (45%)]\tLoss: 20.879846\n",
      "Train Epoch: 17 [4800/8040 (60%)]\tLoss: 19.846529\n",
      "Train Epoch: 17 [6000/8040 (75%)]\tLoss: 20.370874\n",
      "Train Epoch: 17 [7200/8040 (90%)]\tLoss: 20.396979\n",
      "====> Epoch: 17 Average loss: 20.9684\n",
      "epoch: 17, beta=0, frac_anom=0.01\n",
      "Train Epoch: 18 [0/8040 (0%)]\tLoss: 21.095754\n",
      "Train Epoch: 18 [1200/8040 (15%)]\tLoss: 19.486346\n",
      "Train Epoch: 18 [2400/8040 (30%)]\tLoss: 21.352380\n",
      "Train Epoch: 18 [3600/8040 (45%)]\tLoss: 20.698938\n",
      "Train Epoch: 18 [4800/8040 (60%)]\tLoss: 20.025415\n",
      "Train Epoch: 18 [6000/8040 (75%)]\tLoss: 19.837903\n",
      "Train Epoch: 18 [7200/8040 (90%)]\tLoss: 20.175106\n",
      "====> Epoch: 18 Average loss: 20.8193\n",
      "epoch: 18, beta=0, frac_anom=0.01\n",
      "Train Epoch: 19 [0/8040 (0%)]\tLoss: 20.347589\n",
      "Train Epoch: 19 [1200/8040 (15%)]\tLoss: 19.840458\n",
      "Train Epoch: 19 [2400/8040 (30%)]\tLoss: 20.596718\n",
      "Train Epoch: 19 [3600/8040 (45%)]\tLoss: 19.068490\n",
      "Train Epoch: 19 [4800/8040 (60%)]\tLoss: 21.070483\n",
      "Train Epoch: 19 [6000/8040 (75%)]\tLoss: 21.103516\n",
      "Train Epoch: 19 [7200/8040 (90%)]\tLoss: 21.372713\n",
      "====> Epoch: 19 Average loss: 20.7152\n",
      "epoch: 19, beta=0, frac_anom=0.01\n",
      "Train Epoch: 20 [0/8040 (0%)]\tLoss: 20.678208\n",
      "Train Epoch: 20 [1200/8040 (15%)]\tLoss: 20.596128\n",
      "Train Epoch: 20 [2400/8040 (30%)]\tLoss: 20.323962\n",
      "Train Epoch: 20 [3600/8040 (45%)]\tLoss: 21.590137\n",
      "Train Epoch: 20 [4800/8040 (60%)]\tLoss: 19.077702\n",
      "Train Epoch: 20 [6000/8040 (75%)]\tLoss: 20.507933\n",
      "Train Epoch: 20 [7200/8040 (90%)]\tLoss: 20.966846\n",
      "====> Epoch: 20 Average loss: 20.6875\n",
      "epoch: 20, beta=0, frac_anom=0.01\n",
      "Train Epoch: 21 [0/8040 (0%)]\tLoss: 21.588702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 21 [1200/8040 (15%)]\tLoss: 21.147404\n",
      "Train Epoch: 21 [2400/8040 (30%)]\tLoss: 21.243229\n",
      "Train Epoch: 21 [3600/8040 (45%)]\tLoss: 20.875730\n",
      "Train Epoch: 21 [4800/8040 (60%)]\tLoss: 20.226410\n",
      "Train Epoch: 21 [6000/8040 (75%)]\tLoss: 20.567240\n",
      "Train Epoch: 21 [7200/8040 (90%)]\tLoss: 20.220829\n",
      "====> Epoch: 21 Average loss: 20.5569\n",
      "epoch: 21, beta=0, frac_anom=0.01\n",
      "Train Epoch: 22 [0/8040 (0%)]\tLoss: 20.830949\n",
      "Train Epoch: 22 [1200/8040 (15%)]\tLoss: 20.162199\n",
      "Train Epoch: 22 [2400/8040 (30%)]\tLoss: 20.726363\n",
      "Train Epoch: 22 [3600/8040 (45%)]\tLoss: 20.659774\n",
      "Train Epoch: 22 [4800/8040 (60%)]\tLoss: 19.957076\n",
      "Train Epoch: 22 [6000/8040 (75%)]\tLoss: 19.559819\n",
      "Train Epoch: 22 [7200/8040 (90%)]\tLoss: 21.212382\n",
      "====> Epoch: 22 Average loss: 20.5013\n",
      "epoch: 22, beta=0, frac_anom=0.01\n",
      "Train Epoch: 23 [0/8040 (0%)]\tLoss: 20.903697\n",
      "Train Epoch: 23 [1200/8040 (15%)]\tLoss: 22.772335\n",
      "Train Epoch: 23 [2400/8040 (30%)]\tLoss: 20.258230\n",
      "Train Epoch: 23 [3600/8040 (45%)]\tLoss: 20.723690\n",
      "Train Epoch: 23 [4800/8040 (60%)]\tLoss: 21.153868\n",
      "Train Epoch: 23 [6000/8040 (75%)]\tLoss: 19.610986\n",
      "Train Epoch: 23 [7200/8040 (90%)]\tLoss: 20.168860\n",
      "====> Epoch: 23 Average loss: 20.4735\n",
      "epoch: 23, beta=0, frac_anom=0.01\n",
      "Train Epoch: 24 [0/8040 (0%)]\tLoss: 20.271722\n",
      "Train Epoch: 24 [1200/8040 (15%)]\tLoss: 18.816921\n",
      "Train Epoch: 24 [2400/8040 (30%)]\tLoss: 20.557515\n",
      "Train Epoch: 24 [3600/8040 (45%)]\tLoss: 20.137683\n",
      "Train Epoch: 24 [4800/8040 (60%)]\tLoss: 19.926017\n",
      "Train Epoch: 24 [6000/8040 (75%)]\tLoss: 19.818488\n",
      "Train Epoch: 24 [7200/8040 (90%)]\tLoss: 20.626654\n",
      "====> Epoch: 24 Average loss: 20.3910\n",
      "epoch: 24, beta=0, frac_anom=0.01\n",
      "Train Epoch: 25 [0/8040 (0%)]\tLoss: 20.872933\n",
      "Train Epoch: 25 [1200/8040 (15%)]\tLoss: 20.855094\n",
      "Train Epoch: 25 [2400/8040 (30%)]\tLoss: 18.948527\n",
      "Train Epoch: 25 [3600/8040 (45%)]\tLoss: 20.578432\n",
      "Train Epoch: 25 [4800/8040 (60%)]\tLoss: 20.676908\n",
      "Train Epoch: 25 [6000/8040 (75%)]\tLoss: 20.749943\n",
      "Train Epoch: 25 [7200/8040 (90%)]\tLoss: 19.552637\n",
      "====> Epoch: 25 Average loss: 20.3791\n",
      "epoch: 25, beta=0, frac_anom=0.01\n",
      "Train Epoch: 26 [0/8040 (0%)]\tLoss: 19.520986\n",
      "Train Epoch: 26 [1200/8040 (15%)]\tLoss: 19.998368\n",
      "Train Epoch: 26 [2400/8040 (30%)]\tLoss: 19.917163\n",
      "Train Epoch: 26 [3600/8040 (45%)]\tLoss: 21.784066\n",
      "Train Epoch: 26 [4800/8040 (60%)]\tLoss: 20.582269\n",
      "Train Epoch: 26 [6000/8040 (75%)]\tLoss: 21.143412\n",
      "Train Epoch: 26 [7200/8040 (90%)]\tLoss: 20.761007\n",
      "====> Epoch: 26 Average loss: 20.2877\n",
      "epoch: 26, beta=0, frac_anom=0.01\n",
      "Train Epoch: 27 [0/8040 (0%)]\tLoss: 20.482650\n",
      "Train Epoch: 27 [1200/8040 (15%)]\tLoss: 19.994023\n",
      "Train Epoch: 27 [2400/8040 (30%)]\tLoss: 20.341970\n",
      "Train Epoch: 27 [3600/8040 (45%)]\tLoss: 21.529224\n",
      "Train Epoch: 27 [4800/8040 (60%)]\tLoss: 20.396769\n",
      "Train Epoch: 27 [6000/8040 (75%)]\tLoss: 21.294226\n",
      "Train Epoch: 27 [7200/8040 (90%)]\tLoss: 20.670848\n",
      "====> Epoch: 27 Average loss: 20.2803\n",
      "epoch: 27, beta=0, frac_anom=0.01\n",
      "Train Epoch: 28 [0/8040 (0%)]\tLoss: 19.995532\n",
      "Train Epoch: 28 [1200/8040 (15%)]\tLoss: 20.489233\n",
      "Train Epoch: 28 [2400/8040 (30%)]\tLoss: 20.942910\n",
      "Train Epoch: 28 [3600/8040 (45%)]\tLoss: 19.225000\n",
      "Train Epoch: 28 [4800/8040 (60%)]\tLoss: 21.189443\n",
      "Train Epoch: 28 [6000/8040 (75%)]\tLoss: 19.951221\n",
      "Train Epoch: 28 [7200/8040 (90%)]\tLoss: 18.838505\n",
      "====> Epoch: 28 Average loss: 20.2325\n",
      "epoch: 28, beta=0, frac_anom=0.01\n",
      "Train Epoch: 29 [0/8040 (0%)]\tLoss: 19.705162\n",
      "Train Epoch: 29 [1200/8040 (15%)]\tLoss: 19.589339\n",
      "Train Epoch: 29 [2400/8040 (30%)]\tLoss: 20.155957\n",
      "Train Epoch: 29 [3600/8040 (45%)]\tLoss: 20.601676\n",
      "Train Epoch: 29 [4800/8040 (60%)]\tLoss: 19.145007\n",
      "Train Epoch: 29 [6000/8040 (75%)]\tLoss: 21.232544\n",
      "Train Epoch: 29 [7200/8040 (90%)]\tLoss: 20.143402\n",
      "====> Epoch: 29 Average loss: 20.2080\n",
      "epoch: 29, beta=0, frac_anom=0.01\n",
      "Train Epoch: 30 [0/8040 (0%)]\tLoss: 20.018357\n",
      "Train Epoch: 30 [1200/8040 (15%)]\tLoss: 18.592952\n",
      "Train Epoch: 30 [2400/8040 (30%)]\tLoss: 21.897076\n",
      "Train Epoch: 30 [3600/8040 (45%)]\tLoss: 19.772298\n",
      "Train Epoch: 30 [4800/8040 (60%)]\tLoss: 20.737870\n",
      "Train Epoch: 30 [6000/8040 (75%)]\tLoss: 20.706746\n",
      "Train Epoch: 30 [7200/8040 (90%)]\tLoss: 20.812826\n",
      "====> Epoch: 30 Average loss: 20.1588\n",
      "epoch: 30, beta=0, frac_anom=0.01\n",
      "Train Epoch: 31 [0/8040 (0%)]\tLoss: 18.467843\n",
      "Train Epoch: 31 [1200/8040 (15%)]\tLoss: 19.161796\n",
      "Train Epoch: 31 [2400/8040 (30%)]\tLoss: 19.600059\n",
      "Train Epoch: 31 [3600/8040 (45%)]\tLoss: 20.603473\n",
      "Train Epoch: 31 [4800/8040 (60%)]\tLoss: 19.032540\n",
      "Train Epoch: 31 [6000/8040 (75%)]\tLoss: 18.699223\n",
      "Train Epoch: 31 [7200/8040 (90%)]\tLoss: 19.914941\n",
      "====> Epoch: 31 Average loss: 20.0709\n",
      "epoch: 31, beta=0, frac_anom=0.01\n",
      "Train Epoch: 32 [0/8040 (0%)]\tLoss: 19.911121\n",
      "Train Epoch: 32 [1200/8040 (15%)]\tLoss: 20.697970\n",
      "Train Epoch: 32 [2400/8040 (30%)]\tLoss: 20.016081\n",
      "Train Epoch: 32 [3600/8040 (45%)]\tLoss: 20.675631\n",
      "Train Epoch: 32 [4800/8040 (60%)]\tLoss: 19.811271\n",
      "Train Epoch: 32 [6000/8040 (75%)]\tLoss: 18.085421\n",
      "Train Epoch: 32 [7200/8040 (90%)]\tLoss: 21.531431\n",
      "====> Epoch: 32 Average loss: 20.0877\n",
      "epoch: 32, beta=0, frac_anom=0.01\n",
      "Train Epoch: 33 [0/8040 (0%)]\tLoss: 19.609294\n",
      "Train Epoch: 33 [1200/8040 (15%)]\tLoss: 20.152089\n",
      "Train Epoch: 33 [2400/8040 (30%)]\tLoss: 19.745455\n",
      "Train Epoch: 33 [3600/8040 (45%)]\tLoss: 19.754510\n",
      "Train Epoch: 33 [4800/8040 (60%)]\tLoss: 20.138053\n",
      "Train Epoch: 33 [6000/8040 (75%)]\tLoss: 20.685559\n",
      "Train Epoch: 33 [7200/8040 (90%)]\tLoss: 19.612111\n",
      "====> Epoch: 33 Average loss: 20.0528\n",
      "epoch: 33, beta=0, frac_anom=0.01\n",
      "Train Epoch: 34 [0/8040 (0%)]\tLoss: 18.481067\n",
      "Train Epoch: 34 [1200/8040 (15%)]\tLoss: 19.982355\n",
      "Train Epoch: 34 [2400/8040 (30%)]\tLoss: 20.243640\n",
      "Train Epoch: 34 [3600/8040 (45%)]\tLoss: 19.582977\n",
      "Train Epoch: 34 [4800/8040 (60%)]\tLoss: 19.556163\n",
      "Train Epoch: 34 [6000/8040 (75%)]\tLoss: 18.324974\n",
      "Train Epoch: 34 [7200/8040 (90%)]\tLoss: 20.390373\n",
      "====> Epoch: 34 Average loss: 20.0478\n",
      "epoch: 34, beta=0, frac_anom=0.01\n",
      "Train Epoch: 35 [0/8040 (0%)]\tLoss: 20.194385\n",
      "Train Epoch: 35 [1200/8040 (15%)]\tLoss: 20.948910\n",
      "Train Epoch: 35 [2400/8040 (30%)]\tLoss: 19.944244\n",
      "Train Epoch: 35 [3600/8040 (45%)]\tLoss: 18.702692\n",
      "Train Epoch: 35 [4800/8040 (60%)]\tLoss: 20.868266\n",
      "Train Epoch: 35 [6000/8040 (75%)]\tLoss: 19.253019\n",
      "Train Epoch: 35 [7200/8040 (90%)]\tLoss: 20.586741\n",
      "====> Epoch: 35 Average loss: 19.9956\n",
      "epoch: 35, beta=0, frac_anom=0.01\n",
      "Train Epoch: 36 [0/8040 (0%)]\tLoss: 20.109505\n",
      "Train Epoch: 36 [1200/8040 (15%)]\tLoss: 19.288167\n",
      "Train Epoch: 36 [2400/8040 (30%)]\tLoss: 20.433895\n",
      "Train Epoch: 36 [3600/8040 (45%)]\tLoss: 19.200144\n",
      "Train Epoch: 36 [4800/8040 (60%)]\tLoss: 19.860217\n",
      "Train Epoch: 36 [6000/8040 (75%)]\tLoss: 19.332050\n",
      "Train Epoch: 36 [7200/8040 (90%)]\tLoss: 20.024821\n",
      "====> Epoch: 36 Average loss: 19.9588\n",
      "epoch: 36, beta=0, frac_anom=0.01\n",
      "Train Epoch: 37 [0/8040 (0%)]\tLoss: 18.698899\n",
      "Train Epoch: 37 [1200/8040 (15%)]\tLoss: 20.417499\n",
      "Train Epoch: 37 [2400/8040 (30%)]\tLoss: 21.586558\n",
      "Train Epoch: 37 [3600/8040 (45%)]\tLoss: 18.574341\n",
      "Train Epoch: 37 [4800/8040 (60%)]\tLoss: 20.081020\n",
      "Train Epoch: 37 [6000/8040 (75%)]\tLoss: 20.391488\n",
      "Train Epoch: 37 [7200/8040 (90%)]\tLoss: 19.100539\n",
      "====> Epoch: 37 Average loss: 19.9457\n",
      "epoch: 37, beta=0, frac_anom=0.01\n",
      "Train Epoch: 38 [0/8040 (0%)]\tLoss: 20.526152\n",
      "Train Epoch: 38 [1200/8040 (15%)]\tLoss: 19.292249\n",
      "Train Epoch: 38 [2400/8040 (30%)]\tLoss: 19.578442\n",
      "Train Epoch: 38 [3600/8040 (45%)]\tLoss: 20.606036\n",
      "Train Epoch: 38 [4800/8040 (60%)]\tLoss: 20.013790\n",
      "Train Epoch: 38 [6000/8040 (75%)]\tLoss: 20.280583\n",
      "Train Epoch: 38 [7200/8040 (90%)]\tLoss: 18.812439\n",
      "====> Epoch: 38 Average loss: 19.8998\n",
      "epoch: 38, beta=0, frac_anom=0.01\n",
      "Train Epoch: 39 [0/8040 (0%)]\tLoss: 20.100460\n",
      "Train Epoch: 39 [1200/8040 (15%)]\tLoss: 19.872253\n",
      "Train Epoch: 39 [2400/8040 (30%)]\tLoss: 19.048570\n",
      "Train Epoch: 39 [3600/8040 (45%)]\tLoss: 19.288967\n",
      "Train Epoch: 39 [4800/8040 (60%)]\tLoss: 19.043378\n",
      "Train Epoch: 39 [6000/8040 (75%)]\tLoss: 21.292220\n",
      "Train Epoch: 39 [7200/8040 (90%)]\tLoss: 20.097791\n",
      "====> Epoch: 39 Average loss: 19.8680\n",
      "epoch: 39, beta=0, frac_anom=0.01\n",
      "Train Epoch: 40 [0/8040 (0%)]\tLoss: 20.774896\n",
      "Train Epoch: 40 [1200/8040 (15%)]\tLoss: 20.004433\n",
      "Train Epoch: 40 [2400/8040 (30%)]\tLoss: 19.854628\n",
      "Train Epoch: 40 [3600/8040 (45%)]\tLoss: 19.123771\n",
      "Train Epoch: 40 [4800/8040 (60%)]\tLoss: 19.293097\n",
      "Train Epoch: 40 [6000/8040 (75%)]\tLoss: 19.834424\n",
      "Train Epoch: 40 [7200/8040 (90%)]\tLoss: 20.179122\n",
      "====> Epoch: 40 Average loss: 19.8753\n",
      "epoch: 40, beta=0, frac_anom=0.01\n",
      "Train Epoch: 41 [0/8040 (0%)]\tLoss: 19.359908\n",
      "Train Epoch: 41 [1200/8040 (15%)]\tLoss: 20.569535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 41 [2400/8040 (30%)]\tLoss: 19.195675\n",
      "Train Epoch: 41 [3600/8040 (45%)]\tLoss: 19.403430\n",
      "Train Epoch: 41 [4800/8040 (60%)]\tLoss: 18.872306\n",
      "Train Epoch: 41 [6000/8040 (75%)]\tLoss: 20.527905\n",
      "Train Epoch: 41 [7200/8040 (90%)]\tLoss: 19.142336\n",
      "====> Epoch: 41 Average loss: 19.8616\n",
      "epoch: 41, beta=0, frac_anom=0.01\n",
      "Train Epoch: 42 [0/8040 (0%)]\tLoss: 18.885653\n",
      "Train Epoch: 42 [1200/8040 (15%)]\tLoss: 19.509428\n",
      "Train Epoch: 42 [2400/8040 (30%)]\tLoss: 20.391817\n",
      "Train Epoch: 42 [3600/8040 (45%)]\tLoss: 19.717751\n",
      "Train Epoch: 42 [4800/8040 (60%)]\tLoss: 19.518673\n",
      "Train Epoch: 42 [6000/8040 (75%)]\tLoss: 19.194733\n",
      "Train Epoch: 42 [7200/8040 (90%)]\tLoss: 20.549884\n",
      "====> Epoch: 42 Average loss: 19.8321\n",
      "epoch: 42, beta=0, frac_anom=0.01\n",
      "Train Epoch: 43 [0/8040 (0%)]\tLoss: 18.997241\n",
      "Train Epoch: 43 [1200/8040 (15%)]\tLoss: 18.893148\n",
      "Train Epoch: 43 [2400/8040 (30%)]\tLoss: 19.915763\n",
      "Train Epoch: 43 [3600/8040 (45%)]\tLoss: 19.220683\n",
      "Train Epoch: 43 [4800/8040 (60%)]\tLoss: 19.805339\n",
      "Train Epoch: 43 [6000/8040 (75%)]\tLoss: 19.809574\n",
      "Train Epoch: 43 [7200/8040 (90%)]\tLoss: 20.109654\n",
      "====> Epoch: 43 Average loss: 19.8282\n",
      "epoch: 43, beta=0, frac_anom=0.01\n",
      "Train Epoch: 44 [0/8040 (0%)]\tLoss: 19.339396\n",
      "Train Epoch: 44 [1200/8040 (15%)]\tLoss: 19.523435\n",
      "Train Epoch: 44 [2400/8040 (30%)]\tLoss: 20.148663\n",
      "Train Epoch: 44 [3600/8040 (45%)]\tLoss: 19.122083\n",
      "Train Epoch: 44 [4800/8040 (60%)]\tLoss: 19.892879\n",
      "Train Epoch: 44 [6000/8040 (75%)]\tLoss: 19.218211\n",
      "Train Epoch: 44 [7200/8040 (90%)]\tLoss: 19.504415\n",
      "====> Epoch: 44 Average loss: 19.7745\n",
      "epoch: 44, beta=0, frac_anom=0.01\n",
      "Train Epoch: 45 [0/8040 (0%)]\tLoss: 20.238949\n",
      "Train Epoch: 45 [1200/8040 (15%)]\tLoss: 19.208431\n",
      "Train Epoch: 45 [2400/8040 (30%)]\tLoss: 19.555865\n",
      "Train Epoch: 45 [3600/8040 (45%)]\tLoss: 20.623952\n",
      "Train Epoch: 45 [4800/8040 (60%)]\tLoss: 20.535683\n",
      "Train Epoch: 45 [6000/8040 (75%)]\tLoss: 20.130076\n",
      "Train Epoch: 45 [7200/8040 (90%)]\tLoss: 19.031026\n",
      "====> Epoch: 45 Average loss: 19.7748\n",
      "epoch: 45, beta=0, frac_anom=0.01\n",
      "Train Epoch: 46 [0/8040 (0%)]\tLoss: 19.547327\n",
      "Train Epoch: 46 [1200/8040 (15%)]\tLoss: 20.013607\n",
      "Train Epoch: 46 [2400/8040 (30%)]\tLoss: 18.895962\n",
      "Train Epoch: 46 [3600/8040 (45%)]\tLoss: 18.437950\n",
      "Train Epoch: 46 [4800/8040 (60%)]\tLoss: 20.279545\n",
      "Train Epoch: 46 [6000/8040 (75%)]\tLoss: 19.798092\n",
      "Train Epoch: 46 [7200/8040 (90%)]\tLoss: 20.321191\n",
      "====> Epoch: 46 Average loss: 19.7780\n",
      "epoch: 46, beta=0, frac_anom=0.01\n",
      "Train Epoch: 47 [0/8040 (0%)]\tLoss: 18.616774\n",
      "Train Epoch: 47 [1200/8040 (15%)]\tLoss: 19.069173\n",
      "Train Epoch: 47 [2400/8040 (30%)]\tLoss: 21.111446\n",
      "Train Epoch: 47 [3600/8040 (45%)]\tLoss: 20.203430\n",
      "Train Epoch: 47 [4800/8040 (60%)]\tLoss: 19.966077\n",
      "Train Epoch: 47 [6000/8040 (75%)]\tLoss: 19.996977\n",
      "Train Epoch: 47 [7200/8040 (90%)]\tLoss: 20.101817\n",
      "====> Epoch: 47 Average loss: 19.6793\n",
      "epoch: 47, beta=0, frac_anom=0.01\n",
      "Train Epoch: 48 [0/8040 (0%)]\tLoss: 18.806514\n",
      "Train Epoch: 48 [1200/8040 (15%)]\tLoss: 18.659589\n",
      "Train Epoch: 48 [2400/8040 (30%)]\tLoss: 19.242902\n",
      "Train Epoch: 48 [3600/8040 (45%)]\tLoss: 20.365057\n",
      "Train Epoch: 48 [4800/8040 (60%)]\tLoss: 20.112748\n",
      "Train Epoch: 48 [6000/8040 (75%)]\tLoss: 19.811877\n",
      "Train Epoch: 48 [7200/8040 (90%)]\tLoss: 18.031012\n",
      "====> Epoch: 48 Average loss: 19.6789\n",
      "epoch: 48, beta=0, frac_anom=0.01\n",
      "Train Epoch: 49 [0/8040 (0%)]\tLoss: 20.376434\n",
      "Train Epoch: 49 [1200/8040 (15%)]\tLoss: 19.796853\n",
      "Train Epoch: 49 [2400/8040 (30%)]\tLoss: 19.692908\n",
      "Train Epoch: 49 [3600/8040 (45%)]\tLoss: 19.408464\n",
      "Train Epoch: 49 [4800/8040 (60%)]\tLoss: 20.508537\n",
      "Train Epoch: 49 [6000/8040 (75%)]\tLoss: 20.478798\n",
      "Train Epoch: 49 [7200/8040 (90%)]\tLoss: 20.121690\n",
      "====> Epoch: 49 Average loss: 19.6680\n",
      "epoch: 49, beta=0, frac_anom=0.01\n",
      "Train Epoch: 50 [0/8040 (0%)]\tLoss: 19.036910\n",
      "Train Epoch: 50 [1200/8040 (15%)]\tLoss: 19.673971\n",
      "Train Epoch: 50 [2400/8040 (30%)]\tLoss: 20.513462\n",
      "Train Epoch: 50 [3600/8040 (45%)]\tLoss: 18.806382\n",
      "Train Epoch: 50 [4800/8040 (60%)]\tLoss: 19.203074\n",
      "Train Epoch: 50 [6000/8040 (75%)]\tLoss: 20.417240\n",
      "Train Epoch: 50 [7200/8040 (90%)]\tLoss: 20.481350\n",
      "====> Epoch: 50 Average loss: 19.6638\n",
      "epoch: 50, beta=0, frac_anom=0.01\n",
      "Train Epoch: 51 [0/8040 (0%)]\tLoss: 20.912366\n",
      "Train Epoch: 51 [1200/8040 (15%)]\tLoss: 20.486686\n",
      "Train Epoch: 51 [2400/8040 (30%)]\tLoss: 19.904944\n",
      "Train Epoch: 51 [3600/8040 (45%)]\tLoss: 19.888920\n",
      "Train Epoch: 51 [4800/8040 (60%)]\tLoss: 19.232029\n",
      "Train Epoch: 51 [6000/8040 (75%)]\tLoss: 21.190297\n",
      "Train Epoch: 51 [7200/8040 (90%)]\tLoss: 19.089126\n",
      "====> Epoch: 51 Average loss: 19.6253\n",
      "epoch: 51, beta=0, frac_anom=0.01\n",
      "Train Epoch: 52 [0/8040 (0%)]\tLoss: 20.113302\n",
      "Train Epoch: 52 [1200/8040 (15%)]\tLoss: 19.815157\n",
      "Train Epoch: 52 [2400/8040 (30%)]\tLoss: 19.042723\n",
      "Train Epoch: 52 [3600/8040 (45%)]\tLoss: 18.447420\n",
      "Train Epoch: 52 [4800/8040 (60%)]\tLoss: 20.280353\n",
      "Train Epoch: 52 [6000/8040 (75%)]\tLoss: 19.838981\n",
      "Train Epoch: 52 [7200/8040 (90%)]\tLoss: 19.626381\n",
      "====> Epoch: 52 Average loss: 19.5937\n",
      "epoch: 52, beta=0, frac_anom=0.01\n",
      "Train Epoch: 53 [0/8040 (0%)]\tLoss: 18.919027\n",
      "Train Epoch: 53 [1200/8040 (15%)]\tLoss: 21.074923\n",
      "Train Epoch: 53 [2400/8040 (30%)]\tLoss: 20.404189\n",
      "Train Epoch: 53 [3600/8040 (45%)]\tLoss: 19.964274\n",
      "Train Epoch: 53 [4800/8040 (60%)]\tLoss: 20.208512\n",
      "Train Epoch: 53 [6000/8040 (75%)]\tLoss: 19.341954\n",
      "Train Epoch: 53 [7200/8040 (90%)]\tLoss: 19.814803\n",
      "====> Epoch: 53 Average loss: 19.5873\n",
      "epoch: 53, beta=0, frac_anom=0.01\n",
      "Train Epoch: 54 [0/8040 (0%)]\tLoss: 19.527848\n",
      "Train Epoch: 54 [1200/8040 (15%)]\tLoss: 20.613993\n",
      "Train Epoch: 54 [2400/8040 (30%)]\tLoss: 19.840666\n",
      "Train Epoch: 54 [3600/8040 (45%)]\tLoss: 19.620243\n",
      "Train Epoch: 54 [4800/8040 (60%)]\tLoss: 18.697827\n",
      "Train Epoch: 54 [6000/8040 (75%)]\tLoss: 19.077069\n",
      "Train Epoch: 54 [7200/8040 (90%)]\tLoss: 19.025151\n",
      "====> Epoch: 54 Average loss: 19.5971\n",
      "epoch: 54, beta=0, frac_anom=0.01\n",
      "Train Epoch: 55 [0/8040 (0%)]\tLoss: 19.843854\n",
      "Train Epoch: 55 [1200/8040 (15%)]\tLoss: 19.670129\n",
      "Train Epoch: 55 [2400/8040 (30%)]\tLoss: 19.958931\n",
      "Train Epoch: 55 [3600/8040 (45%)]\tLoss: 19.027950\n",
      "Train Epoch: 55 [4800/8040 (60%)]\tLoss: 19.377710\n",
      "Train Epoch: 55 [6000/8040 (75%)]\tLoss: 18.893032\n",
      "Train Epoch: 55 [7200/8040 (90%)]\tLoss: 20.081826\n",
      "====> Epoch: 55 Average loss: 19.5537\n",
      "epoch: 55, beta=0, frac_anom=0.01\n",
      "Train Epoch: 56 [0/8040 (0%)]\tLoss: 19.190063\n",
      "Train Epoch: 56 [1200/8040 (15%)]\tLoss: 18.591567\n",
      "Train Epoch: 56 [2400/8040 (30%)]\tLoss: 19.043404\n",
      "Train Epoch: 56 [3600/8040 (45%)]\tLoss: 21.128632\n",
      "Train Epoch: 56 [4800/8040 (60%)]\tLoss: 18.584568\n",
      "Train Epoch: 56 [6000/8040 (75%)]\tLoss: 19.953237\n",
      "Train Epoch: 56 [7200/8040 (90%)]\tLoss: 19.843569\n",
      "====> Epoch: 56 Average loss: 19.5481\n",
      "epoch: 56, beta=0, frac_anom=0.01\n",
      "Train Epoch: 57 [0/8040 (0%)]\tLoss: 18.304755\n",
      "Train Epoch: 57 [1200/8040 (15%)]\tLoss: 19.883675\n",
      "Train Epoch: 57 [2400/8040 (30%)]\tLoss: 20.400287\n",
      "Train Epoch: 57 [3600/8040 (45%)]\tLoss: 18.576872\n",
      "Train Epoch: 57 [4800/8040 (60%)]\tLoss: 20.463519\n",
      "Train Epoch: 57 [6000/8040 (75%)]\tLoss: 20.271899\n",
      "Train Epoch: 57 [7200/8040 (90%)]\tLoss: 19.348578\n",
      "====> Epoch: 57 Average loss: 19.5006\n",
      "epoch: 57, beta=0, frac_anom=0.01\n",
      "Train Epoch: 58 [0/8040 (0%)]\tLoss: 19.706492\n",
      "Train Epoch: 58 [1200/8040 (15%)]\tLoss: 19.748057\n",
      "Train Epoch: 58 [2400/8040 (30%)]\tLoss: 19.358779\n",
      "Train Epoch: 58 [3600/8040 (45%)]\tLoss: 20.575307\n",
      "Train Epoch: 58 [4800/8040 (60%)]\tLoss: 21.046600\n",
      "Train Epoch: 58 [6000/8040 (75%)]\tLoss: 20.263143\n",
      "Train Epoch: 58 [7200/8040 (90%)]\tLoss: 19.436910\n",
      "====> Epoch: 58 Average loss: 19.5330\n",
      "epoch: 58, beta=0, frac_anom=0.01\n",
      "Train Epoch: 59 [0/8040 (0%)]\tLoss: 19.332760\n",
      "Train Epoch: 59 [1200/8040 (15%)]\tLoss: 18.681962\n",
      "Train Epoch: 59 [2400/8040 (30%)]\tLoss: 19.093311\n",
      "Train Epoch: 59 [3600/8040 (45%)]\tLoss: 20.234375\n",
      "Train Epoch: 59 [4800/8040 (60%)]\tLoss: 19.584957\n",
      "Train Epoch: 59 [6000/8040 (75%)]\tLoss: 19.302128\n",
      "Train Epoch: 59 [7200/8040 (90%)]\tLoss: 18.294454\n",
      "====> Epoch: 59 Average loss: 19.5112\n",
      "epoch: 59, beta=0, frac_anom=0.01\n",
      "Train Epoch: 60 [0/8040 (0%)]\tLoss: 19.514335\n",
      "Train Epoch: 60 [1200/8040 (15%)]\tLoss: 19.194491\n",
      "Train Epoch: 60 [2400/8040 (30%)]\tLoss: 19.386251\n",
      "Train Epoch: 60 [3600/8040 (45%)]\tLoss: 20.251855\n",
      "Train Epoch: 60 [4800/8040 (60%)]\tLoss: 19.837254\n",
      "Train Epoch: 60 [6000/8040 (75%)]\tLoss: 18.844629\n",
      "Train Epoch: 60 [7200/8040 (90%)]\tLoss: 19.141121\n",
      "====> Epoch: 60 Average loss: 19.4841\n",
      "epoch: 60, beta=0, frac_anom=0.01\n",
      "Train Epoch: 61 [0/8040 (0%)]\tLoss: 19.008822\n",
      "Train Epoch: 61 [1200/8040 (15%)]\tLoss: 20.030326\n",
      "Train Epoch: 61 [2400/8040 (30%)]\tLoss: 20.905383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 61 [3600/8040 (45%)]\tLoss: 19.521303\n",
      "Train Epoch: 61 [4800/8040 (60%)]\tLoss: 18.973775\n",
      "Train Epoch: 61 [6000/8040 (75%)]\tLoss: 19.497770\n",
      "Train Epoch: 61 [7200/8040 (90%)]\tLoss: 19.889583\n",
      "====> Epoch: 61 Average loss: 19.4878\n",
      "epoch: 61, beta=0, frac_anom=0.01\n",
      "Train Epoch: 62 [0/8040 (0%)]\tLoss: 18.525142\n",
      "Train Epoch: 62 [1200/8040 (15%)]\tLoss: 20.259591\n",
      "Train Epoch: 62 [2400/8040 (30%)]\tLoss: 19.668599\n",
      "Train Epoch: 62 [3600/8040 (45%)]\tLoss: 19.080611\n",
      "Train Epoch: 62 [4800/8040 (60%)]\tLoss: 18.715900\n",
      "Train Epoch: 62 [6000/8040 (75%)]\tLoss: 19.398787\n",
      "Train Epoch: 62 [7200/8040 (90%)]\tLoss: 20.552445\n",
      "====> Epoch: 62 Average loss: 19.4299\n",
      "epoch: 62, beta=0, frac_anom=0.01\n",
      "Train Epoch: 63 [0/8040 (0%)]\tLoss: 19.898830\n",
      "Train Epoch: 63 [1200/8040 (15%)]\tLoss: 19.227498\n",
      "Train Epoch: 63 [2400/8040 (30%)]\tLoss: 19.470660\n",
      "Train Epoch: 63 [3600/8040 (45%)]\tLoss: 19.234717\n",
      "Train Epoch: 63 [4800/8040 (60%)]\tLoss: 18.902869\n",
      "Train Epoch: 63 [6000/8040 (75%)]\tLoss: 20.424353\n",
      "Train Epoch: 63 [7200/8040 (90%)]\tLoss: 18.580025\n",
      "====> Epoch: 63 Average loss: 19.4680\n",
      "epoch: 63, beta=0, frac_anom=0.01\n",
      "Train Epoch: 64 [0/8040 (0%)]\tLoss: 20.301241\n",
      "Train Epoch: 64 [1200/8040 (15%)]\tLoss: 19.032395\n",
      "Train Epoch: 64 [2400/8040 (30%)]\tLoss: 19.813041\n",
      "Train Epoch: 64 [3600/8040 (45%)]\tLoss: 18.791978\n",
      "Train Epoch: 64 [4800/8040 (60%)]\tLoss: 19.327799\n",
      "Train Epoch: 64 [6000/8040 (75%)]\tLoss: 18.528080\n",
      "Train Epoch: 64 [7200/8040 (90%)]\tLoss: 18.546790\n",
      "====> Epoch: 64 Average loss: 19.4451\n",
      "epoch: 64, beta=0, frac_anom=0.01\n",
      "Train Epoch: 65 [0/8040 (0%)]\tLoss: 20.187036\n",
      "Train Epoch: 65 [1200/8040 (15%)]\tLoss: 19.425124\n",
      "Train Epoch: 65 [2400/8040 (30%)]\tLoss: 19.743341\n",
      "Train Epoch: 65 [3600/8040 (45%)]\tLoss: 18.924713\n",
      "Train Epoch: 65 [4800/8040 (60%)]\tLoss: 20.458872\n",
      "Train Epoch: 65 [6000/8040 (75%)]\tLoss: 20.078707\n",
      "Train Epoch: 65 [7200/8040 (90%)]\tLoss: 19.199634\n",
      "====> Epoch: 65 Average loss: 19.4532\n",
      "epoch: 65, beta=0, frac_anom=0.01\n",
      "Train Epoch: 66 [0/8040 (0%)]\tLoss: 20.224801\n",
      "Train Epoch: 66 [1200/8040 (15%)]\tLoss: 18.270333\n",
      "Train Epoch: 66 [2400/8040 (30%)]\tLoss: 18.836983\n",
      "Train Epoch: 66 [3600/8040 (45%)]\tLoss: 19.729814\n",
      "Train Epoch: 66 [4800/8040 (60%)]\tLoss: 17.687687\n",
      "Train Epoch: 66 [6000/8040 (75%)]\tLoss: 18.839716\n",
      "Train Epoch: 66 [7200/8040 (90%)]\tLoss: 19.813814\n",
      "====> Epoch: 66 Average loss: 19.3805\n",
      "epoch: 66, beta=0, frac_anom=0.01\n",
      "Train Epoch: 67 [0/8040 (0%)]\tLoss: 20.733398\n",
      "Train Epoch: 67 [1200/8040 (15%)]\tLoss: 18.897681\n",
      "Train Epoch: 67 [2400/8040 (30%)]\tLoss: 19.256036\n",
      "Train Epoch: 67 [3600/8040 (45%)]\tLoss: 18.584359\n",
      "Train Epoch: 67 [4800/8040 (60%)]\tLoss: 18.634906\n",
      "Train Epoch: 67 [6000/8040 (75%)]\tLoss: 20.384851\n",
      "Train Epoch: 67 [7200/8040 (90%)]\tLoss: 20.990759\n",
      "====> Epoch: 67 Average loss: 19.4546\n",
      "epoch: 67, beta=0, frac_anom=0.01\n",
      "Train Epoch: 68 [0/8040 (0%)]\tLoss: 17.920742\n",
      "Train Epoch: 68 [1200/8040 (15%)]\tLoss: 19.946077\n",
      "Train Epoch: 68 [2400/8040 (30%)]\tLoss: 20.271981\n",
      "Train Epoch: 68 [3600/8040 (45%)]\tLoss: 19.180414\n",
      "Train Epoch: 68 [4800/8040 (60%)]\tLoss: 19.179346\n",
      "Train Epoch: 68 [6000/8040 (75%)]\tLoss: 20.295982\n",
      "Train Epoch: 68 [7200/8040 (90%)]\tLoss: 19.588664\n",
      "====> Epoch: 68 Average loss: 19.4163\n",
      "epoch: 68, beta=0, frac_anom=0.01\n",
      "Train Epoch: 69 [0/8040 (0%)]\tLoss: 19.658722\n",
      "Train Epoch: 69 [1200/8040 (15%)]\tLoss: 18.778192\n",
      "Train Epoch: 69 [2400/8040 (30%)]\tLoss: 18.809094\n",
      "Train Epoch: 69 [3600/8040 (45%)]\tLoss: 19.457556\n",
      "Train Epoch: 69 [4800/8040 (60%)]\tLoss: 19.704317\n",
      "Train Epoch: 69 [6000/8040 (75%)]\tLoss: 20.445024\n",
      "Train Epoch: 69 [7200/8040 (90%)]\tLoss: 20.532910\n",
      "====> Epoch: 69 Average loss: 19.3722\n",
      "epoch: 69, beta=0, frac_anom=0.01\n",
      "Train Epoch: 70 [0/8040 (0%)]\tLoss: 20.244503\n",
      "Train Epoch: 70 [1200/8040 (15%)]\tLoss: 19.545540\n",
      "Train Epoch: 70 [2400/8040 (30%)]\tLoss: 19.867102\n",
      "Train Epoch: 70 [3600/8040 (45%)]\tLoss: 19.031311\n",
      "Train Epoch: 70 [4800/8040 (60%)]\tLoss: 18.919871\n",
      "Train Epoch: 70 [6000/8040 (75%)]\tLoss: 19.120217\n",
      "Train Epoch: 70 [7200/8040 (90%)]\tLoss: 18.306783\n",
      "====> Epoch: 70 Average loss: 19.3607\n",
      "epoch: 70, beta=0, frac_anom=0.01\n",
      "Train Epoch: 71 [0/8040 (0%)]\tLoss: 18.882914\n",
      "Train Epoch: 71 [1200/8040 (15%)]\tLoss: 19.333769\n",
      "Train Epoch: 71 [2400/8040 (30%)]\tLoss: 19.286442\n",
      "Train Epoch: 71 [3600/8040 (45%)]\tLoss: 19.665861\n",
      "Train Epoch: 71 [4800/8040 (60%)]\tLoss: 20.097264\n",
      "Train Epoch: 71 [6000/8040 (75%)]\tLoss: 19.553459\n",
      "Train Epoch: 71 [7200/8040 (90%)]\tLoss: 19.990578\n",
      "====> Epoch: 71 Average loss: 19.3717\n",
      "epoch: 71, beta=0, frac_anom=0.01\n",
      "Train Epoch: 72 [0/8040 (0%)]\tLoss: 18.110042\n",
      "Train Epoch: 72 [1200/8040 (15%)]\tLoss: 19.123704\n",
      "Train Epoch: 72 [2400/8040 (30%)]\tLoss: 18.271592\n",
      "Train Epoch: 72 [3600/8040 (45%)]\tLoss: 20.035091\n",
      "Train Epoch: 72 [4800/8040 (60%)]\tLoss: 19.158667\n",
      "Train Epoch: 72 [6000/8040 (75%)]\tLoss: 18.776917\n",
      "Train Epoch: 72 [7200/8040 (90%)]\tLoss: 19.038236\n",
      "====> Epoch: 72 Average loss: 19.3071\n",
      "epoch: 72, beta=0, frac_anom=0.01\n",
      "Train Epoch: 73 [0/8040 (0%)]\tLoss: 18.295713\n",
      "Train Epoch: 73 [1200/8040 (15%)]\tLoss: 20.781464\n",
      "Train Epoch: 73 [2400/8040 (30%)]\tLoss: 19.462628\n",
      "Train Epoch: 73 [3600/8040 (45%)]\tLoss: 19.679879\n",
      "Train Epoch: 73 [4800/8040 (60%)]\tLoss: 18.902346\n",
      "Train Epoch: 73 [6000/8040 (75%)]\tLoss: 18.933162\n",
      "Train Epoch: 73 [7200/8040 (90%)]\tLoss: 19.045907\n",
      "====> Epoch: 73 Average loss: 19.3606\n",
      "epoch: 73, beta=0, frac_anom=0.01\n",
      "Train Epoch: 74 [0/8040 (0%)]\tLoss: 18.903383\n",
      "Train Epoch: 74 [1200/8040 (15%)]\tLoss: 20.037246\n",
      "Train Epoch: 74 [2400/8040 (30%)]\tLoss: 19.634855\n",
      "Train Epoch: 74 [3600/8040 (45%)]\tLoss: 19.869586\n",
      "Train Epoch: 74 [4800/8040 (60%)]\tLoss: 18.908854\n",
      "Train Epoch: 74 [6000/8040 (75%)]\tLoss: 18.657520\n",
      "Train Epoch: 74 [7200/8040 (90%)]\tLoss: 19.536983\n",
      "====> Epoch: 74 Average loss: 19.3573\n",
      "epoch: 74, beta=0, frac_anom=0.01\n",
      "Train Epoch: 75 [0/8040 (0%)]\tLoss: 19.318339\n",
      "Train Epoch: 75 [1200/8040 (15%)]\tLoss: 18.867653\n",
      "Train Epoch: 75 [2400/8040 (30%)]\tLoss: 19.738633\n",
      "Train Epoch: 75 [3600/8040 (45%)]\tLoss: 19.388263\n",
      "Train Epoch: 75 [4800/8040 (60%)]\tLoss: 19.953009\n",
      "Train Epoch: 75 [6000/8040 (75%)]\tLoss: 19.618241\n",
      "Train Epoch: 75 [7200/8040 (90%)]\tLoss: 19.515619\n",
      "====> Epoch: 75 Average loss: 19.2905\n",
      "epoch: 75, beta=0, frac_anom=0.01\n",
      "Train Epoch: 76 [0/8040 (0%)]\tLoss: 18.607104\n",
      "Train Epoch: 76 [1200/8040 (15%)]\tLoss: 19.023661\n",
      "Train Epoch: 76 [2400/8040 (30%)]\tLoss: 19.833211\n",
      "Train Epoch: 76 [3600/8040 (45%)]\tLoss: 19.653025\n",
      "Train Epoch: 76 [4800/8040 (60%)]\tLoss: 19.054395\n",
      "Train Epoch: 76 [6000/8040 (75%)]\tLoss: 17.782023\n",
      "Train Epoch: 76 [7200/8040 (90%)]\tLoss: 20.074394\n",
      "====> Epoch: 76 Average loss: 19.2804\n",
      "epoch: 76, beta=0, frac_anom=0.01\n",
      "Train Epoch: 77 [0/8040 (0%)]\tLoss: 18.404150\n",
      "Train Epoch: 77 [1200/8040 (15%)]\tLoss: 19.307943\n",
      "Train Epoch: 77 [2400/8040 (30%)]\tLoss: 19.814740\n",
      "Train Epoch: 77 [3600/8040 (45%)]\tLoss: 18.658582\n",
      "Train Epoch: 77 [4800/8040 (60%)]\tLoss: 19.183219\n",
      "Train Epoch: 77 [6000/8040 (75%)]\tLoss: 20.117110\n",
      "Train Epoch: 77 [7200/8040 (90%)]\tLoss: 18.990922\n",
      "====> Epoch: 77 Average loss: 19.3137\n",
      "epoch: 77, beta=0, frac_anom=0.01\n",
      "Train Epoch: 78 [0/8040 (0%)]\tLoss: 18.689937\n",
      "Train Epoch: 78 [1200/8040 (15%)]\tLoss: 19.464661\n",
      "Train Epoch: 78 [2400/8040 (30%)]\tLoss: 18.703564\n",
      "Train Epoch: 78 [3600/8040 (45%)]\tLoss: 19.103906\n",
      "Train Epoch: 78 [4800/8040 (60%)]\tLoss: 19.421761\n",
      "Train Epoch: 78 [6000/8040 (75%)]\tLoss: 20.100612\n",
      "Train Epoch: 78 [7200/8040 (90%)]\tLoss: 18.564705\n",
      "====> Epoch: 78 Average loss: 19.3078\n",
      "epoch: 78, beta=0, frac_anom=0.01\n",
      "Train Epoch: 79 [0/8040 (0%)]\tLoss: 19.186218\n",
      "Train Epoch: 79 [1200/8040 (15%)]\tLoss: 18.883671\n",
      "Train Epoch: 79 [2400/8040 (30%)]\tLoss: 18.997831\n",
      "Train Epoch: 79 [3600/8040 (45%)]\tLoss: 18.966089\n",
      "Train Epoch: 79 [4800/8040 (60%)]\tLoss: 19.621830\n",
      "Train Epoch: 79 [6000/8040 (75%)]\tLoss: 19.151286\n",
      "Train Epoch: 79 [7200/8040 (90%)]\tLoss: 18.642973\n",
      "====> Epoch: 79 Average loss: 19.3139\n",
      "epoch: 79, beta=0, frac_anom=0.01\n",
      "Train Epoch: 80 [0/8040 (0%)]\tLoss: 18.400114\n",
      "Train Epoch: 80 [1200/8040 (15%)]\tLoss: 19.123824\n",
      "Train Epoch: 80 [2400/8040 (30%)]\tLoss: 19.072207\n",
      "Train Epoch: 80 [3600/8040 (45%)]\tLoss: 18.751021\n",
      "Train Epoch: 80 [4800/8040 (60%)]\tLoss: 18.341097\n",
      "Train Epoch: 80 [6000/8040 (75%)]\tLoss: 19.183252\n",
      "Train Epoch: 80 [7200/8040 (90%)]\tLoss: 18.448287\n",
      "====> Epoch: 80 Average loss: 19.2376\n",
      "epoch: 80, beta=0, frac_anom=0.01\n",
      "Train Epoch: 81 [0/8040 (0%)]\tLoss: 19.291890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 81 [1200/8040 (15%)]\tLoss: 17.763350\n",
      "Train Epoch: 81 [2400/8040 (30%)]\tLoss: 19.326298\n",
      "Train Epoch: 81 [3600/8040 (45%)]\tLoss: 18.690251\n",
      "Train Epoch: 81 [4800/8040 (60%)]\tLoss: 19.071539\n",
      "Train Epoch: 81 [6000/8040 (75%)]\tLoss: 19.540159\n",
      "Train Epoch: 81 [7200/8040 (90%)]\tLoss: 18.594857\n",
      "====> Epoch: 81 Average loss: 19.2203\n",
      "epoch: 81, beta=0, frac_anom=0.01\n",
      "Train Epoch: 82 [0/8040 (0%)]\tLoss: 19.708604\n",
      "Train Epoch: 82 [1200/8040 (15%)]\tLoss: 19.538544\n",
      "Train Epoch: 82 [2400/8040 (30%)]\tLoss: 19.703149\n",
      "Train Epoch: 82 [3600/8040 (45%)]\tLoss: 19.069977\n",
      "Train Epoch: 82 [4800/8040 (60%)]\tLoss: 18.381173\n",
      "Train Epoch: 82 [6000/8040 (75%)]\tLoss: 19.756759\n",
      "Train Epoch: 82 [7200/8040 (90%)]\tLoss: 19.453784\n",
      "====> Epoch: 82 Average loss: 19.2381\n",
      "epoch: 82, beta=0, frac_anom=0.01\n",
      "Train Epoch: 83 [0/8040 (0%)]\tLoss: 19.538220\n",
      "Train Epoch: 83 [1200/8040 (15%)]\tLoss: 18.851644\n",
      "Train Epoch: 83 [2400/8040 (30%)]\tLoss: 18.341762\n",
      "Train Epoch: 83 [3600/8040 (45%)]\tLoss: 20.087954\n",
      "Train Epoch: 83 [4800/8040 (60%)]\tLoss: 19.677437\n",
      "Train Epoch: 83 [6000/8040 (75%)]\tLoss: 18.468622\n",
      "Train Epoch: 83 [7200/8040 (90%)]\tLoss: 19.130908\n",
      "====> Epoch: 83 Average loss: 19.2246\n",
      "epoch: 83, beta=0, frac_anom=0.01\n",
      "Train Epoch: 84 [0/8040 (0%)]\tLoss: 20.242690\n",
      "Train Epoch: 84 [1200/8040 (15%)]\tLoss: 19.579932\n",
      "Train Epoch: 84 [2400/8040 (30%)]\tLoss: 19.959172\n",
      "Train Epoch: 84 [3600/8040 (45%)]\tLoss: 18.864612\n",
      "Train Epoch: 84 [4800/8040 (60%)]\tLoss: 19.991789\n",
      "Train Epoch: 84 [6000/8040 (75%)]\tLoss: 18.509672\n",
      "Train Epoch: 84 [7200/8040 (90%)]\tLoss: 19.162362\n",
      "====> Epoch: 84 Average loss: 19.2457\n",
      "epoch: 84, beta=0, frac_anom=0.01\n",
      "Train Epoch: 85 [0/8040 (0%)]\tLoss: 18.451196\n",
      "Train Epoch: 85 [1200/8040 (15%)]\tLoss: 18.909314\n",
      "Train Epoch: 85 [2400/8040 (30%)]\tLoss: 19.566874\n",
      "Train Epoch: 85 [3600/8040 (45%)]\tLoss: 18.080516\n",
      "Train Epoch: 85 [4800/8040 (60%)]\tLoss: 19.158242\n",
      "Train Epoch: 85 [6000/8040 (75%)]\tLoss: 19.214628\n",
      "Train Epoch: 85 [7200/8040 (90%)]\tLoss: 20.422359\n",
      "====> Epoch: 85 Average loss: 19.2018\n",
      "epoch: 85, beta=0, frac_anom=0.01\n",
      "Train Epoch: 86 [0/8040 (0%)]\tLoss: 19.880731\n",
      "Train Epoch: 86 [1200/8040 (15%)]\tLoss: 19.834749\n",
      "Train Epoch: 86 [2400/8040 (30%)]\tLoss: 18.184688\n",
      "Train Epoch: 86 [3600/8040 (45%)]\tLoss: 19.228479\n",
      "Train Epoch: 86 [4800/8040 (60%)]\tLoss: 18.968772\n",
      "Train Epoch: 86 [6000/8040 (75%)]\tLoss: 19.322862\n",
      "Train Epoch: 86 [7200/8040 (90%)]\tLoss: 19.354865\n",
      "====> Epoch: 86 Average loss: 19.2302\n",
      "epoch: 86, beta=0, frac_anom=0.01\n",
      "Train Epoch: 87 [0/8040 (0%)]\tLoss: 19.226701\n",
      "Train Epoch: 87 [1200/8040 (15%)]\tLoss: 19.018166\n",
      "Train Epoch: 87 [2400/8040 (30%)]\tLoss: 19.917952\n",
      "Train Epoch: 87 [3600/8040 (45%)]\tLoss: 19.300301\n",
      "Train Epoch: 87 [4800/8040 (60%)]\tLoss: 18.020996\n",
      "Train Epoch: 87 [6000/8040 (75%)]\tLoss: 19.032456\n",
      "Train Epoch: 87 [7200/8040 (90%)]\tLoss: 19.092090\n",
      "====> Epoch: 87 Average loss: 19.1801\n",
      "epoch: 87, beta=0, frac_anom=0.01\n",
      "Train Epoch: 88 [0/8040 (0%)]\tLoss: 18.822107\n",
      "Train Epoch: 88 [1200/8040 (15%)]\tLoss: 18.325342\n",
      "Train Epoch: 88 [2400/8040 (30%)]\tLoss: 19.344505\n",
      "Train Epoch: 88 [3600/8040 (45%)]\tLoss: 19.505227\n",
      "Train Epoch: 88 [4800/8040 (60%)]\tLoss: 18.297563\n",
      "Train Epoch: 88 [6000/8040 (75%)]\tLoss: 17.871372\n",
      "Train Epoch: 88 [7200/8040 (90%)]\tLoss: 19.507241\n",
      "====> Epoch: 88 Average loss: 19.2082\n",
      "epoch: 88, beta=0, frac_anom=0.01\n",
      "Train Epoch: 89 [0/8040 (0%)]\tLoss: 18.184515\n",
      "Train Epoch: 89 [1200/8040 (15%)]\tLoss: 18.411497\n",
      "Train Epoch: 89 [2400/8040 (30%)]\tLoss: 19.713275\n",
      "Train Epoch: 89 [3600/8040 (45%)]\tLoss: 19.244234\n",
      "Train Epoch: 89 [4800/8040 (60%)]\tLoss: 18.935988\n",
      "Train Epoch: 89 [6000/8040 (75%)]\tLoss: 19.613033\n",
      "Train Epoch: 89 [7200/8040 (90%)]\tLoss: 17.744460\n",
      "====> Epoch: 89 Average loss: 19.2073\n",
      "epoch: 89, beta=0, frac_anom=0.01\n",
      "Train Epoch: 90 [0/8040 (0%)]\tLoss: 18.222498\n",
      "Train Epoch: 90 [1200/8040 (15%)]\tLoss: 18.802096\n",
      "Train Epoch: 90 [2400/8040 (30%)]\tLoss: 20.001054\n",
      "Train Epoch: 90 [3600/8040 (45%)]\tLoss: 19.189891\n",
      "Train Epoch: 90 [4800/8040 (60%)]\tLoss: 18.551933\n",
      "Train Epoch: 90 [6000/8040 (75%)]\tLoss: 18.634908\n",
      "Train Epoch: 90 [7200/8040 (90%)]\tLoss: 18.636053\n",
      "====> Epoch: 90 Average loss: 19.1982\n",
      "epoch: 90, beta=0, frac_anom=0.01\n",
      "Train Epoch: 91 [0/8040 (0%)]\tLoss: 19.065074\n",
      "Train Epoch: 91 [1200/8040 (15%)]\tLoss: 18.895459\n",
      "Train Epoch: 91 [2400/8040 (30%)]\tLoss: 19.497906\n",
      "Train Epoch: 91 [3600/8040 (45%)]\tLoss: 18.565401\n",
      "Train Epoch: 91 [4800/8040 (60%)]\tLoss: 18.524044\n",
      "Train Epoch: 91 [6000/8040 (75%)]\tLoss: 20.307184\n",
      "Train Epoch: 91 [7200/8040 (90%)]\tLoss: 19.236426\n",
      "====> Epoch: 91 Average loss: 19.1470\n",
      "epoch: 91, beta=0, frac_anom=0.01\n",
      "Train Epoch: 92 [0/8040 (0%)]\tLoss: 18.978707\n",
      "Train Epoch: 92 [1200/8040 (15%)]\tLoss: 19.430058\n",
      "Train Epoch: 92 [2400/8040 (30%)]\tLoss: 19.885227\n",
      "Train Epoch: 92 [3600/8040 (45%)]\tLoss: 18.912921\n",
      "Train Epoch: 92 [4800/8040 (60%)]\tLoss: 20.027818\n",
      "Train Epoch: 92 [6000/8040 (75%)]\tLoss: 18.869645\n",
      "Train Epoch: 92 [7200/8040 (90%)]\tLoss: 19.260529\n",
      "====> Epoch: 92 Average loss: 19.1569\n",
      "epoch: 92, beta=0, frac_anom=0.01\n",
      "Train Epoch: 93 [0/8040 (0%)]\tLoss: 19.649748\n",
      "Train Epoch: 93 [1200/8040 (15%)]\tLoss: 19.927759\n",
      "Train Epoch: 93 [2400/8040 (30%)]\tLoss: 19.792289\n",
      "Train Epoch: 93 [3600/8040 (45%)]\tLoss: 18.403746\n",
      "Train Epoch: 93 [4800/8040 (60%)]\tLoss: 18.846122\n",
      "Train Epoch: 93 [6000/8040 (75%)]\tLoss: 17.823999\n",
      "Train Epoch: 93 [7200/8040 (90%)]\tLoss: 19.176912\n",
      "====> Epoch: 93 Average loss: 19.1497\n",
      "epoch: 93, beta=0, frac_anom=0.01\n",
      "Train Epoch: 94 [0/8040 (0%)]\tLoss: 18.725989\n",
      "Train Epoch: 94 [1200/8040 (15%)]\tLoss: 19.179081\n",
      "Train Epoch: 94 [2400/8040 (30%)]\tLoss: 19.794779\n",
      "Train Epoch: 94 [3600/8040 (45%)]\tLoss: 18.078853\n",
      "Train Epoch: 94 [4800/8040 (60%)]\tLoss: 18.063704\n",
      "Train Epoch: 94 [6000/8040 (75%)]\tLoss: 17.976556\n",
      "Train Epoch: 94 [7200/8040 (90%)]\tLoss: 19.274345\n",
      "====> Epoch: 94 Average loss: 19.1063\n",
      "epoch: 94, beta=0, frac_anom=0.01\n",
      "Train Epoch: 95 [0/8040 (0%)]\tLoss: 19.437984\n",
      "Train Epoch: 95 [1200/8040 (15%)]\tLoss: 19.753487\n",
      "Train Epoch: 95 [2400/8040 (30%)]\tLoss: 18.965796\n",
      "Train Epoch: 95 [3600/8040 (45%)]\tLoss: 19.124565\n",
      "Train Epoch: 95 [4800/8040 (60%)]\tLoss: 18.780819\n",
      "Train Epoch: 95 [6000/8040 (75%)]\tLoss: 19.365393\n",
      "Train Epoch: 95 [7200/8040 (90%)]\tLoss: 18.704279\n",
      "====> Epoch: 95 Average loss: 19.1373\n",
      "epoch: 95, beta=0, frac_anom=0.01\n",
      "Train Epoch: 96 [0/8040 (0%)]\tLoss: 19.260280\n",
      "Train Epoch: 96 [1200/8040 (15%)]\tLoss: 19.633628\n",
      "Train Epoch: 96 [2400/8040 (30%)]\tLoss: 19.244991\n",
      "Train Epoch: 96 [3600/8040 (45%)]\tLoss: 19.582888\n",
      "Train Epoch: 96 [4800/8040 (60%)]\tLoss: 18.286096\n",
      "Train Epoch: 96 [6000/8040 (75%)]\tLoss: 18.803400\n",
      "Train Epoch: 96 [7200/8040 (90%)]\tLoss: 19.443339\n",
      "====> Epoch: 96 Average loss: 19.1625\n",
      "epoch: 96, beta=0, frac_anom=0.01\n",
      "Train Epoch: 97 [0/8040 (0%)]\tLoss: 18.936743\n",
      "Train Epoch: 97 [1200/8040 (15%)]\tLoss: 19.498708\n",
      "Train Epoch: 97 [2400/8040 (30%)]\tLoss: 18.269725\n",
      "Train Epoch: 97 [3600/8040 (45%)]\tLoss: 18.412109\n",
      "Train Epoch: 97 [4800/8040 (60%)]\tLoss: 18.451945\n",
      "Train Epoch: 97 [6000/8040 (75%)]\tLoss: 18.626129\n",
      "Train Epoch: 97 [7200/8040 (90%)]\tLoss: 19.080347\n",
      "====> Epoch: 97 Average loss: 19.0990\n",
      "epoch: 97, beta=0, frac_anom=0.01\n",
      "Train Epoch: 98 [0/8040 (0%)]\tLoss: 19.589091\n",
      "Train Epoch: 98 [1200/8040 (15%)]\tLoss: 19.405424\n",
      "Train Epoch: 98 [2400/8040 (30%)]\tLoss: 19.297420\n",
      "Train Epoch: 98 [3600/8040 (45%)]\tLoss: 18.589213\n",
      "Train Epoch: 98 [4800/8040 (60%)]\tLoss: 19.895211\n",
      "Train Epoch: 98 [6000/8040 (75%)]\tLoss: 19.949461\n",
      "Train Epoch: 98 [7200/8040 (90%)]\tLoss: 18.649559\n",
      "====> Epoch: 98 Average loss: 19.1877\n",
      "epoch: 98, beta=0, frac_anom=0.01\n",
      "Train Epoch: 99 [0/8040 (0%)]\tLoss: 19.249699\n",
      "Train Epoch: 99 [1200/8040 (15%)]\tLoss: 19.020593\n",
      "Train Epoch: 99 [2400/8040 (30%)]\tLoss: 18.921295\n",
      "Train Epoch: 99 [3600/8040 (45%)]\tLoss: 18.349028\n",
      "Train Epoch: 99 [4800/8040 (60%)]\tLoss: 19.120304\n",
      "Train Epoch: 99 [6000/8040 (75%)]\tLoss: 18.949451\n",
      "Train Epoch: 99 [7200/8040 (90%)]\tLoss: 18.822552\n",
      "====> Epoch: 99 Average loss: 19.1504\n",
      "epoch: 99, beta=0, frac_anom=0.01\n",
      "Train Epoch: 100 [0/8040 (0%)]\tLoss: 18.831228\n",
      "Train Epoch: 100 [1200/8040 (15%)]\tLoss: 19.181578\n",
      "Train Epoch: 100 [2400/8040 (30%)]\tLoss: 17.685974\n",
      "Train Epoch: 100 [3600/8040 (45%)]\tLoss: 19.256368\n",
      "Train Epoch: 100 [4800/8040 (60%)]\tLoss: 19.130304\n",
      "Train Epoch: 100 [6000/8040 (75%)]\tLoss: 18.562150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 100 [7200/8040 (90%)]\tLoss: 19.750378\n",
      "====> Epoch: 100 Average loss: 19.1568\n",
      "epoch: 100, beta=0, frac_anom=0.01\n",
      "Train Epoch: 101 [0/8040 (0%)]\tLoss: 20.387962\n",
      "Train Epoch: 101 [1200/8040 (15%)]\tLoss: 19.609336\n",
      "Train Epoch: 101 [2400/8040 (30%)]\tLoss: 19.406384\n",
      "Train Epoch: 101 [3600/8040 (45%)]\tLoss: 19.652690\n",
      "Train Epoch: 101 [4800/8040 (60%)]\tLoss: 18.786564\n",
      "Train Epoch: 101 [6000/8040 (75%)]\tLoss: 17.790588\n",
      "Train Epoch: 101 [7200/8040 (90%)]\tLoss: 19.160260\n",
      "====> Epoch: 101 Average loss: 19.0891\n",
      "epoch: 101, beta=0, frac_anom=0.01\n",
      "Train Epoch: 102 [0/8040 (0%)]\tLoss: 18.062555\n",
      "Train Epoch: 102 [1200/8040 (15%)]\tLoss: 19.721659\n",
      "Train Epoch: 102 [2400/8040 (30%)]\tLoss: 20.051941\n",
      "Train Epoch: 102 [3600/8040 (45%)]\tLoss: 19.370217\n",
      "Train Epoch: 102 [4800/8040 (60%)]\tLoss: 18.227661\n",
      "Train Epoch: 102 [6000/8040 (75%)]\tLoss: 18.223820\n",
      "Train Epoch: 102 [7200/8040 (90%)]\tLoss: 19.053776\n",
      "====> Epoch: 102 Average loss: 19.0568\n",
      "epoch: 102, beta=0, frac_anom=0.01\n",
      "Train Epoch: 103 [0/8040 (0%)]\tLoss: 20.418099\n",
      "Train Epoch: 103 [1200/8040 (15%)]\tLoss: 19.003094\n",
      "Train Epoch: 103 [2400/8040 (30%)]\tLoss: 19.107747\n",
      "Train Epoch: 103 [3600/8040 (45%)]\tLoss: 19.725387\n",
      "Train Epoch: 103 [4800/8040 (60%)]\tLoss: 18.867716\n",
      "Train Epoch: 103 [6000/8040 (75%)]\tLoss: 18.266280\n",
      "Train Epoch: 103 [7200/8040 (90%)]\tLoss: 18.935734\n",
      "====> Epoch: 103 Average loss: 19.0749\n",
      "epoch: 103, beta=0, frac_anom=0.01\n",
      "Train Epoch: 104 [0/8040 (0%)]\tLoss: 19.256864\n",
      "Train Epoch: 104 [1200/8040 (15%)]\tLoss: 18.512937\n",
      "Train Epoch: 104 [2400/8040 (30%)]\tLoss: 18.804256\n",
      "Train Epoch: 104 [3600/8040 (45%)]\tLoss: 18.884953\n",
      "Train Epoch: 104 [4800/8040 (60%)]\tLoss: 18.779215\n",
      "Train Epoch: 104 [6000/8040 (75%)]\tLoss: 19.740706\n",
      "Train Epoch: 104 [7200/8040 (90%)]\tLoss: 19.285146\n",
      "====> Epoch: 104 Average loss: 19.0786\n",
      "epoch: 104, beta=0, frac_anom=0.01\n",
      "Train Epoch: 105 [0/8040 (0%)]\tLoss: 18.358010\n",
      "Train Epoch: 105 [1200/8040 (15%)]\tLoss: 19.113304\n",
      "Train Epoch: 105 [2400/8040 (30%)]\tLoss: 19.181222\n",
      "Train Epoch: 105 [3600/8040 (45%)]\tLoss: 18.967407\n",
      "Train Epoch: 105 [4800/8040 (60%)]\tLoss: 18.364695\n",
      "Train Epoch: 105 [6000/8040 (75%)]\tLoss: 18.866101\n",
      "Train Epoch: 105 [7200/8040 (90%)]\tLoss: 20.234806\n",
      "====> Epoch: 105 Average loss: 19.0770\n",
      "epoch: 105, beta=0, frac_anom=0.01\n",
      "Train Epoch: 106 [0/8040 (0%)]\tLoss: 19.974087\n",
      "Train Epoch: 106 [1200/8040 (15%)]\tLoss: 19.202618\n",
      "Train Epoch: 106 [2400/8040 (30%)]\tLoss: 19.381344\n",
      "Train Epoch: 106 [3600/8040 (45%)]\tLoss: 19.363548\n",
      "Train Epoch: 106 [4800/8040 (60%)]\tLoss: 19.386279\n",
      "Train Epoch: 106 [6000/8040 (75%)]\tLoss: 19.721566\n",
      "Train Epoch: 106 [7200/8040 (90%)]\tLoss: 19.398794\n",
      "====> Epoch: 106 Average loss: 19.0913\n",
      "epoch: 106, beta=0, frac_anom=0.01\n",
      "Train Epoch: 107 [0/8040 (0%)]\tLoss: 19.419476\n",
      "Train Epoch: 107 [1200/8040 (15%)]\tLoss: 18.700743\n",
      "Train Epoch: 107 [2400/8040 (30%)]\tLoss: 19.050798\n",
      "Train Epoch: 107 [3600/8040 (45%)]\tLoss: 19.397457\n",
      "Train Epoch: 107 [4800/8040 (60%)]\tLoss: 18.303528\n",
      "Train Epoch: 107 [6000/8040 (75%)]\tLoss: 19.763660\n",
      "Train Epoch: 107 [7200/8040 (90%)]\tLoss: 19.422192\n",
      "====> Epoch: 107 Average loss: 19.0682\n",
      "epoch: 107, beta=0, frac_anom=0.01\n",
      "Train Epoch: 108 [0/8040 (0%)]\tLoss: 18.824359\n",
      "Train Epoch: 108 [1200/8040 (15%)]\tLoss: 18.785374\n",
      "Train Epoch: 108 [2400/8040 (30%)]\tLoss: 18.836300\n",
      "Train Epoch: 108 [3600/8040 (45%)]\tLoss: 18.686757\n",
      "Train Epoch: 108 [4800/8040 (60%)]\tLoss: 19.904073\n",
      "Train Epoch: 108 [6000/8040 (75%)]\tLoss: 19.478005\n",
      "Train Epoch: 108 [7200/8040 (90%)]\tLoss: 19.642432\n",
      "====> Epoch: 108 Average loss: 19.0853\n",
      "epoch: 108, beta=0, frac_anom=0.01\n",
      "Train Epoch: 109 [0/8040 (0%)]\tLoss: 19.596263\n",
      "Train Epoch: 109 [1200/8040 (15%)]\tLoss: 19.514882\n",
      "Train Epoch: 109 [2400/8040 (30%)]\tLoss: 19.109648\n",
      "Train Epoch: 109 [3600/8040 (45%)]\tLoss: 18.635524\n",
      "Train Epoch: 109 [4800/8040 (60%)]\tLoss: 19.561422\n",
      "Train Epoch: 109 [6000/8040 (75%)]\tLoss: 19.125395\n",
      "Train Epoch: 109 [7200/8040 (90%)]\tLoss: 18.849483\n",
      "====> Epoch: 109 Average loss: 19.1149\n",
      "epoch: 109, beta=0, frac_anom=0.01\n",
      "Train Epoch: 110 [0/8040 (0%)]\tLoss: 18.444584\n",
      "Train Epoch: 110 [1200/8040 (15%)]\tLoss: 18.970217\n",
      "Train Epoch: 110 [2400/8040 (30%)]\tLoss: 19.098242\n",
      "Train Epoch: 110 [3600/8040 (45%)]\tLoss: 19.138590\n",
      "Train Epoch: 110 [4800/8040 (60%)]\tLoss: 18.685620\n",
      "Train Epoch: 110 [6000/8040 (75%)]\tLoss: 19.546879\n",
      "Train Epoch: 110 [7200/8040 (90%)]\tLoss: 19.008614\n",
      "====> Epoch: 110 Average loss: 19.0306\n",
      "epoch: 110, beta=0, frac_anom=0.01\n",
      "Train Epoch: 111 [0/8040 (0%)]\tLoss: 19.916097\n",
      "Train Epoch: 111 [1200/8040 (15%)]\tLoss: 18.604848\n",
      "Train Epoch: 111 [2400/8040 (30%)]\tLoss: 19.205499\n",
      "Train Epoch: 111 [3600/8040 (45%)]\tLoss: 18.429584\n",
      "Train Epoch: 111 [4800/8040 (60%)]\tLoss: 18.865173\n",
      "Train Epoch: 111 [6000/8040 (75%)]\tLoss: 18.681038\n",
      "Train Epoch: 111 [7200/8040 (90%)]\tLoss: 17.921859\n",
      "====> Epoch: 111 Average loss: 19.0336\n",
      "epoch: 111, beta=0, frac_anom=0.01\n",
      "Train Epoch: 112 [0/8040 (0%)]\tLoss: 19.206488\n",
      "Train Epoch: 112 [1200/8040 (15%)]\tLoss: 19.061064\n",
      "Train Epoch: 112 [2400/8040 (30%)]\tLoss: 19.426111\n",
      "Train Epoch: 112 [3600/8040 (45%)]\tLoss: 18.441410\n",
      "Train Epoch: 112 [4800/8040 (60%)]\tLoss: 19.345245\n",
      "Train Epoch: 112 [6000/8040 (75%)]\tLoss: 18.902403\n",
      "Train Epoch: 112 [7200/8040 (90%)]\tLoss: 17.885520\n",
      "====> Epoch: 112 Average loss: 19.0263\n",
      "epoch: 112, beta=0, frac_anom=0.01\n",
      "Train Epoch: 113 [0/8040 (0%)]\tLoss: 19.244759\n",
      "Train Epoch: 113 [1200/8040 (15%)]\tLoss: 18.763534\n",
      "Train Epoch: 113 [2400/8040 (30%)]\tLoss: 20.065889\n",
      "Train Epoch: 113 [3600/8040 (45%)]\tLoss: 18.597917\n",
      "Train Epoch: 113 [4800/8040 (60%)]\tLoss: 18.438847\n",
      "Train Epoch: 113 [6000/8040 (75%)]\tLoss: 19.447872\n",
      "Train Epoch: 113 [7200/8040 (90%)]\tLoss: 19.003739\n",
      "====> Epoch: 113 Average loss: 19.0863\n",
      "epoch: 113, beta=0, frac_anom=0.01\n",
      "Train Epoch: 114 [0/8040 (0%)]\tLoss: 18.156293\n",
      "Train Epoch: 114 [1200/8040 (15%)]\tLoss: 19.217200\n",
      "Train Epoch: 114 [2400/8040 (30%)]\tLoss: 19.566097\n",
      "Train Epoch: 114 [3600/8040 (45%)]\tLoss: 19.872864\n",
      "Train Epoch: 114 [4800/8040 (60%)]\tLoss: 19.468915\n",
      "Train Epoch: 114 [6000/8040 (75%)]\tLoss: 19.051851\n",
      "Train Epoch: 114 [7200/8040 (90%)]\tLoss: 18.711993\n",
      "====> Epoch: 114 Average loss: 18.9890\n",
      "epoch: 114, beta=0, frac_anom=0.01\n",
      "Train Epoch: 115 [0/8040 (0%)]\tLoss: 19.303731\n",
      "Train Epoch: 115 [1200/8040 (15%)]\tLoss: 18.205320\n",
      "Train Epoch: 115 [2400/8040 (30%)]\tLoss: 18.321322\n",
      "Train Epoch: 115 [3600/8040 (45%)]\tLoss: 18.812819\n",
      "Train Epoch: 115 [4800/8040 (60%)]\tLoss: 18.458358\n",
      "Train Epoch: 115 [6000/8040 (75%)]\tLoss: 19.533693\n",
      "Train Epoch: 115 [7200/8040 (90%)]\tLoss: 18.107532\n",
      "====> Epoch: 115 Average loss: 19.0340\n",
      "epoch: 115, beta=0, frac_anom=0.01\n",
      "Train Epoch: 116 [0/8040 (0%)]\tLoss: 18.706476\n",
      "Train Epoch: 116 [1200/8040 (15%)]\tLoss: 18.975822\n",
      "Train Epoch: 116 [2400/8040 (30%)]\tLoss: 19.036772\n",
      "Train Epoch: 116 [3600/8040 (45%)]\tLoss: 19.190413\n",
      "Train Epoch: 116 [4800/8040 (60%)]\tLoss: 18.330577\n",
      "Train Epoch: 116 [6000/8040 (75%)]\tLoss: 19.648523\n",
      "Train Epoch: 116 [7200/8040 (90%)]\tLoss: 18.468437\n",
      "====> Epoch: 116 Average loss: 19.0432\n",
      "epoch: 116, beta=0, frac_anom=0.01\n",
      "Train Epoch: 117 [0/8040 (0%)]\tLoss: 18.421202\n",
      "Train Epoch: 117 [1200/8040 (15%)]\tLoss: 18.874312\n",
      "Train Epoch: 117 [2400/8040 (30%)]\tLoss: 18.441203\n",
      "Train Epoch: 117 [3600/8040 (45%)]\tLoss: 19.344389\n",
      "Train Epoch: 117 [4800/8040 (60%)]\tLoss: 19.937205\n",
      "Train Epoch: 117 [6000/8040 (75%)]\tLoss: 17.661987\n",
      "Train Epoch: 117 [7200/8040 (90%)]\tLoss: 18.152818\n",
      "====> Epoch: 117 Average loss: 18.9991\n",
      "epoch: 117, beta=0, frac_anom=0.01\n",
      "Train Epoch: 118 [0/8040 (0%)]\tLoss: 18.551463\n",
      "Train Epoch: 118 [1200/8040 (15%)]\tLoss: 19.313310\n",
      "Train Epoch: 118 [2400/8040 (30%)]\tLoss: 19.170764\n",
      "Train Epoch: 118 [3600/8040 (45%)]\tLoss: 19.249569\n",
      "Train Epoch: 118 [4800/8040 (60%)]\tLoss: 19.403219\n",
      "Train Epoch: 118 [6000/8040 (75%)]\tLoss: 19.575985\n",
      "Train Epoch: 118 [7200/8040 (90%)]\tLoss: 18.397284\n",
      "====> Epoch: 118 Average loss: 19.0563\n",
      "epoch: 118, beta=0, frac_anom=0.01\n",
      "Train Epoch: 119 [0/8040 (0%)]\tLoss: 19.223364\n",
      "Train Epoch: 119 [1200/8040 (15%)]\tLoss: 19.684442\n",
      "Train Epoch: 119 [2400/8040 (30%)]\tLoss: 18.902856\n",
      "Train Epoch: 119 [3600/8040 (45%)]\tLoss: 18.948934\n",
      "Train Epoch: 119 [4800/8040 (60%)]\tLoss: 19.572935\n",
      "Train Epoch: 119 [6000/8040 (75%)]\tLoss: 18.678467\n",
      "Train Epoch: 119 [7200/8040 (90%)]\tLoss: 18.377136\n",
      "====> Epoch: 119 Average loss: 19.0215\n",
      "epoch: 119, beta=0, frac_anom=0.01\n",
      "Train Epoch: 120 [0/8040 (0%)]\tLoss: 18.498608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 120 [1200/8040 (15%)]\tLoss: 19.600321\n",
      "Train Epoch: 120 [2400/8040 (30%)]\tLoss: 19.138188\n",
      "Train Epoch: 120 [3600/8040 (45%)]\tLoss: 19.624782\n",
      "Train Epoch: 120 [4800/8040 (60%)]\tLoss: 18.950385\n",
      "Train Epoch: 120 [6000/8040 (75%)]\tLoss: 19.585384\n",
      "Train Epoch: 120 [7200/8040 (90%)]\tLoss: 18.840112\n",
      "====> Epoch: 120 Average loss: 19.0380\n",
      "epoch: 120, beta=0, frac_anom=0.01\n",
      "Train Epoch: 121 [0/8040 (0%)]\tLoss: 17.511593\n",
      "Train Epoch: 121 [1200/8040 (15%)]\tLoss: 17.953109\n",
      "Train Epoch: 121 [2400/8040 (30%)]\tLoss: 18.617326\n",
      "Train Epoch: 121 [3600/8040 (45%)]\tLoss: 18.891477\n",
      "Train Epoch: 121 [4800/8040 (60%)]\tLoss: 19.697945\n",
      "Train Epoch: 121 [6000/8040 (75%)]\tLoss: 17.342527\n",
      "Train Epoch: 121 [7200/8040 (90%)]\tLoss: 18.864117\n",
      "====> Epoch: 121 Average loss: 19.0133\n",
      "epoch: 121, beta=0, frac_anom=0.01\n",
      "Train Epoch: 122 [0/8040 (0%)]\tLoss: 19.166473\n",
      "Train Epoch: 122 [1200/8040 (15%)]\tLoss: 18.587646\n",
      "Train Epoch: 122 [2400/8040 (30%)]\tLoss: 19.011507\n",
      "Train Epoch: 122 [3600/8040 (45%)]\tLoss: 18.876282\n",
      "Train Epoch: 122 [4800/8040 (60%)]\tLoss: 19.483974\n",
      "Train Epoch: 122 [6000/8040 (75%)]\tLoss: 19.705107\n",
      "Train Epoch: 122 [7200/8040 (90%)]\tLoss: 19.707298\n",
      "====> Epoch: 122 Average loss: 18.9906\n",
      "epoch: 122, beta=0, frac_anom=0.01\n",
      "Train Epoch: 123 [0/8040 (0%)]\tLoss: 18.614543\n",
      "Train Epoch: 123 [1200/8040 (15%)]\tLoss: 17.990920\n",
      "Train Epoch: 123 [2400/8040 (30%)]\tLoss: 18.054167\n",
      "Train Epoch: 123 [3600/8040 (45%)]\tLoss: 19.316311\n",
      "Train Epoch: 123 [4800/8040 (60%)]\tLoss: 18.220707\n",
      "Train Epoch: 123 [6000/8040 (75%)]\tLoss: 17.925562\n",
      "Train Epoch: 123 [7200/8040 (90%)]\tLoss: 19.319401\n",
      "====> Epoch: 123 Average loss: 18.9691\n",
      "epoch: 123, beta=0, frac_anom=0.01\n",
      "Train Epoch: 124 [0/8040 (0%)]\tLoss: 18.563053\n",
      "Train Epoch: 124 [1200/8040 (15%)]\tLoss: 20.456425\n",
      "Train Epoch: 124 [2400/8040 (30%)]\tLoss: 18.571861\n",
      "Train Epoch: 124 [3600/8040 (45%)]\tLoss: 17.802124\n",
      "Train Epoch: 124 [4800/8040 (60%)]\tLoss: 18.301424\n",
      "Train Epoch: 124 [6000/8040 (75%)]\tLoss: 18.847131\n",
      "Train Epoch: 124 [7200/8040 (90%)]\tLoss: 18.041846\n",
      "====> Epoch: 124 Average loss: 18.9731\n",
      "epoch: 124, beta=0, frac_anom=0.01\n",
      "Train Epoch: 125 [0/8040 (0%)]\tLoss: 19.780237\n",
      "Train Epoch: 125 [1200/8040 (15%)]\tLoss: 19.446263\n",
      "Train Epoch: 125 [2400/8040 (30%)]\tLoss: 19.033712\n",
      "Train Epoch: 125 [3600/8040 (45%)]\tLoss: 18.743498\n",
      "Train Epoch: 125 [4800/8040 (60%)]\tLoss: 19.361051\n",
      "Train Epoch: 125 [6000/8040 (75%)]\tLoss: 18.670662\n",
      "Train Epoch: 125 [7200/8040 (90%)]\tLoss: 18.860801\n",
      "====> Epoch: 125 Average loss: 18.9307\n",
      "epoch: 125, beta=0, frac_anom=0.01\n",
      "Train Epoch: 126 [0/8040 (0%)]\tLoss: 19.399304\n",
      "Train Epoch: 126 [1200/8040 (15%)]\tLoss: 18.494731\n",
      "Train Epoch: 126 [2400/8040 (30%)]\tLoss: 18.744975\n",
      "Train Epoch: 126 [3600/8040 (45%)]\tLoss: 19.416416\n",
      "Train Epoch: 126 [4800/8040 (60%)]\tLoss: 18.432058\n",
      "Train Epoch: 126 [6000/8040 (75%)]\tLoss: 18.971126\n",
      "Train Epoch: 126 [7200/8040 (90%)]\tLoss: 17.634229\n",
      "====> Epoch: 126 Average loss: 18.9868\n",
      "epoch: 126, beta=0, frac_anom=0.01\n",
      "Train Epoch: 127 [0/8040 (0%)]\tLoss: 18.769281\n",
      "Train Epoch: 127 [1200/8040 (15%)]\tLoss: 19.616091\n",
      "Train Epoch: 127 [2400/8040 (30%)]\tLoss: 19.301501\n",
      "Train Epoch: 127 [3600/8040 (45%)]\tLoss: 18.440560\n",
      "Train Epoch: 127 [4800/8040 (60%)]\tLoss: 19.048269\n",
      "Train Epoch: 127 [6000/8040 (75%)]\tLoss: 19.519609\n",
      "Train Epoch: 127 [7200/8040 (90%)]\tLoss: 19.285488\n",
      "====> Epoch: 127 Average loss: 18.9555\n",
      "epoch: 127, beta=0, frac_anom=0.01\n",
      "Train Epoch: 128 [0/8040 (0%)]\tLoss: 20.042885\n",
      "Train Epoch: 128 [1200/8040 (15%)]\tLoss: 19.198730\n",
      "Train Epoch: 128 [2400/8040 (30%)]\tLoss: 18.237972\n",
      "Train Epoch: 128 [3600/8040 (45%)]\tLoss: 17.994232\n",
      "Train Epoch: 128 [4800/8040 (60%)]\tLoss: 18.894670\n",
      "Train Epoch: 128 [6000/8040 (75%)]\tLoss: 19.184039\n",
      "Train Epoch: 128 [7200/8040 (90%)]\tLoss: 19.602037\n",
      "====> Epoch: 128 Average loss: 18.9628\n",
      "epoch: 128, beta=0, frac_anom=0.01\n",
      "Train Epoch: 129 [0/8040 (0%)]\tLoss: 18.292240\n",
      "Train Epoch: 129 [1200/8040 (15%)]\tLoss: 19.048794\n",
      "Train Epoch: 129 [2400/8040 (30%)]\tLoss: 18.564630\n",
      "Train Epoch: 129 [3600/8040 (45%)]\tLoss: 19.154427\n",
      "Train Epoch: 129 [4800/8040 (60%)]\tLoss: 18.973836\n",
      "Train Epoch: 129 [6000/8040 (75%)]\tLoss: 19.114191\n",
      "Train Epoch: 129 [7200/8040 (90%)]\tLoss: 19.144169\n",
      "====> Epoch: 129 Average loss: 18.9522\n",
      "epoch: 129, beta=0, frac_anom=0.01\n",
      "Train Epoch: 130 [0/8040 (0%)]\tLoss: 18.842236\n",
      "Train Epoch: 130 [1200/8040 (15%)]\tLoss: 18.993164\n",
      "Train Epoch: 130 [2400/8040 (30%)]\tLoss: 19.315271\n",
      "Train Epoch: 130 [3600/8040 (45%)]\tLoss: 19.836784\n",
      "Train Epoch: 130 [4800/8040 (60%)]\tLoss: 18.501864\n",
      "Train Epoch: 130 [6000/8040 (75%)]\tLoss: 18.777710\n",
      "Train Epoch: 130 [7200/8040 (90%)]\tLoss: 18.530939\n",
      "====> Epoch: 130 Average loss: 18.9494\n",
      "epoch: 130, beta=0, frac_anom=0.01\n",
      "Train Epoch: 131 [0/8040 (0%)]\tLoss: 18.206783\n",
      "Train Epoch: 131 [1200/8040 (15%)]\tLoss: 18.879671\n",
      "Train Epoch: 131 [2400/8040 (30%)]\tLoss: 18.718797\n",
      "Train Epoch: 131 [3600/8040 (45%)]\tLoss: 19.788556\n",
      "Train Epoch: 131 [4800/8040 (60%)]\tLoss: 18.878819\n",
      "Train Epoch: 131 [6000/8040 (75%)]\tLoss: 18.559450\n",
      "Train Epoch: 131 [7200/8040 (90%)]\tLoss: 17.925570\n",
      "====> Epoch: 131 Average loss: 18.9528\n",
      "epoch: 131, beta=0, frac_anom=0.01\n",
      "Train Epoch: 132 [0/8040 (0%)]\tLoss: 19.326481\n",
      "Train Epoch: 132 [1200/8040 (15%)]\tLoss: 19.009167\n",
      "Train Epoch: 132 [2400/8040 (30%)]\tLoss: 18.216223\n",
      "Train Epoch: 132 [3600/8040 (45%)]\tLoss: 19.449524\n",
      "Train Epoch: 132 [4800/8040 (60%)]\tLoss: 19.287118\n",
      "Train Epoch: 132 [6000/8040 (75%)]\tLoss: 19.227163\n",
      "Train Epoch: 132 [7200/8040 (90%)]\tLoss: 19.512476\n",
      "====> Epoch: 132 Average loss: 18.9643\n",
      "epoch: 132, beta=0, frac_anom=0.01\n",
      "Train Epoch: 133 [0/8040 (0%)]\tLoss: 18.867729\n",
      "Train Epoch: 133 [1200/8040 (15%)]\tLoss: 18.469029\n",
      "Train Epoch: 133 [2400/8040 (30%)]\tLoss: 19.514742\n",
      "Train Epoch: 133 [3600/8040 (45%)]\tLoss: 19.378251\n",
      "Train Epoch: 133 [4800/8040 (60%)]\tLoss: 19.464343\n",
      "Train Epoch: 133 [6000/8040 (75%)]\tLoss: 19.758913\n",
      "Train Epoch: 133 [7200/8040 (90%)]\tLoss: 18.732306\n",
      "====> Epoch: 133 Average loss: 18.9199\n",
      "epoch: 133, beta=0, frac_anom=0.01\n",
      "Train Epoch: 134 [0/8040 (0%)]\tLoss: 19.707682\n",
      "Train Epoch: 134 [1200/8040 (15%)]\tLoss: 19.399622\n",
      "Train Epoch: 134 [2400/8040 (30%)]\tLoss: 18.457351\n",
      "Train Epoch: 134 [3600/8040 (45%)]\tLoss: 18.941862\n",
      "Train Epoch: 134 [4800/8040 (60%)]\tLoss: 18.029909\n",
      "Train Epoch: 134 [6000/8040 (75%)]\tLoss: 17.830550\n",
      "Train Epoch: 134 [7200/8040 (90%)]\tLoss: 19.713566\n",
      "====> Epoch: 134 Average loss: 18.9087\n",
      "epoch: 134, beta=0, frac_anom=0.01\n",
      "Train Epoch: 135 [0/8040 (0%)]\tLoss: 18.969519\n",
      "Train Epoch: 135 [1200/8040 (15%)]\tLoss: 18.628564\n",
      "Train Epoch: 135 [2400/8040 (30%)]\tLoss: 19.634058\n",
      "Train Epoch: 135 [3600/8040 (45%)]\tLoss: 19.409229\n",
      "Train Epoch: 135 [4800/8040 (60%)]\tLoss: 18.716809\n",
      "Train Epoch: 135 [6000/8040 (75%)]\tLoss: 18.771694\n",
      "Train Epoch: 135 [7200/8040 (90%)]\tLoss: 19.662767\n",
      "====> Epoch: 135 Average loss: 18.9781\n",
      "epoch: 135, beta=0, frac_anom=0.01\n",
      "Train Epoch: 136 [0/8040 (0%)]\tLoss: 19.254903\n",
      "Train Epoch: 136 [1200/8040 (15%)]\tLoss: 18.739140\n",
      "Train Epoch: 136 [2400/8040 (30%)]\tLoss: 19.099491\n",
      "Train Epoch: 136 [3600/8040 (45%)]\tLoss: 18.930750\n",
      "Train Epoch: 136 [4800/8040 (60%)]\tLoss: 19.027954\n",
      "Train Epoch: 136 [6000/8040 (75%)]\tLoss: 19.189842\n",
      "Train Epoch: 136 [7200/8040 (90%)]\tLoss: 18.859900\n",
      "====> Epoch: 136 Average loss: 18.9287\n",
      "epoch: 136, beta=0, frac_anom=0.01\n",
      "Train Epoch: 137 [0/8040 (0%)]\tLoss: 20.404854\n",
      "Train Epoch: 137 [1200/8040 (15%)]\tLoss: 18.340271\n",
      "Train Epoch: 137 [2400/8040 (30%)]\tLoss: 19.201693\n",
      "Train Epoch: 137 [3600/8040 (45%)]\tLoss: 19.492609\n",
      "Train Epoch: 137 [4800/8040 (60%)]\tLoss: 19.527513\n",
      "Train Epoch: 137 [6000/8040 (75%)]\tLoss: 19.731746\n",
      "Train Epoch: 137 [7200/8040 (90%)]\tLoss: 18.434847\n",
      "====> Epoch: 137 Average loss: 18.9564\n",
      "epoch: 137, beta=0, frac_anom=0.01\n",
      "Train Epoch: 138 [0/8040 (0%)]\tLoss: 19.268984\n",
      "Train Epoch: 138 [1200/8040 (15%)]\tLoss: 19.336580\n",
      "Train Epoch: 138 [2400/8040 (30%)]\tLoss: 19.246141\n",
      "Train Epoch: 138 [3600/8040 (45%)]\tLoss: 17.799428\n",
      "Train Epoch: 138 [4800/8040 (60%)]\tLoss: 19.951149\n",
      "Train Epoch: 138 [6000/8040 (75%)]\tLoss: 19.033122\n",
      "Train Epoch: 138 [7200/8040 (90%)]\tLoss: 19.086709\n",
      "====> Epoch: 138 Average loss: 18.9270\n",
      "epoch: 138, beta=0, frac_anom=0.01\n",
      "Train Epoch: 139 [0/8040 (0%)]\tLoss: 18.934672\n",
      "Train Epoch: 139 [1200/8040 (15%)]\tLoss: 19.047571\n",
      "Train Epoch: 139 [2400/8040 (30%)]\tLoss: 18.327771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 139 [3600/8040 (45%)]\tLoss: 19.082815\n",
      "Train Epoch: 139 [4800/8040 (60%)]\tLoss: 19.311283\n",
      "Train Epoch: 139 [6000/8040 (75%)]\tLoss: 19.101335\n",
      "Train Epoch: 139 [7200/8040 (90%)]\tLoss: 19.202230\n",
      "====> Epoch: 139 Average loss: 18.9412\n",
      "epoch: 139, beta=0, frac_anom=0.01\n",
      "Train Epoch: 140 [0/8040 (0%)]\tLoss: 20.076904\n",
      "Train Epoch: 140 [1200/8040 (15%)]\tLoss: 20.311497\n",
      "Train Epoch: 140 [2400/8040 (30%)]\tLoss: 19.332963\n",
      "Train Epoch: 140 [3600/8040 (45%)]\tLoss: 18.424959\n",
      "Train Epoch: 140 [4800/8040 (60%)]\tLoss: 19.635105\n",
      "Train Epoch: 140 [6000/8040 (75%)]\tLoss: 18.631301\n",
      "Train Epoch: 140 [7200/8040 (90%)]\tLoss: 19.609707\n",
      "====> Epoch: 140 Average loss: 18.9418\n",
      "epoch: 140, beta=0, frac_anom=0.01\n",
      "Train Epoch: 141 [0/8040 (0%)]\tLoss: 18.266154\n",
      "Train Epoch: 141 [1200/8040 (15%)]\tLoss: 18.762360\n",
      "Train Epoch: 141 [2400/8040 (30%)]\tLoss: 18.342285\n",
      "Train Epoch: 141 [3600/8040 (45%)]\tLoss: 19.006429\n",
      "Train Epoch: 141 [4800/8040 (60%)]\tLoss: 18.159601\n",
      "Train Epoch: 141 [6000/8040 (75%)]\tLoss: 18.819360\n",
      "Train Epoch: 141 [7200/8040 (90%)]\tLoss: 18.230607\n",
      "====> Epoch: 141 Average loss: 18.8667\n",
      "epoch: 141, beta=0, frac_anom=0.01\n",
      "Train Epoch: 142 [0/8040 (0%)]\tLoss: 18.811308\n",
      "Train Epoch: 142 [1200/8040 (15%)]\tLoss: 19.777071\n",
      "Train Epoch: 142 [2400/8040 (30%)]\tLoss: 19.044094\n",
      "Train Epoch: 142 [3600/8040 (45%)]\tLoss: 18.287911\n",
      "Train Epoch: 142 [4800/8040 (60%)]\tLoss: 18.749589\n",
      "Train Epoch: 142 [6000/8040 (75%)]\tLoss: 18.826689\n",
      "Train Epoch: 142 [7200/8040 (90%)]\tLoss: 18.884216\n",
      "====> Epoch: 142 Average loss: 18.8705\n",
      "epoch: 142, beta=0, frac_anom=0.01\n",
      "Train Epoch: 143 [0/8040 (0%)]\tLoss: 18.050073\n",
      "Train Epoch: 143 [1200/8040 (15%)]\tLoss: 19.149862\n",
      "Train Epoch: 143 [2400/8040 (30%)]\tLoss: 18.014337\n",
      "Train Epoch: 143 [3600/8040 (45%)]\tLoss: 18.714852\n",
      "Train Epoch: 143 [4800/8040 (60%)]\tLoss: 19.434465\n",
      "Train Epoch: 143 [6000/8040 (75%)]\tLoss: 19.786981\n",
      "Train Epoch: 143 [7200/8040 (90%)]\tLoss: 18.782874\n",
      "====> Epoch: 143 Average loss: 18.8674\n",
      "epoch: 143, beta=0, frac_anom=0.01\n",
      "Train Epoch: 144 [0/8040 (0%)]\tLoss: 18.330477\n",
      "Train Epoch: 144 [1200/8040 (15%)]\tLoss: 19.481205\n",
      "Train Epoch: 144 [2400/8040 (30%)]\tLoss: 18.859977\n",
      "Train Epoch: 144 [3600/8040 (45%)]\tLoss: 19.330819\n",
      "Train Epoch: 144 [4800/8040 (60%)]\tLoss: 18.861123\n",
      "Train Epoch: 144 [6000/8040 (75%)]\tLoss: 19.459131\n",
      "Train Epoch: 144 [7200/8040 (90%)]\tLoss: 19.622406\n",
      "====> Epoch: 144 Average loss: 18.8869\n",
      "epoch: 144, beta=0, frac_anom=0.01\n",
      "Train Epoch: 145 [0/8040 (0%)]\tLoss: 18.484312\n",
      "Train Epoch: 145 [1200/8040 (15%)]\tLoss: 18.097463\n",
      "Train Epoch: 145 [2400/8040 (30%)]\tLoss: 18.704043\n",
      "Train Epoch: 145 [3600/8040 (45%)]\tLoss: 18.917649\n",
      "Train Epoch: 145 [4800/8040 (60%)]\tLoss: 18.905876\n",
      "Train Epoch: 145 [6000/8040 (75%)]\tLoss: 18.420017\n",
      "Train Epoch: 145 [7200/8040 (90%)]\tLoss: 18.774465\n",
      "====> Epoch: 145 Average loss: 18.9162\n",
      "epoch: 145, beta=0, frac_anom=0.01\n",
      "Train Epoch: 146 [0/8040 (0%)]\tLoss: 18.325696\n",
      "Train Epoch: 146 [1200/8040 (15%)]\tLoss: 18.911855\n",
      "Train Epoch: 146 [2400/8040 (30%)]\tLoss: 18.972599\n",
      "Train Epoch: 146 [3600/8040 (45%)]\tLoss: 18.444438\n",
      "Train Epoch: 146 [4800/8040 (60%)]\tLoss: 18.946535\n",
      "Train Epoch: 146 [6000/8040 (75%)]\tLoss: 19.816785\n",
      "Train Epoch: 146 [7200/8040 (90%)]\tLoss: 19.473079\n",
      "====> Epoch: 146 Average loss: 18.8922\n",
      "epoch: 146, beta=0, frac_anom=0.01\n",
      "Train Epoch: 147 [0/8040 (0%)]\tLoss: 19.415633\n",
      "Train Epoch: 147 [1200/8040 (15%)]\tLoss: 18.344584\n",
      "Train Epoch: 147 [2400/8040 (30%)]\tLoss: 18.239801\n",
      "Train Epoch: 147 [3600/8040 (45%)]\tLoss: 19.111021\n",
      "Train Epoch: 147 [4800/8040 (60%)]\tLoss: 18.938088\n",
      "Train Epoch: 147 [6000/8040 (75%)]\tLoss: 19.081512\n",
      "Train Epoch: 147 [7200/8040 (90%)]\tLoss: 18.987549\n",
      "====> Epoch: 147 Average loss: 18.9467\n",
      "epoch: 147, beta=0, frac_anom=0.01\n",
      "Train Epoch: 148 [0/8040 (0%)]\tLoss: 18.144476\n",
      "Train Epoch: 148 [1200/8040 (15%)]\tLoss: 18.724902\n",
      "Train Epoch: 148 [2400/8040 (30%)]\tLoss: 19.391634\n",
      "Train Epoch: 148 [3600/8040 (45%)]\tLoss: 18.975468\n",
      "Train Epoch: 148 [4800/8040 (60%)]\tLoss: 20.504415\n",
      "Train Epoch: 148 [6000/8040 (75%)]\tLoss: 18.732104\n",
      "Train Epoch: 148 [7200/8040 (90%)]\tLoss: 18.186705\n",
      "====> Epoch: 148 Average loss: 18.8889\n",
      "epoch: 148, beta=0, frac_anom=0.01\n",
      "Train Epoch: 149 [0/8040 (0%)]\tLoss: 19.052230\n",
      "Train Epoch: 149 [1200/8040 (15%)]\tLoss: 18.698405\n",
      "Train Epoch: 149 [2400/8040 (30%)]\tLoss: 18.835488\n",
      "Train Epoch: 149 [3600/8040 (45%)]\tLoss: 19.415253\n",
      "Train Epoch: 149 [4800/8040 (60%)]\tLoss: 18.519312\n",
      "Train Epoch: 149 [6000/8040 (75%)]\tLoss: 18.439850\n",
      "Train Epoch: 149 [7200/8040 (90%)]\tLoss: 19.638318\n",
      "====> Epoch: 149 Average loss: 18.9041\n",
      "epoch: 149, beta=0, frac_anom=0.01\n",
      "Train Epoch: 150 [0/8040 (0%)]\tLoss: 18.735488\n",
      "Train Epoch: 150 [1200/8040 (15%)]\tLoss: 18.760280\n",
      "Train Epoch: 150 [2400/8040 (30%)]\tLoss: 17.749782\n",
      "Train Epoch: 150 [3600/8040 (45%)]\tLoss: 18.013692\n",
      "Train Epoch: 150 [4800/8040 (60%)]\tLoss: 18.186357\n",
      "Train Epoch: 150 [6000/8040 (75%)]\tLoss: 18.166095\n",
      "Train Epoch: 150 [7200/8040 (90%)]\tLoss: 18.416231\n",
      "====> Epoch: 150 Average loss: 18.8629\n",
      "epoch: 150, beta=0, frac_anom=0.01\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (28) must match the size of tensor b (3960) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f03f6499875a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             test_loss_total[a, b], test_loss_anom[a, b], test_loss_normals[\n\u001b[0;32m---> 27\u001b[0;31m                 a, b] = test(frac_anom, beta_val=betaval)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-aed7a7d26525>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(frac_anom, beta_val)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             test_loss_anom += torch.sum(MSE_loss(recon_batch * anom_lab,\n\u001b[0;32m---> 16\u001b[0;31m                                       data * anom_lab)).item()\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mtest_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMSE_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (28) must match the size of tensor b (3960) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    brange=[0,0.01]\n",
    "    erange = range(1, epochs + 1)\n",
    "    anrange = np.array([0.01,0.05,0.1])\n",
    "    \n",
    "    test_loss_total = np.zeros((len(anrange), len(brange)))\n",
    "    test_loss_anom = np.zeros((len(anrange), len(brange)))\n",
    "    test_loss_normals = np.zeros((len(anrange), len(brange)))\n",
    "\n",
    "    for b, betaval in enumerate(brange):\n",
    "\n",
    "        for a, frac_anom in enumerate(anrange):\n",
    "            train_loader, test_loader = create_data(frac_anom)\n",
    "            model_reset()\n",
    "            for epoch in erange:\n",
    "\n",
    "                train(epoch, beta_val=betaval)\n",
    "\n",
    "                print('epoch: %d, beta=%g, frac_anom=%g' %\n",
    "                      (epoch, betaval, frac_anom))\n",
    "\n",
    "            # save the model\n",
    "            torch.save(model, 'fashion_mnist_beta_shallow_' + str(betaval) + '_frac_anom_' + str(frac_anom))\n",
    "\n",
    "            test_loss_total[a, b], test_loss_anom[a, b], test_loss_normals[\n",
    "                a, b] = test(frac_anom, beta_val=betaval)\n",
    "\n",
    "\n",
    "\n",
    "        np.savez('test_loss_fashionmnist_beta_shallow' + str(b) + '.npz',\n",
    "                 test_loss_total=test_loss_total,\n",
    "                 test_loss_anom=test_loss_anom,\n",
    "                 test_loss_normals=test_loss_normals,\n",
    "                 brange=brange,\n",
    "                 anrange=anrange)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors and legends for ROC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "FPRs = dict()\n",
    "TPRs = dict()\n",
    "AUC = dict()\n",
    "\n",
    "lgd = {\n",
    "    0: 'VAE-1%',\n",
    "    1: 'VAE-5%',\n",
    "    2: 'VAE-10%',\n",
    "    3: 'RVAE-1%',\n",
    "    4: 'RVAE-5%',\n",
    "    5: 'RVAE-10%'\n",
    "}\n",
    "colors = {0: 'r', 1: 'b', 2: 'k', 3: 'r', 4: 'b', 5: 'k'}\n",
    "lsty = {0: '--', 1: '--', 2: '--', 3: '-', 4: '-', 5: '-'}\n",
    "c = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
