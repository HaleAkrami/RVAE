{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Basics of Neural Networks\n",
    "* <b>Learning Objective:</b> In this problem, you are asked to implement a basic multi-layer fully connected neural network from scratch, including forward and backward passes of certain essential layers, to perform an image classification task on CIFAR-10 dataset. You need to implement essential functions in different indicated python files under directory `lib`.\n",
    "* <b>Provided Code:</b> We provide the skeletons of classes you need to complete. Forward checking and gradient checkings are provided for verifying your implementation as well.\n",
    "* <b>TODOs:</b> You are asked to implement the forward passes and backward passes for standard layers and loss functions, various widely-used optimizers, and part of the training procedure. And finally we want you to train a network from scratch on your own. Also, there are inline questions you need to answer. See `README.md` to set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.fully_conn import *\n",
    "from lib.layer_utils import *\n",
    "from lib.grad_check import *\n",
    "from lib.datasets import *\n",
    "from lib.optim import *\n",
    "from lib.train import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data (CIFAR-10)\n",
    "Run the following code block to download CIFAR-10 dataset and load in the properly splitted CIFAR-10 data. The script `get_datasets.sh` use `wget` to download the CIFAR-10 dataset. If you have a trouble with executing `get_datasets.sh`, you can manually download the dataset and extract files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./get_datasets.sh\n",
    "# !get_datasets.sh for windows users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CIFAR10_data()\n",
    "for k, v in data.items():\n",
    "    print (\"Name: {} Shape: {}\".format(k, v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Standard Layers\n",
    "You will now implement all the following standard layers commonly seen in a fully connected neural network. Please refer to the file `lib/layer_utils.py`. Take a look at each class skeleton, and we will walk you through the network layer by layer. We provide results of some examples we pre-computed for you for checking the forward pass, and also the gradient checking for the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC Forward [2pt]\n",
    "In the class skeleton `flatten` and `fc` in `lib/layer_utils.py`, please complete the forward pass in function `forward`. The input to the `fc` layer may not be of dimension (batch size, features size), it could be an image or any higher dimensional data. We want to convert the input to have a shape of (batch size, features size). Make sure that you handle this dimensionality issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the fc forward function\n",
    "input_bz = 3 # batch size\n",
    "input_dim = (7, 6, 4)\n",
    "output_dim = 4\n",
    "\n",
    "input_size = input_bz * np.prod(input_dim)\n",
    "weight_size = output_dim * np.prod(input_dim)\n",
    "\n",
    "flatten_layer = flatten(name=\"flatten_test\")\n",
    "single_fc = fc(np.prod(input_dim), output_dim, init_scale=0.02, name=\"fc_test\")\n",
    "\n",
    "x = np.linspace(-0.1, 0.4, num=input_size).reshape(input_bz, *input_dim)\n",
    "w = np.linspace(-0.2, 0.2, num=weight_size).reshape(np.prod(input_dim), output_dim)\n",
    "b = np.linspace(-0.3, 0.3, num=output_dim)\n",
    "\n",
    "single_fc.params[single_fc.w_name] = w\n",
    "single_fc.params[single_fc.b_name] = b\n",
    "\n",
    "out = single_fc.forward(flatten_layer.forward(x))\n",
    "\n",
    "correct_out = np.array([[0.63910291, 0.83740057, 1.03569824, 1.23399591],\n",
    "                        [0.61401587, 0.82903823, 1.04406058, 1.25908294],\n",
    "                        [0.58892884, 0.82067589, 1.05242293, 1.28416997]])\n",
    "\n",
    "# Compare your output with the above pre-computed ones. \n",
    "# The difference should not be larger than 1e-8\n",
    "print (\"Difference: \", rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC Backward [2pt]\n",
    "Please complete the function `backward` as the backward pass of the `flatten` and `fc` layers. Follow the instructions in the comments to store gradients into the predefined dictionaries in the attributes of the class. Parameters of the layer are also stored in the predefined dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the fc backward function\n",
    "inp = np.random.randn(15, 2, 2, 3)\n",
    "w = np.random.randn(12, 15)\n",
    "b = np.random.randn(15)\n",
    "dout = np.random.randn(15, 15)\n",
    "\n",
    "flatten_layer = flatten(name=\"flatten_test\")\n",
    "x = flatten_layer.forward(inp)\n",
    "single_fc = fc(np.prod(x.shape[1:]), 15, init_scale=5e-2, name=\"fc_test\")\n",
    "single_fc.params[single_fc.w_name] = w\n",
    "single_fc.params[single_fc.b_name] = b\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: single_fc.forward(x), x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: single_fc.forward(x), w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: single_fc.forward(x), b, dout)\n",
    "\n",
    "out = single_fc.forward(x)\n",
    "dx = single_fc.backward(dout)\n",
    "dw = single_fc.grads[single_fc.w_name]\n",
    "db = single_fc.grads[single_fc.b_name]\n",
    "dinp = flatten_layer.backward(dx)\n",
    "\n",
    "# The error should be around 1e-9\n",
    "print(\"dx Error: \", rel_error(dx_num, dx))\n",
    "# The errors should be around 1e-10\n",
    "print(\"dw Error: \", rel_error(dw_num, dw))\n",
    "print(\"db Error: \", rel_error(db_num, db))\n",
    "# The shapes should be same\n",
    "print(\"dinp Shape: \", dinp.shape, inp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Forward [2pt]\n",
    "In the class skeleton `relu` in `lib/layer_utils.py`, please complete the `forward` pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the relu forward function\n",
    "x = np.linspace(-1.5, 1.5, num=12).reshape(3, 4)\n",
    "relu_f = relu(name=\"relu_f\")\n",
    "\n",
    "out = relu_f.forward(x)\n",
    "\n",
    "correct_out = np.array([[0.,          0.,         0.,         0.        ],\n",
    "                        [0.,          0.,         0.13636364, 0.40909091],\n",
    "                        [0.68181818,  0.95454545, 1.22727273, 1.5      ]])\n",
    "\n",
    "# Compare your output with the above pre-computed ones. \n",
    "# The difference should not be larger than 1e-7\n",
    "print (\"Difference: \", rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Backward [2pt]\n",
    "Please complete the `backward` pass of the class `relu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the relu backward function\n",
    "x = np.random.randn(15, 15)\n",
    "dout = np.random.randn(*x.shape)\n",
    "relu_b = relu(name=\"relu_b\")\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_b.forward(x), x, dout)\n",
    "\n",
    "out = relu_b.forward(x)\n",
    "dx = relu_b.backward(dout)\n",
    "\n",
    "# The error should not be larger than 1e-10\n",
    "print (\"dx Error: \", rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Forward [2pt]\n",
    "In the class `dropout` in `lib/layer_utils.py`, please complete the `forward` pass.  \n",
    "Remember that the dropout is only applied during training phase, you should pay attention to this while implementing the function.\n",
    "##### Important Note1: The probability argument input to the function is the \"keep probability\": probability that each activation is kept.\n",
    "##### Important Note2: If the keep_prob is set to 0, make it as no dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "x = np.random.randn(100, 100) + 5.0\n",
    "\n",
    "print (\"----------------------------------------------------------------\")\n",
    "for p in [0, 0.25, 0.50, 0.75, 1]:\n",
    "    dropout_f = dropout(keep_prob=p)\n",
    "    out = dropout_f.forward(x, True)\n",
    "    out_test = dropout_f.forward(x, False)\n",
    "\n",
    "    # Mean of output should be similar to mean of input\n",
    "    # Means of output during training time and testing time should be similar\n",
    "    print (\"Dropout Keep Prob = \", p)\n",
    "    print (\"Mean of input: \", x.mean())\n",
    "    print (\"Mean of output during training time: \", out.mean())\n",
    "    print (\"Mean of output during testing time: \", out_test.mean())\n",
    "    print (\"Fraction of output set to zero during training time: \", (out == 0).mean())\n",
    "    print (\"Fraction of output set to zero during testing time: \", (out_test == 0).mean())\n",
    "    print (\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Backward [2pt]\n",
    "Please complete the `backward` pass. Again remember that the dropout is only applied during training phase, handle this in the backward pass as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "x = np.random.randn(5, 5) + 5\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "keep_prob = 0.75\n",
    "dropout_b = dropout(keep_prob, seed=100)\n",
    "out = dropout_b.forward(x, True, seed=1)\n",
    "dx = dropout_b.backward(dout)\n",
    "dx_num = eval_numerical_gradient_array(lambda xx: dropout_b.forward(xx, True, seed=1), x, dout)\n",
    "\n",
    "# The error should not be larger than 1e-10\n",
    "print ('dx relative error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing cascaded layers: FC + ReLU [2pt]\n",
    "Please find the `TestFCReLU` function in `lib/fully_conn.py`. <br />\n",
    "You only need to complete a few lines of code in the TODO block. <br />\n",
    "Please design an `Flatten -> FC -> ReLU` network where the parameters of them match the given x, w, and b. <br />\n",
    "Please insert the corresponding names you defined for each layer to param_name_w, and param_name_b respectively. Here you only modify the param_name part, the `_w`, and `_b` are automatically assigned during network setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "x = np.random.randn(3, 4, 5)  # the input features\n",
    "w = np.random.randn(20, 10)   # the weight of fc layer\n",
    "b = np.random.randn(10)       # the bias of fc layer\n",
    "dout = np.random.randn(3, 10) # the gradients to the output, notice the shape\n",
    "\n",
    "tiny_net = TestFCReLU()\n",
    "\n",
    "###################################################\n",
    "# TODO: param_name should be replaced accordingly #\n",
    "###################################################\n",
    "tiny_net.net.assign(\"param_name_w\", w)\n",
    "tiny_net.net.assign(\"param_name_b\", b)\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "out = tiny_net.forward(x)\n",
    "dx = tiny_net.backward(dout)\n",
    "\n",
    "###################################################\n",
    "# TODO: param_name should be replaced accordingly #\n",
    "###################################################\n",
    "dw = tiny_net.net.get_grads(\"param_name_w\")\n",
    "db = tiny_net.net.get_grads(\"param_name_b\")\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: tiny_net.forward(x), x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: tiny_net.forward(x), w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: tiny_net.forward(x), b, dout)\n",
    "\n",
    "# The errors should not be larger than 1e-7\n",
    "print (\"dx error: \", rel_error(dx_num, dx))\n",
    "print (\"dw error: \", rel_error(dw_num, dw))\n",
    "print (\"db error: \", rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SoftMax Function and Loss Layer [2pt]\n",
    "In the `lib/layer_utils.py`, please first complete the function `softmax`, which will be used in the function `cross_entropy`. Then, implement `corss_entropy` using `softmax`.\n",
    "Please refer to the lecture slides of the mathematical expressions of the cross entropy loss function, and complete its forward pass and backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "num_classes, num_inputs = 6, 100\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "test_loss = cross_entropy()\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: test_loss.forward(x, y), x, verbose=False)\n",
    "\n",
    "loss = test_loss.forward(x, y)\n",
    "dx = test_loss.backward()\n",
    "\n",
    "# Test softmax_loss function. Loss should be around 1.792\n",
    "# and dx error should be at the scale of 1e-8 (or smaller)\n",
    "print (\"Cross Entropy Loss: \", loss)\n",
    "print (\"dx error: \", rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a Small Fully Connected Network [2pt]\n",
    "Please find the `SmallFullyConnectedNetwork` function in `lib/fully_conn.py`. <br />\n",
    "Again you only need to complete few lines of code in the TODO block. <br />\n",
    "Please design an `FC --> ReLU --> FC --> ReLU` network where the shapes of parameters match the given shapes. <br />\n",
    "Please insert the corresponding names you defined for each layer to param_name_w, and param_name_b respectively. <br />\n",
    "Here you only modify the param_name part, the `_w`, and `_b` are automatically assigned during network setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "model = SmallFullyConnectedNetwork()\n",
    "loss_func = cross_entropy()\n",
    "\n",
    "N, D, = 4, 4  # N: batch size, D: input dimension\n",
    "H, C  = 30, 7 # H: hidden dimension, C: output dimension\n",
    "std = 0.02\n",
    "x = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "print (\"Testing initialization ... \")\n",
    "\n",
    "###################################################\n",
    "# TODO: param_name should be replaced accordingly #\n",
    "###################################################\n",
    "w1_std = abs(model.net.get_params(\"param_name_w\").std() - std)\n",
    "b1 = model.net.get_params(\"param_name_b\").std()\n",
    "w2_std = abs(model.net.get_params(\"param_name_w\").std() - std)\n",
    "b2 = model.net.get_params(\"param_name_b\").std()\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "assert w1_std < std / 10, \"First layer weights do not seem right\"\n",
    "assert np.all(b1 == 0), \"First layer biases do not seem right\"\n",
    "assert w2_std < std / 10, \"Second layer weights do not seem right\"\n",
    "assert np.all(b2 == 0), \"Second layer biases do not seem right\"\n",
    "print (\"Passed!\")\n",
    "\n",
    "print (\"Testing test-time forward pass ... \")\n",
    "w1 = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "w2 = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "b1 = np.linspace(-0.1, 0.9, num=H)\n",
    "b2 = np.linspace(-0.9, 0.1, num=C)\n",
    "\n",
    "###################################################\n",
    "# TODO: param_name should be replaced accordingly #\n",
    "###################################################\n",
    "model.net.assign(\"param_name_w\", w1)\n",
    "model.net.assign(\"param_name_b\", b1)\n",
    "model.net.assign(\"param_name_w\", w2)\n",
    "model.net.assign(\"param_name_b\", b2)\n",
    "###################################################\n",
    "#                END OF YOUR CODE                 #\n",
    "###################################################\n",
    "\n",
    "feats = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.forward(feats)\n",
    "correct_scores = np.asarray([[4.20670862, 4.87188359, 5.53705856, 6.20223352, 6.86740849, 7.53258346, 8.19775843],\n",
    "                             [4.74826036, 5.35984681, 5.97143326, 6.58301972, 7.19460617, 7.80619262, 8.41777907],\n",
    "                             [5.2898121,  5.84781003, 6.40580797, 6.96380591, 7.52180384, 8.07980178, 8.63779971],\n",
    "                             [5.83136384, 6.33577326, 6.84018268, 7.3445921,  7.84900151, 8.35341093, 8.85782035]])\n",
    "scores_diff = np.sum(np.abs(scores - correct_scores))\n",
    "assert scores_diff < 1e-6, \"Your implementation might went wrong!\"\n",
    "print (\"Passed!\")\n",
    "\n",
    "print (\"Testing the loss ...\",)\n",
    "y = np.asarray([0, 5, 1, 4])\n",
    "loss = loss_func.forward(scores, y)\n",
    "dLoss = loss_func.backward()\n",
    "correct_loss = 2.90181552716\n",
    "assert abs(loss - correct_loss) < 1e-10, \"Your implementation might went wrong!\"\n",
    "print (\"Passed!\")\n",
    "\n",
    "print (\"Testing the gradients (error should be no larger than 1e-6) ...\")\n",
    "din = model.backward(dLoss)\n",
    "for layer in model.net.layers:\n",
    "    if not layer.params:\n",
    "        continue\n",
    "    for name in sorted(layer.grads):\n",
    "        f = lambda _: loss_func.forward(model.forward(feats), y)\n",
    "        grad_num = eval_numerical_gradient(f, layer.params[name], verbose=False)\n",
    "        print ('%s relative error: %.2e' % (name, rel_error(grad_num, layer.grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a Fully Connected Network regularized with Dropout [2pt]\n",
    "Please find the `DropoutNet` function in `fully_conn.py` under lib directory. <br />\n",
    "For this part you don't need to design a new network, just simply run the following test code. <br />\n",
    "If something goes wrong, you might want to double check your dropout implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "seed = 1234\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "N, D, C = 3, 15, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for keep_prob in [0, 0.25, 0.5]:\n",
    "    np.random.seed(seed=seed)\n",
    "    print (\"Dropout p =\", keep_prob)\n",
    "    model = DropoutNet(keep_prob=keep_prob, seed=seed)\n",
    "    loss_func = cross_entropy()\n",
    "    output = model.forward(X, True, seed=seed)\n",
    "    loss = loss_func.forward(output, y)\n",
    "    dLoss = loss_func.backward()\n",
    "    dX = model.backward(dLoss)\n",
    "    grads = model.net.grads\n",
    "\n",
    "    print (\"Error of gradients should be around or less than 1e-5\")\n",
    "    for name in sorted(grads):\n",
    "        if name not in model.net.params.keys():\n",
    "            continue\n",
    "        f = lambda _: loss_func.forward(model.forward(X, True, seed=seed), y)\n",
    "        grad_num = eval_numerical_gradient(f, model.net.params[name], verbose=False, h=1e-5)\n",
    "        print (\"{} relative error: {}\".format(name, rel_error(grad_num, grads[name])))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Network\n",
    "In this section, we defined a `TinyNet` class for you to fill in the TODO block in `lib/fully_conn.py`.\n",
    "* Here please design a two layer fully connected network with ReLU activation (`Flatten --> FC --> ReLU --> FC`).\n",
    "* You can adjusting the number of hidden neurons, batch_size, and epochs.\n",
    "* Please read the `lib/train.py` carefully and complete the TODO blocks in the `train_net` function first. Codes in \"Test a Small Fully Connected Network\" can be helpful.\n",
    "* In addition, read how the SGD function is implemented in `lib/optim.py`, you will be asked to complete three other optimization methods in the later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange the data\n",
    "data_dict = {\n",
    "    \"data_train\": (data[\"data_train\"], data[\"labels_train\"]),\n",
    "    \"data_val\": (data[\"data_val\"], data[\"labels_val\"]),\n",
    "    \"data_test\": (data[\"data_test\"], data[\"labels_test\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"data_train\"].shape)\n",
    "print(data[\"labels_train\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now train the network to achieve at least 50% validation accuracy [5pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "model = TinyNet()\n",
    "loss_f = cross_entropy()\n",
    "optimizer = SGD(model.net, 1e-4)\n",
    "\n",
    "results = None\n",
    "#############################################################################\n",
    "# TODO: Use the train_net function you completed to train a network         #\n",
    "#############################################################################\n",
    "\n",
    "batch_size = None\n",
    "epochs = None\n",
    "lr_decay = None\n",
    "lr_decay_every = None\n",
    "\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################\n",
    "results = train_net(data_dict, model, loss_f, optimizer, batch_size, epochs, \n",
    "                    lr_decay, lr_decay_every, show_every=10000, verbose=True)\n",
    "opt_params, loss_hist, train_acc_hist, val_acc_hist = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at what names of params were stored\n",
    "print (opt_params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: How to load the parameters to a newly defined network\n",
    "model = TinyNet()\n",
    "model.net.load(opt_params)\n",
    "val_acc = compute_acc(model, data[\"data_val\"], data[\"labels_val\"])\n",
    "print (\"Validation Accuracy: {}%\".format(val_acc*100))\n",
    "test_acc = compute_acc(model, data[\"data_test\"], data[\"labels_test\"])\n",
    "print (\"Testing Accuracy: {}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curves\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "loss_hist_ = loss_hist[1::100] # sparse the curve a bit\n",
    "plt.plot(loss_hist_, '-o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(train_acc_hist, '-o', label='Training')\n",
    "plt.plot(val_acc_hist, '-o', label='Validation')\n",
    "plt.plot([0.5] * len(val_acc_hist), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Optimizers\n",
    "There are several more advanced optimizers than vanilla SGD, you will implement three more sophisticated and widely-used methods in this section.  \n",
    "Please complete the TODOs in the `lib/optim.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD + Momentum [2pt]\n",
    "The update rule of SGD plus momentum is as shown below:  \n",
    "\\begin{equation}\n",
    "v_t: last\\ update\\ of\\ the\\ velocity \\\\\n",
    "\\gamma: momentum \\\\\n",
    "\\eta: learning\\ rate \\\\\n",
    "v_t = \\gamma v_{t-1} - \\eta \\nabla_{\\theta}J(\\theta) \\\\\n",
    "\\theta = \\theta + v_t\n",
    "\\end{equation}\n",
    "Complete the `SGDM()` function in `lib/optim.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "# Test the implementation of SGD with Momentum\n",
    "seed = 123\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "N, D = 4, 5\n",
    "test_sgd = sequential(fc(N, D, name=\"sgd_fc\"))\n",
    "\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "test_sgd.layers[0].params = {\"sgd_fc_w\": w}\n",
    "test_sgd.layers[0].grads = {\"sgd_fc_w\": dw}\n",
    "\n",
    "test_sgd_momentum = SGDM(test_sgd, 1e-3, 0.9)\n",
    "test_sgd_momentum.velocity = {\"sgd_fc_w\": v}\n",
    "test_sgd_momentum.step()\n",
    "\n",
    "updated_w = test_sgd.layers[0].params[\"sgd_fc_w\"]\n",
    "velocity = test_sgd_momentum.velocity[\"sgd_fc_w\"]\n",
    "\n",
    "expected_updated_w = np.asarray([\n",
    "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
    "expected_velocity = np.asarray([\n",
    "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
    "\n",
    "print ('The following errors should be around or less than 1e-8')\n",
    "print ('updated_w error: ', rel_error(updated_w, expected_updated_w))\n",
    "print ('velocity error: ', rel_error(expected_velocity, velocity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing SGD and SGD with Momentum [2pt]\n",
    "Run the following code block to train a multi-layer fully connected network with both SGD and SGD plus Momentum. The network trained with SGDM optimizer should converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "# Arrange a small data\n",
    "num_train = 4000\n",
    "small_data_dict = {\n",
    "    \"data_train\": (data[\"data_train\"][:num_train], data[\"labels_train\"][:num_train]),\n",
    "    \"data_val\": (data[\"data_val\"], data[\"labels_val\"]),\n",
    "    \"data_test\": (data[\"data_test\"], data[\"labels_test\"])\n",
    "}\n",
    "\n",
    "model_sgd      = FullyConnectedNetwork()\n",
    "model_sgdm     = FullyConnectedNetwork()\n",
    "loss_f_sgd     = cross_entropy()\n",
    "loss_f_sgdm    = cross_entropy()\n",
    "optimizer_sgd  = SGD(model_sgd.net, 1e-2)\n",
    "optimizer_sgdm = SGDM(model_sgdm.net, 1e-2, 0.9)\n",
    "\n",
    "print (\"Training with Vanilla SGD...\")\n",
    "results_sgd = train_net(small_data_dict, model_sgd, loss_f_sgd, optimizer_sgd, batch_size=100, \n",
    "                        max_epochs=5, show_every=100, verbose=True)\n",
    "\n",
    "print (\"\\nTraining with SGD plus Momentum...\")\n",
    "results_sgdm = train_net(small_data_dict, model_sgdm, loss_f_sgdm, optimizer_sgdm, batch_size=100, \n",
    "                         max_epochs=5, show_every=100, verbose=True)\n",
    "\n",
    "opt_params_sgd,  loss_hist_sgd,  train_acc_hist_sgd,  val_acc_hist_sgd  = results_sgd\n",
    "opt_params_sgdm, loss_hist_sgdm, train_acc_hist_sgdm, val_acc_hist_sgdm = results_sgdm\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgd, 'o', label=\"Vanilla SGD\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgd, '-o', label=\"Vanilla SGD\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgd, '-o', label=\"Vanilla SGD\")\n",
    "         \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgdm, 'o', label=\"SGD with Momentum\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgdm, '-o', label=\"SGD with Momentum\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgdm, '-o', label=\"SGD with Momentum\")\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp [2pt]\n",
    "The update rule of RMSProp is as shown below:  \n",
    "\\begin{equation}\n",
    "\\gamma: decay\\ rate \\\\\n",
    "\\epsilon: small\\ number \\\\\n",
    "g_t^2: squared\\ gradients \\\\\n",
    "\\eta: learning\\ rate \\\\\n",
    "E[g^2]_t: decaying\\ average\\ of\\ past\\ squared\\ gradients\\ at\\ update\\ step\\ t \\\\\n",
    "E[g^2]_t = \\gamma E[g^2]_{t-1} + (1-\\gamma)g_t^2 \\\\\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta \\nabla_{\\theta}J(\\theta)}{\\sqrt{E[g^2]_t+\\epsilon}}\n",
    "\\end{equation}\n",
    "Complete the `RMSProp()` function in `lib/optim.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "# Test RMSProp implementation; you should see errors less than 1e-7\n",
    "N, D = 4, 5\n",
    "test_rms = sequential(fc(N, D, name=\"rms_fc\"))\n",
    "\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "test_rms.layers[0].params = {\"rms_fc_w\": w}\n",
    "test_rms.layers[0].grads = {\"rms_fc_w\": dw}\n",
    "\n",
    "opt_rms = RMSProp(test_rms, 1e-2, 0.99)\n",
    "opt_rms.cache = {\"rms_fc_w\": cache}\n",
    "opt_rms.step()\n",
    "\n",
    "updated_w = test_rms.layers[0].params[\"rms_fc_w\"]\n",
    "cache = opt_rms.cache[\"rms_fc_w\"]\n",
    "\n",
    "expected_updated_w = np.asarray([\n",
    "  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n",
    "  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n",
    "  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n",
    "  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n",
    "expected_cache = np.asarray([\n",
    "  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n",
    "  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n",
    "  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n",
    "  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n",
    "\n",
    "print ('The following errors should be around or less than 1e-7')\n",
    "print ('updated_w error: ', rel_error(expected_updated_w, updated_w))\n",
    "print ('cache error: ', rel_error(expected_cache, opt_rms.cache[\"rms_fc_w\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam [2pt]\n",
    "The update rule of Adam is as shown below:  \n",
    "\\begin{equation}\n",
    "t = t + 1 \\\\\n",
    "g_t: gradients\\ at\\ update\\ step\\ t \\\\\n",
    "m_t = \\beta_1m_{t-1} + (1-\\beta_1)g_t \\\\\n",
    "v_t = \\beta_2v_{t-1} + (1-\\beta_2)g_t^2 \\\\\n",
    "\\hat{m_t} = m_t / (1 - \\beta_1^t) \\\\\n",
    "\\hat{v_t} = v_t / (1 - \\beta_2^t) \\\\\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta\\ \\hat{m_t}}{\\sqrt{\\hat{v_t}}+\\epsilon} \\\\\n",
    "\\end{equation}\n",
    "Complete the `Adam()` function in `lib/optim.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "# Test Adam implementation; you should see errors around 1e-7 or less\n",
    "N, D = 4, 5\n",
    "test_adam = sequential(fc(N, D, name=\"adam_fc\"))\n",
    "\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
    "\n",
    "test_adam.layers[0].params = {\"adam_fc_w\": w}\n",
    "test_adam.layers[0].grads = {\"adam_fc_w\": dw}\n",
    "\n",
    "opt_adam = Adam(test_adam, 1e-2, 0.9, 0.999, t=5)\n",
    "opt_adam.mt = {\"adam_fc_w\": m}\n",
    "opt_adam.vt = {\"adam_fc_w\": v}\n",
    "opt_adam.step()\n",
    "\n",
    "updated_w = test_adam.layers[0].params[\"adam_fc_w\"]\n",
    "mt = opt_adam.mt[\"adam_fc_w\"]\n",
    "vt = opt_adam.vt[\"adam_fc_w\"]\n",
    "\n",
    "expected_updated_w = np.asarray([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
    "expected_m = np.asarray([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "print ('The following errors should be around or less than 1e-7')\n",
    "print ('updated_w error: ', rel_error(expected_updated_w, updated_w))\n",
    "print ('mt error: ', rel_error(expected_m, mt))\n",
    "print ('vt error: ', rel_error(expected_v, vt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the optimizers [2pt]\n",
    "Run the following code block to compare the plotted results among all the above optimizers. You should see SGD with Momentum, RMSProp, and Adam optimizers work better than Vanilla SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed=seed)\n",
    "\n",
    "model_rms      = FullyConnectedNetwork()\n",
    "model_adam     = FullyConnectedNetwork()\n",
    "loss_f_rms     = cross_entropy()\n",
    "loss_f_adam    = cross_entropy()\n",
    "optimizer_rms  = RMSProp(model_rms.net, 5e-4)\n",
    "optimizer_adam = Adam(model_adam.net, 5e-4)\n",
    "\n",
    "print (\"Training with RMSProp...\")\n",
    "results_rms = train_net(small_data_dict, model_rms, loss_f_rms, optimizer_rms, batch_size=100, \n",
    "                        max_epochs=5, show_every=100, verbose=True)\n",
    "\n",
    "print (\"\\nTraining with Adam...\")\n",
    "results_adam = train_net(small_data_dict, model_adam, loss_f_adam, optimizer_adam, batch_size=100, \n",
    "                         max_epochs=5, show_every=100, verbose=True)\n",
    "\n",
    "opt_params_rms,  loss_hist_rms,  train_acc_hist_rms,  val_acc_hist_rms  = results_rms\n",
    "opt_params_adam, loss_hist_adam, train_acc_hist_adam, val_acc_hist_adam = results_adam\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgd, 'o', label=\"Vanilla SGD\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgd, '-o', label=\"Vanilla SGD\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgd, '-o', label=\"Vanilla SGD\")\n",
    "         \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_sgdm, 'o', label=\"SGD with Momentum\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_sgdm, '-o', label=\"SGD with Momentum\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_sgdm, '-o', label=\"SGD with Momentum\")\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_rms, 'o', label=\"RMSProp\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_rms, '-o', label=\"RMSProp\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_rms, '-o', label=\"RMSProp\")\n",
    "         \n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(loss_hist_adam, 'o', label=\"Adam\")\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(train_acc_hist_adam, '-o', label=\"Adam\")\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(val_acc_hist_adam, '-o', label=\"Adam\")\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Network with Dropout [2pt]\n",
    "Run the following code blocks to compare the results with and without dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train two identical nets, one with dropout and one without\n",
    "num_train = 100\n",
    "data_dict_100 = {\n",
    "    \"data_train\": (data[\"data_train\"][:num_train], data[\"labels_train\"][:num_train]),\n",
    "    \"data_val\": (data[\"data_val\"], data[\"labels_val\"]),\n",
    "    \"data_test\": (data[\"data_test\"], data[\"labels_test\"])\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "keep_ps = [0, 0.25, 0.50, 0.75]\n",
    "\n",
    "results_dict = {}\n",
    "for keep_prob in keep_ps:\n",
    "    results_dict[keep_prob] = {}\n",
    "\n",
    "for keep_prob in keep_ps:\n",
    "    seed = 123\n",
    "    np.random.seed(seed=seed)\n",
    "\n",
    "    print (\"Dropout Keep Prob =\", keep_prob)\n",
    "    model = DropoutNetTest(keep_prob=keep_prob)\n",
    "    loss_f = cross_entropy()\n",
    "    optimizer = SGD(model.net, 1e-4)\n",
    "    results = train_net(data_dict_100, model, loss_f, optimizer, batch_size=20, \n",
    "                        max_epochs=50, show_every=1000, verbose=True)\n",
    "    opt_params, loss_hist, train_acc_hist, val_acc_hist = results\n",
    "    results_dict[keep_prob] = {\n",
    "        \"opt_params\": opt_params, \n",
    "        \"loss_hist\": loss_hist, \n",
    "        \"train_acc_hist\": train_acc_hist, \n",
    "        \"val_acc_hist\": val_acc_hist\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train and validation accuracies of the two models\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "for keep_prob in keep_ps:\n",
    "    curr_dict = results_dict[keep_prob]\n",
    "    train_accs.append(curr_dict[\"train_acc_hist\"][-1])\n",
    "    val_accs.append(curr_dict[\"val_acc_hist\"][-1])\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "for keep_prob in keep_ps:\n",
    "    curr_dict = results_dict[keep_prob]\n",
    "    plt.plot(curr_dict[\"train_acc_hist\"], 'o', label='%.2f dropout' % keep_prob)\n",
    "plt.title('Train accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "  \n",
    "plt.subplot(3, 1, 2)\n",
    "for keep_prob in keep_ps:\n",
    "    curr_dict = results_dict[keep_prob]\n",
    "    plt.plot(curr_dict[\"val_acc_hist\"], 'o', label='%.2f dropout' % keep_prob)\n",
    "plt.title('Val accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(ncol=2, loc='lower right')\n",
    "\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline Question: Describe what you observe from the above results and graphs about dropout [2pt]\n",
    "#### Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Activation Functions [1pt]\n",
    "In each of the activation function, use the given lambda function template to plot their corresponding curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "left, right = -10, 10\n",
    "X  = np.linspace(left, right, 100)\n",
    "XS = np.linspace(-5, 5, 10)\n",
    "lw = 4\n",
    "alpha = 0.1 # alpha for leaky_relu\n",
    "elu_alpha = 0.5\n",
    "selu_alpha = 1.6732\n",
    "selu_scale = 1.0507\n",
    "\n",
    "###################\n",
    "# TODO: YOUR CODE #\n",
    "###################\n",
    "sigmoid = lambda x: x\n",
    "leaky_relu = lambda x: x\n",
    "relu = lambda x: x\n",
    "elu = lambda x: x\n",
    "selu = lambda x: x\n",
    "tanh = lambda x: x\n",
    "####################\n",
    "# END OF YOUR CODE #\n",
    "####################\n",
    "\n",
    "activations = {\n",
    "    \"Sigmoid\": sigmoid,\n",
    "    \"LeakyReLU\": leaky_relu,\n",
    "    \"ReLU\": relu,\n",
    "    \"ELU\": elu,\n",
    "    \"SeLU\": selu,\n",
    "    \"Tanh\": tanh\n",
    "}\n",
    "\n",
    "# Ground Truth activations\n",
    "GT_Act = {\n",
    "    \"Sigmoid\": [0.00669285092428, 0.0200575365379, 0.0585369028744, 0.158869104881, 0.364576440742, \n",
    "                0.635423559258, 0.841130895119, 0.941463097126, 0.979942463462, 0.993307149076],\n",
    "    \"LeakyReLU\": [-0.5, -0.388888888889, -0.277777777778, -0.166666666667, -0.0555555555556, \n",
    "                  0.555555555556, 1.66666666667, 2.77777777778, 3.88888888889, 5.0],\n",
    "    \"ReLU\": [-0.0, -0.0, -0.0, -0.0, -0.0, 0.555555555556, 1.66666666667, 2.77777777778, 3.88888888889, 5.0],\n",
    "    \"ELU\": [-0.4966310265, -0.489765962143, -0.468911737989, -0.405562198581, -0.213123289631, \n",
    "            0.555555555556, 1.66666666667, 2.77777777778, 3.88888888889, 5.0],\n",
    "    \"SeLU\": [-1.74618571868, -1.72204772347, -1.64872296837, -1.42598202974, -0.749354802287, \n",
    "             0.583722222222, 1.75116666667, 2.91861111111, 4.08605555556, 5.2535],\n",
    "    \"Tanh\": [-0.999909204263, -0.999162466631, -0.992297935288, -0.931109608668, -0.504672397722, \n",
    "             0.504672397722, 0.931109608668, 0.992297935288, 0.999162466631, 0.999909204263]\n",
    "} \n",
    "\n",
    "for label in activations:\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.plot(X, activations[label](X), color='darkorchid', lw=lw, label=label)\n",
    "    assert rel_error(activations[label](XS), GT_Act[label]) < 1e-9, \\\n",
    "           \"Your implementation of {} might be wrong\".format(label)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.axhline(0, color='black')\n",
    "    ax.axvline(0, color='black')\n",
    "    ax.set_title('{}'.format(label), fontsize=14)\n",
    "    plt.xlabel(r\"X\")\n",
    "    plt.ylabel(r\"Y\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
