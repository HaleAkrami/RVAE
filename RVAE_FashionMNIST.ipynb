{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply RVAE for FashionMNIST data set\n",
    "* <b>Objective:</b> In this problem, the purpose is trian a robust varational autoencoder when the training is polluted with outliers. Here we chose shoes and sneakers as inliers classes and samples from other categories as outliers. Since these images contain a significant range of gray scales, we chose the Gaussian model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import math\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import fashion_mnist\n",
    "import scipy.io as spio\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define parameters\n",
    "seed:random seed \n",
    "epochs:number of epochs\n",
    "CODE_SIZE: z dimention\n",
    "SIGMA:constant variance for Guassian loss function\n",
    "batch_size:batch size for training\n",
    "log_interval:how many batches to wait before logging training status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10004\n",
    "epochs = 150 \n",
    "batch_size = 120\n",
    "log_interval = 10\n",
    "CODE_SIZE = 20\n",
    "SIGMA = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creat data for test loader and train loader\n",
    "input: anomoly percentage\n",
    "output: train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(frac_anom):\n",
    "\n",
    "    torch.manual_seed(seed=seed)\n",
    "    np.random.seed(seed=seed)\n",
    "\n",
    "    (X, X_lab), (_test_images, _test_lab) = fashion_mnist.load_data()\n",
    "    X_lab = np.array(X_lab)\n",
    "\n",
    "    # find other categories\n",
    "    ind = np.isin(X_lab, (0, 1, 2, 3, 4, 5, 6, 8))  #(1, 5, 7, 9)\n",
    "    X_lab_outliers = X_lab[ind]\n",
    "    X_outliers = X[ind]\n",
    "\n",
    "    # find sneaker and ankle boots\n",
    "    ind = np.isin(X_lab, (7, 9))  # (0, 2, 3, 4, 6))  #\n",
    "    X_lab = X_lab[ind]\n",
    "    X = X[ind]\n",
    "\n",
    "    #normalize the data\n",
    "    X = X / 255.0\n",
    "    X_outliers = X_outliers / 255.0\n",
    "\n",
    "    # add ouliers to the data the label for outliers is 10\n",
    "    Nsamp = np.int(np.rint(len(X) * frac_anom)) + 1\n",
    "    X[:Nsamp, :, :] = X_outliers[:Nsamp, :, :]\n",
    "    X_lab[:Nsamp] = 10\n",
    "\n",
    "    #split data to train and test\n",
    "    X_train, X_test, X_lab_train, X_lab_test = train_test_split(\n",
    "        X, X_lab, test_size=0.33, random_state=10003)\n",
    "    X_train = X_train.reshape((len(X_train), np.prod(X_train.shape[1:])))\n",
    "    X_test = X_test.reshape((len(X_test), np.prod(X_test.shape[1:])))\n",
    "\n",
    "\n",
    "    \n",
    "    #append samples and labels\n",
    "    train_data = []\n",
    "    for i in range(len(X_train)):\n",
    "        train_data.append(\n",
    "            [torch.from_numpy(X_train[i]).float(), X_lab_train[i]])\n",
    "\n",
    "    test_data = []\n",
    "    for i in range(len(X_test)):\n",
    "        test_data.append(\n",
    "            [torch.from_numpy(X_test[i]).float(), X_lab_test[i]])\n",
    "        \n",
    "        \n",
    "    #generate train loader and test loader\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                              batch_size=len(test_data),\n",
    "                                              shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define MSE loss and beta loss for Guassian posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE loss\n",
    "def MSE_loss(Y, X):\n",
    "    ret = (X - Y)**2\n",
    "    ret = torch.sum(ret)\n",
    "    return ret\n",
    "\n",
    "#beta loss\n",
    "def SE_loss(Y,X):\n",
    "    ret = (X - Y)**2\n",
    "    ret = torch.sum(ret,1)\n",
    "    return ret\n",
    "    \n",
    "def Gaussian_CE_loss(Y, X, beta, sigma=SIGMA):  # 784 for mnist\n",
    "    Dim = Y.shape[1]\n",
    "    const1 = -((1 + beta) / beta)\n",
    "    const2 = 1 / pow((2 * math.pi * (sigma**2)), (beta * Dim / 2))\n",
    "    SE = SE_loss(Y, X)\n",
    "    term1 = torch.exp(-(beta / (2 * (sigma**2))) * SE)\n",
    "    loss = torch.sum(const1 * (const2* term1 - 1))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def beta_loss_function(recon_x, x, mu, logvar, beta):\n",
    "\n",
    "    if beta > 0:\n",
    "        # If beta is nonzero, use the beta entropy\n",
    "        BBCE = Gaussian_CE_loss(recon_x.view(-1, 784), x.view(-1, 784), beta)\n",
    "    else:\n",
    "        # if beta is zero use binary cross entropy\n",
    "        BBCE = MSE_loss(recon_x.view(-1, 784), x.view(-1, 784))\n",
    "\n",
    "    # compute KL divergence\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BBCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RVAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, CODE_SIZE)\n",
    "        self.fc22 = nn.Linear(400, CODE_SIZE)\n",
    "        self.fc3 = nn.Linear(CODE_SIZE, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "    \n",
    "    # for reseting network weights\n",
    "    def weight_reset(self):\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                m.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model reset\n",
    "This function calls weight_reset from the network class and reset the weights of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_reset():\n",
    "    model.weight_reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RVAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function\n",
    "input: number of epochs, value of beta\n",
    "\n",
    "Prints loss after each log intervalof bathces and after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, beta_val):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, data_lab) in enumerate(train_loader):\n",
    "   \n",
    "        data = (data).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = beta_loss_function(recon_batch, data, mu, logvar, beta=beta_val)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing function\n",
    "input: number of epochs, value of beta\n",
    "out put: total loss for the test samples, loss for the inlier samples in the test, loss for the out lier samples in the test\n",
    "Saves the resconstruction of 8 random samples(4 inliers, 4 outliers)\n",
    "Saves data, reconstruction and anomolies lables in a npz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(frac_anom, beta_val):\n",
    "    model.eval()\n",
    "    test_loss_total = 0\n",
    "    test_loss_anom = 0\n",
    "    num_anom = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, data_lab) in enumerate(test_loader):\n",
    "        \n",
    "            data = (data).to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            anom_lab = data_lab == 10\n",
    "            num_anom += np.sum(anom_lab.numpy())  # count number of anomalies\n",
    "            anom_lab = (anom_lab[:, None].float()).to(device)\n",
    "\n",
    "            test_loss_anom += MSE_loss(recon_batch * anom_lab,\n",
    "                                      data * anom_lab).item()\n",
    "            test_loss_total += MSE_loss(recon_batch, data).item()\n",
    "\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 100)\n",
    "                samp=[4, 14, 50, 60, 25, 29, 32, 65]\n",
    "                comparison = torch.cat([\n",
    "                    data.view(len(recon_batch), 1, 28, 28)[samp],\n",
    "                    recon_batch.view(len(recon_batch), 1, 28, 28)[samp]\n",
    "                ])\n",
    "                save_image(comparison.cpu(),\n",
    "                           'results/fashion_mnist_recon_shallow_' +\n",
    "                           str(beta_val) + '_' + str(frac_anom) + '.png',\n",
    "                           nrow=n)\n",
    "\n",
    "        np.savez('results/fashion_mnist_' + str(beta_val) + '_' +\n",
    "                 str(frac_anom) + '.npz',\n",
    "                 recon=recon_batch.cpu(),\n",
    "                 data=data.cpu(),\n",
    "                 anom_lab=anom_lab.cpu())\n",
    "\n",
    "    test_loss_normals = (test_loss_total - test_loss_anom) / (\n",
    "        len(test_loader.dataset) - num_anom)\n",
    "    test_loss_anom /= num_anom\n",
    "    test_loss_total /= len(test_loader.dataset)\n",
    "\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss_total))\n",
    "\n",
    "    return test_loss_total, test_loss_anom, test_loss_normals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function\n",
    "\n",
    "Runs training and testing for a givern values of beta and percentage of anomolies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/8040 (0%)]\tLoss: 148.044954\n",
      "Train Epoch: 1 [1200/8040 (15%)]\tLoss: 65.095964\n",
      "Train Epoch: 1 [2400/8040 (30%)]\tLoss: 49.679757\n",
      "Train Epoch: 1 [3600/8040 (45%)]\tLoss: 40.826095\n",
      "Train Epoch: 1 [4800/8040 (60%)]\tLoss: 35.149117\n",
      "Train Epoch: 1 [6000/8040 (75%)]\tLoss: 34.623633\n",
      "Train Epoch: 1 [7200/8040 (90%)]\tLoss: 31.267784\n",
      "====> Epoch: 1 Average loss: 49.2008\n",
      "epoch: 1, beta=0, frac_anom=0.01\n",
      "Train Epoch: 2 [0/8040 (0%)]\tLoss: 31.286776\n",
      "Train Epoch: 2 [1200/8040 (15%)]\tLoss: 30.850677\n",
      "Train Epoch: 2 [2400/8040 (30%)]\tLoss: 31.270905\n",
      "Train Epoch: 2 [3600/8040 (45%)]\tLoss: 27.609705\n",
      "Train Epoch: 2 [4800/8040 (60%)]\tLoss: 30.763525\n",
      "Train Epoch: 2 [6000/8040 (75%)]\tLoss: 28.453111\n",
      "Train Epoch: 2 [7200/8040 (90%)]\tLoss: 29.369615\n",
      "====> Epoch: 2 Average loss: 29.8053\n",
      "epoch: 2, beta=0, frac_anom=0.01\n",
      "Train Epoch: 3 [0/8040 (0%)]\tLoss: 31.534399\n",
      "Train Epoch: 3 [1200/8040 (15%)]\tLoss: 29.216134\n",
      "Train Epoch: 3 [2400/8040 (30%)]\tLoss: 28.247026\n",
      "Train Epoch: 3 [3600/8040 (45%)]\tLoss: 28.512516\n",
      "Train Epoch: 3 [4800/8040 (60%)]\tLoss: 29.428434\n",
      "Train Epoch: 3 [6000/8040 (75%)]\tLoss: 27.539563\n",
      "Train Epoch: 3 [7200/8040 (90%)]\tLoss: 26.727185\n",
      "====> Epoch: 3 Average loss: 28.1062\n",
      "epoch: 3, beta=0, frac_anom=0.01\n",
      "Train Epoch: 4 [0/8040 (0%)]\tLoss: 26.723222\n",
      "Train Epoch: 4 [1200/8040 (15%)]\tLoss: 25.084727\n",
      "Train Epoch: 4 [2400/8040 (30%)]\tLoss: 26.003151\n",
      "Train Epoch: 4 [3600/8040 (45%)]\tLoss: 25.041040\n",
      "Train Epoch: 4 [4800/8040 (60%)]\tLoss: 26.957477\n",
      "Train Epoch: 4 [6000/8040 (75%)]\tLoss: 28.067995\n",
      "Train Epoch: 4 [7200/8040 (90%)]\tLoss: 23.357304\n",
      "====> Epoch: 4 Average loss: 26.4765\n",
      "epoch: 4, beta=0, frac_anom=0.01\n",
      "Train Epoch: 5 [0/8040 (0%)]\tLoss: 25.420695\n",
      "Train Epoch: 5 [1200/8040 (15%)]\tLoss: 23.899697\n",
      "Train Epoch: 5 [2400/8040 (30%)]\tLoss: 24.779574\n",
      "Train Epoch: 5 [3600/8040 (45%)]\tLoss: 26.028259\n",
      "Train Epoch: 5 [4800/8040 (60%)]\tLoss: 27.317407\n",
      "Train Epoch: 5 [6000/8040 (75%)]\tLoss: 25.900798\n",
      "Train Epoch: 5 [7200/8040 (90%)]\tLoss: 24.625053\n",
      "====> Epoch: 5 Average loss: 25.2629\n",
      "epoch: 5, beta=0, frac_anom=0.01\n",
      "Train Epoch: 6 [0/8040 (0%)]\tLoss: 24.206942\n",
      "Train Epoch: 6 [1200/8040 (15%)]\tLoss: 23.421025\n",
      "Train Epoch: 6 [2400/8040 (30%)]\tLoss: 25.163556\n",
      "Train Epoch: 6 [3600/8040 (45%)]\tLoss: 23.751241\n",
      "Train Epoch: 6 [4800/8040 (60%)]\tLoss: 22.572807\n",
      "Train Epoch: 6 [6000/8040 (75%)]\tLoss: 24.222644\n",
      "Train Epoch: 6 [7200/8040 (90%)]\tLoss: 24.243158\n",
      "====> Epoch: 6 Average loss: 24.3131\n",
      "epoch: 6, beta=0, frac_anom=0.01\n",
      "Train Epoch: 7 [0/8040 (0%)]\tLoss: 23.559066\n",
      "Train Epoch: 7 [1200/8040 (15%)]\tLoss: 22.896659\n",
      "Train Epoch: 7 [2400/8040 (30%)]\tLoss: 26.425631\n",
      "Train Epoch: 7 [3600/8040 (45%)]\tLoss: 24.593905\n",
      "Train Epoch: 7 [4800/8040 (60%)]\tLoss: 24.528491\n",
      "Train Epoch: 7 [6000/8040 (75%)]\tLoss: 25.178054\n",
      "Train Epoch: 7 [7200/8040 (90%)]\tLoss: 23.577216\n",
      "====> Epoch: 7 Average loss: 23.7551\n",
      "epoch: 7, beta=0, frac_anom=0.01\n",
      "Train Epoch: 8 [0/8040 (0%)]\tLoss: 22.239060\n",
      "Train Epoch: 8 [1200/8040 (15%)]\tLoss: 22.706616\n",
      "Train Epoch: 8 [2400/8040 (30%)]\tLoss: 25.423956\n",
      "Train Epoch: 8 [3600/8040 (45%)]\tLoss: 25.042265\n",
      "Train Epoch: 8 [4800/8040 (60%)]\tLoss: 22.471739\n",
      "Train Epoch: 8 [6000/8040 (75%)]\tLoss: 23.638450\n",
      "Train Epoch: 8 [7200/8040 (90%)]\tLoss: 22.126711\n",
      "====> Epoch: 8 Average loss: 23.1808\n",
      "epoch: 8, beta=0, frac_anom=0.01\n",
      "Train Epoch: 9 [0/8040 (0%)]\tLoss: 21.500053\n",
      "Train Epoch: 9 [1200/8040 (15%)]\tLoss: 22.575519\n",
      "Train Epoch: 9 [2400/8040 (30%)]\tLoss: 22.496102\n",
      "Train Epoch: 9 [3600/8040 (45%)]\tLoss: 22.591189\n",
      "Train Epoch: 9 [4800/8040 (60%)]\tLoss: 21.096293\n",
      "Train Epoch: 9 [6000/8040 (75%)]\tLoss: 23.297685\n",
      "Train Epoch: 9 [7200/8040 (90%)]\tLoss: 22.635050\n",
      "====> Epoch: 9 Average loss: 22.7443\n",
      "epoch: 9, beta=0, frac_anom=0.01\n",
      "Train Epoch: 10 [0/8040 (0%)]\tLoss: 23.030396\n",
      "Train Epoch: 10 [1200/8040 (15%)]\tLoss: 23.534347\n",
      "Train Epoch: 10 [2400/8040 (30%)]\tLoss: 22.149858\n",
      "Train Epoch: 10 [3600/8040 (45%)]\tLoss: 22.130933\n",
      "Train Epoch: 10 [4800/8040 (60%)]\tLoss: 22.518791\n",
      "Train Epoch: 10 [6000/8040 (75%)]\tLoss: 21.672974\n",
      "Train Epoch: 10 [7200/8040 (90%)]\tLoss: 22.252950\n",
      "====> Epoch: 10 Average loss: 22.4382\n",
      "epoch: 10, beta=0, frac_anom=0.01\n",
      "Train Epoch: 11 [0/8040 (0%)]\tLoss: 22.154582\n",
      "Train Epoch: 11 [1200/8040 (15%)]\tLoss: 22.592322\n",
      "Train Epoch: 11 [2400/8040 (30%)]\tLoss: 21.111888\n",
      "Train Epoch: 11 [3600/8040 (45%)]\tLoss: 22.967053\n",
      "Train Epoch: 11 [4800/8040 (60%)]\tLoss: 21.850415\n",
      "Train Epoch: 11 [6000/8040 (75%)]\tLoss: 21.960706\n",
      "Train Epoch: 11 [7200/8040 (90%)]\tLoss: 22.313247\n",
      "====> Epoch: 11 Average loss: 22.0494\n",
      "epoch: 11, beta=0, frac_anom=0.01\n",
      "Train Epoch: 12 [0/8040 (0%)]\tLoss: 21.831364\n",
      "Train Epoch: 12 [1200/8040 (15%)]\tLoss: 21.744248\n",
      "Train Epoch: 12 [2400/8040 (30%)]\tLoss: 21.276668\n",
      "Train Epoch: 12 [3600/8040 (45%)]\tLoss: 21.063715\n",
      "Train Epoch: 12 [4800/8040 (60%)]\tLoss: 21.935203\n",
      "Train Epoch: 12 [6000/8040 (75%)]\tLoss: 21.383643\n",
      "Train Epoch: 12 [7200/8040 (90%)]\tLoss: 21.716406\n",
      "====> Epoch: 12 Average loss: 21.7376\n",
      "epoch: 12, beta=0, frac_anom=0.01\n",
      "Train Epoch: 13 [0/8040 (0%)]\tLoss: 20.929470\n",
      "Train Epoch: 13 [1200/8040 (15%)]\tLoss: 21.307340\n",
      "Train Epoch: 13 [2400/8040 (30%)]\tLoss: 22.458354\n",
      "Train Epoch: 13 [3600/8040 (45%)]\tLoss: 22.019663\n",
      "Train Epoch: 13 [4800/8040 (60%)]\tLoss: 21.400012\n",
      "Train Epoch: 13 [6000/8040 (75%)]\tLoss: 21.126626\n",
      "Train Epoch: 13 [7200/8040 (90%)]\tLoss: 21.384875\n",
      "====> Epoch: 13 Average loss: 21.6504\n",
      "epoch: 13, beta=0, frac_anom=0.01\n",
      "Train Epoch: 14 [0/8040 (0%)]\tLoss: 22.062402\n",
      "Train Epoch: 14 [1200/8040 (15%)]\tLoss: 21.265468\n",
      "Train Epoch: 14 [2400/8040 (30%)]\tLoss: 22.848022\n",
      "Train Epoch: 14 [3600/8040 (45%)]\tLoss: 22.623189\n",
      "Train Epoch: 14 [4800/8040 (60%)]\tLoss: 20.960246\n",
      "Train Epoch: 14 [6000/8040 (75%)]\tLoss: 20.277964\n",
      "Train Epoch: 14 [7200/8040 (90%)]\tLoss: 20.026902\n",
      "====> Epoch: 14 Average loss: 21.4506\n",
      "epoch: 14, beta=0, frac_anom=0.01\n",
      "Train Epoch: 15 [0/8040 (0%)]\tLoss: 20.683695\n",
      "Train Epoch: 15 [1200/8040 (15%)]\tLoss: 21.107210\n",
      "Train Epoch: 15 [2400/8040 (30%)]\tLoss: 21.809037\n",
      "Train Epoch: 15 [3600/8040 (45%)]\tLoss: 20.446419\n",
      "Train Epoch: 15 [4800/8040 (60%)]\tLoss: 23.784347\n",
      "Train Epoch: 15 [6000/8040 (75%)]\tLoss: 22.164876\n",
      "Train Epoch: 15 [7200/8040 (90%)]\tLoss: 21.132068\n",
      "====> Epoch: 15 Average loss: 21.3702\n",
      "epoch: 15, beta=0, frac_anom=0.01\n",
      "Train Epoch: 16 [0/8040 (0%)]\tLoss: 20.946301\n",
      "Train Epoch: 16 [1200/8040 (15%)]\tLoss: 21.195907\n",
      "Train Epoch: 16 [2400/8040 (30%)]\tLoss: 21.890566\n",
      "Train Epoch: 16 [3600/8040 (45%)]\tLoss: 22.274146\n",
      "Train Epoch: 16 [4800/8040 (60%)]\tLoss: 20.714036\n",
      "Train Epoch: 16 [6000/8040 (75%)]\tLoss: 22.562819\n",
      "Train Epoch: 16 [7200/8040 (90%)]\tLoss: 20.790218\n",
      "====> Epoch: 16 Average loss: 21.2511\n",
      "epoch: 16, beta=0, frac_anom=0.01\n",
      "Train Epoch: 17 [0/8040 (0%)]\tLoss: 21.441597\n",
      "Train Epoch: 17 [1200/8040 (15%)]\tLoss: 21.273745\n",
      "Train Epoch: 17 [2400/8040 (30%)]\tLoss: 20.209155\n",
      "Train Epoch: 17 [3600/8040 (45%)]\tLoss: 21.133059\n",
      "Train Epoch: 17 [4800/8040 (60%)]\tLoss: 20.196027\n",
      "Train Epoch: 17 [6000/8040 (75%)]\tLoss: 20.515285\n",
      "Train Epoch: 17 [7200/8040 (90%)]\tLoss: 20.328094\n",
      "====> Epoch: 17 Average loss: 21.1907\n",
      "epoch: 17, beta=0, frac_anom=0.01\n",
      "Train Epoch: 18 [0/8040 (0%)]\tLoss: 21.035876\n",
      "Train Epoch: 18 [1200/8040 (15%)]\tLoss: 19.580412\n",
      "Train Epoch: 18 [2400/8040 (30%)]\tLoss: 21.908787\n",
      "Train Epoch: 18 [3600/8040 (45%)]\tLoss: 21.085437\n",
      "Train Epoch: 18 [4800/8040 (60%)]\tLoss: 20.283964\n",
      "Train Epoch: 18 [6000/8040 (75%)]\tLoss: 20.476139\n",
      "Train Epoch: 18 [7200/8040 (90%)]\tLoss: 20.500012\n",
      "====> Epoch: 18 Average loss: 21.0658\n",
      "epoch: 18, beta=0, frac_anom=0.01\n",
      "Train Epoch: 19 [0/8040 (0%)]\tLoss: 20.122557\n",
      "Train Epoch: 19 [1200/8040 (15%)]\tLoss: 20.301963\n",
      "Train Epoch: 19 [2400/8040 (30%)]\tLoss: 21.033931\n",
      "Train Epoch: 19 [3600/8040 (45%)]\tLoss: 19.533586\n",
      "Train Epoch: 19 [4800/8040 (60%)]\tLoss: 21.026752\n",
      "Train Epoch: 19 [6000/8040 (75%)]\tLoss: 21.544102\n",
      "Train Epoch: 19 [7200/8040 (90%)]\tLoss: 21.944088\n",
      "====> Epoch: 19 Average loss: 20.9668\n",
      "epoch: 19, beta=0, frac_anom=0.01\n",
      "Train Epoch: 20 [0/8040 (0%)]\tLoss: 21.207497\n",
      "Train Epoch: 20 [1200/8040 (15%)]\tLoss: 20.754938\n",
      "Train Epoch: 20 [2400/8040 (30%)]\tLoss: 20.435130\n",
      "Train Epoch: 20 [3600/8040 (45%)]\tLoss: 22.070982\n",
      "Train Epoch: 20 [4800/8040 (60%)]\tLoss: 19.272729\n",
      "Train Epoch: 20 [6000/8040 (75%)]\tLoss: 20.629458\n",
      "Train Epoch: 20 [7200/8040 (90%)]\tLoss: 21.368123\n",
      "====> Epoch: 20 Average loss: 20.9628\n",
      "epoch: 20, beta=0, frac_anom=0.01\n",
      "Train Epoch: 21 [0/8040 (0%)]\tLoss: 21.541219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 21 [1200/8040 (15%)]\tLoss: 21.447196\n",
      "Train Epoch: 21 [2400/8040 (30%)]\tLoss: 21.565450\n",
      "Train Epoch: 21 [3600/8040 (45%)]\tLoss: 21.191667\n",
      "Train Epoch: 21 [4800/8040 (60%)]\tLoss: 20.174821\n",
      "Train Epoch: 21 [6000/8040 (75%)]\tLoss: 20.789294\n",
      "Train Epoch: 21 [7200/8040 (90%)]\tLoss: 20.410522\n",
      "====> Epoch: 21 Average loss: 20.8854\n",
      "epoch: 21, beta=0, frac_anom=0.01\n",
      "Train Epoch: 22 [0/8040 (0%)]\tLoss: 21.069674\n",
      "Train Epoch: 22 [1200/8040 (15%)]\tLoss: 20.868742\n",
      "Train Epoch: 22 [2400/8040 (30%)]\tLoss: 21.285433\n",
      "Train Epoch: 22 [3600/8040 (45%)]\tLoss: 21.170522\n",
      "Train Epoch: 22 [4800/8040 (60%)]\tLoss: 19.800435\n",
      "Train Epoch: 22 [6000/8040 (75%)]\tLoss: 19.726198\n",
      "Train Epoch: 22 [7200/8040 (90%)]\tLoss: 21.361662\n",
      "====> Epoch: 22 Average loss: 20.7231\n",
      "epoch: 22, beta=0, frac_anom=0.01\n",
      "Train Epoch: 23 [0/8040 (0%)]\tLoss: 21.154189\n",
      "Train Epoch: 23 [1200/8040 (15%)]\tLoss: 23.634395\n",
      "Train Epoch: 23 [2400/8040 (30%)]\tLoss: 20.669847\n",
      "Train Epoch: 23 [3600/8040 (45%)]\tLoss: 21.187146\n",
      "Train Epoch: 23 [4800/8040 (60%)]\tLoss: 21.291923\n",
      "Train Epoch: 23 [6000/8040 (75%)]\tLoss: 19.715281\n",
      "Train Epoch: 23 [7200/8040 (90%)]\tLoss: 20.461117\n",
      "====> Epoch: 23 Average loss: 20.7371\n",
      "epoch: 23, beta=0, frac_anom=0.01\n",
      "Train Epoch: 24 [0/8040 (0%)]\tLoss: 20.339085\n",
      "Train Epoch: 24 [1200/8040 (15%)]\tLoss: 19.035946\n",
      "Train Epoch: 24 [2400/8040 (30%)]\tLoss: 21.316018\n",
      "Train Epoch: 24 [3600/8040 (45%)]\tLoss: 20.223446\n",
      "Train Epoch: 24 [4800/8040 (60%)]\tLoss: 20.157251\n",
      "Train Epoch: 24 [6000/8040 (75%)]\tLoss: 19.970935\n",
      "Train Epoch: 24 [7200/8040 (90%)]\tLoss: 20.692297\n",
      "====> Epoch: 24 Average loss: 20.6930\n",
      "epoch: 24, beta=0, frac_anom=0.01\n",
      "Train Epoch: 25 [0/8040 (0%)]\tLoss: 20.933889\n",
      "Train Epoch: 25 [1200/8040 (15%)]\tLoss: 21.464343\n",
      "Train Epoch: 25 [2400/8040 (30%)]\tLoss: 19.174579\n",
      "Train Epoch: 25 [3600/8040 (45%)]\tLoss: 21.199196\n",
      "Train Epoch: 25 [4800/8040 (60%)]\tLoss: 20.797557\n",
      "Train Epoch: 25 [6000/8040 (75%)]\tLoss: 20.865723\n",
      "Train Epoch: 25 [7200/8040 (90%)]\tLoss: 19.647624\n",
      "====> Epoch: 25 Average loss: 20.5882\n",
      "epoch: 25, beta=0, frac_anom=0.01\n",
      "Train Epoch: 26 [0/8040 (0%)]\tLoss: 19.645789\n",
      "Train Epoch: 26 [1200/8040 (15%)]\tLoss: 20.202712\n",
      "Train Epoch: 26 [2400/8040 (30%)]\tLoss: 19.842277\n",
      "Train Epoch: 26 [3600/8040 (45%)]\tLoss: 21.376009\n",
      "Train Epoch: 26 [4800/8040 (60%)]\tLoss: 21.210579\n",
      "Train Epoch: 26 [6000/8040 (75%)]\tLoss: 21.302004\n",
      "Train Epoch: 26 [7200/8040 (90%)]\tLoss: 20.987781\n",
      "====> Epoch: 26 Average loss: 20.5371\n",
      "epoch: 26, beta=0, frac_anom=0.01\n",
      "Train Epoch: 27 [0/8040 (0%)]\tLoss: 20.847795\n",
      "Train Epoch: 27 [1200/8040 (15%)]\tLoss: 20.122099\n",
      "Train Epoch: 27 [2400/8040 (30%)]\tLoss: 20.333173\n",
      "Train Epoch: 27 [3600/8040 (45%)]\tLoss: 21.727494\n",
      "Train Epoch: 27 [4800/8040 (60%)]\tLoss: 20.704993\n",
      "Train Epoch: 27 [6000/8040 (75%)]\tLoss: 21.148737\n",
      "Train Epoch: 27 [7200/8040 (90%)]\tLoss: 20.302498\n",
      "====> Epoch: 27 Average loss: 20.5132\n",
      "epoch: 27, beta=0, frac_anom=0.01\n",
      "Train Epoch: 28 [0/8040 (0%)]\tLoss: 20.473155\n",
      "Train Epoch: 28 [1200/8040 (15%)]\tLoss: 20.861041\n",
      "Train Epoch: 28 [2400/8040 (30%)]\tLoss: 21.054704\n",
      "Train Epoch: 28 [3600/8040 (45%)]\tLoss: 19.234473\n",
      "Train Epoch: 28 [4800/8040 (60%)]\tLoss: 21.281761\n",
      "Train Epoch: 28 [6000/8040 (75%)]\tLoss: 20.031852\n",
      "Train Epoch: 28 [7200/8040 (90%)]\tLoss: 19.003896\n",
      "====> Epoch: 28 Average loss: 20.4325\n",
      "epoch: 28, beta=0, frac_anom=0.01\n",
      "Train Epoch: 29 [0/8040 (0%)]\tLoss: 19.823401\n",
      "Train Epoch: 29 [1200/8040 (15%)]\tLoss: 19.577537\n",
      "Train Epoch: 29 [2400/8040 (30%)]\tLoss: 20.433000\n",
      "Train Epoch: 29 [3600/8040 (45%)]\tLoss: 20.964571\n",
      "Train Epoch: 29 [4800/8040 (60%)]\tLoss: 19.570876\n",
      "Train Epoch: 29 [6000/8040 (75%)]\tLoss: 20.949117\n",
      "Train Epoch: 29 [7200/8040 (90%)]\tLoss: 20.099019\n",
      "====> Epoch: 29 Average loss: 20.4073\n",
      "epoch: 29, beta=0, frac_anom=0.01\n",
      "Train Epoch: 30 [0/8040 (0%)]\tLoss: 20.545650\n",
      "Train Epoch: 30 [1200/8040 (15%)]\tLoss: 19.015894\n",
      "Train Epoch: 30 [2400/8040 (30%)]\tLoss: 21.693294\n",
      "Train Epoch: 30 [3600/8040 (45%)]\tLoss: 19.925134\n",
      "Train Epoch: 30 [4800/8040 (60%)]\tLoss: 20.992653\n",
      "Train Epoch: 30 [6000/8040 (75%)]\tLoss: 20.813704\n",
      "Train Epoch: 30 [7200/8040 (90%)]\tLoss: 21.233130\n",
      "====> Epoch: 30 Average loss: 20.3893\n",
      "epoch: 30, beta=0, frac_anom=0.01\n",
      "Train Epoch: 31 [0/8040 (0%)]\tLoss: 18.975012\n",
      "Train Epoch: 31 [1200/8040 (15%)]\tLoss: 19.282259\n",
      "Train Epoch: 31 [2400/8040 (30%)]\tLoss: 19.804067\n",
      "Train Epoch: 31 [3600/8040 (45%)]\tLoss: 20.928487\n",
      "Train Epoch: 31 [4800/8040 (60%)]\tLoss: 19.188660\n",
      "Train Epoch: 31 [6000/8040 (75%)]\tLoss: 19.204376\n",
      "Train Epoch: 31 [7200/8040 (90%)]\tLoss: 20.145858\n",
      "====> Epoch: 31 Average loss: 20.3115\n",
      "epoch: 31, beta=0, frac_anom=0.01\n",
      "Train Epoch: 32 [0/8040 (0%)]\tLoss: 20.200920\n",
      "Train Epoch: 32 [1200/8040 (15%)]\tLoss: 20.717757\n",
      "Train Epoch: 32 [2400/8040 (30%)]\tLoss: 20.148100\n",
      "Train Epoch: 32 [3600/8040 (45%)]\tLoss: 20.581488\n",
      "Train Epoch: 32 [4800/8040 (60%)]\tLoss: 20.052568\n",
      "Train Epoch: 32 [6000/8040 (75%)]\tLoss: 18.534900\n",
      "Train Epoch: 32 [7200/8040 (90%)]\tLoss: 21.599402\n",
      "====> Epoch: 32 Average loss: 20.2608\n",
      "epoch: 32, beta=0, frac_anom=0.01\n",
      "Train Epoch: 33 [0/8040 (0%)]\tLoss: 19.776426\n",
      "Train Epoch: 33 [1200/8040 (15%)]\tLoss: 20.296159\n",
      "Train Epoch: 33 [2400/8040 (30%)]\tLoss: 19.977834\n",
      "Train Epoch: 33 [3600/8040 (45%)]\tLoss: 19.961112\n",
      "Train Epoch: 33 [4800/8040 (60%)]\tLoss: 20.051084\n",
      "Train Epoch: 33 [6000/8040 (75%)]\tLoss: 20.645976\n",
      "Train Epoch: 33 [7200/8040 (90%)]\tLoss: 19.737362\n",
      "====> Epoch: 33 Average loss: 20.2276\n",
      "epoch: 33, beta=0, frac_anom=0.01\n",
      "Train Epoch: 34 [0/8040 (0%)]\tLoss: 18.449028\n",
      "Train Epoch: 34 [1200/8040 (15%)]\tLoss: 19.982778\n",
      "Train Epoch: 34 [2400/8040 (30%)]\tLoss: 20.644442\n",
      "Train Epoch: 34 [3600/8040 (45%)]\tLoss: 19.583254\n",
      "Train Epoch: 34 [4800/8040 (60%)]\tLoss: 19.621181\n",
      "Train Epoch: 34 [6000/8040 (75%)]\tLoss: 18.596065\n",
      "Train Epoch: 34 [7200/8040 (90%)]\tLoss: 20.842977\n",
      "====> Epoch: 34 Average loss: 20.2155\n",
      "epoch: 34, beta=0, frac_anom=0.01\n",
      "Train Epoch: 35 [0/8040 (0%)]\tLoss: 20.659509\n",
      "Train Epoch: 35 [1200/8040 (15%)]\tLoss: 20.636084\n",
      "Train Epoch: 35 [2400/8040 (30%)]\tLoss: 19.778428\n",
      "Train Epoch: 35 [3600/8040 (45%)]\tLoss: 19.074050\n",
      "Train Epoch: 35 [4800/8040 (60%)]\tLoss: 20.742306\n",
      "Train Epoch: 35 [6000/8040 (75%)]\tLoss: 19.506578\n",
      "Train Epoch: 35 [7200/8040 (90%)]\tLoss: 20.916231\n",
      "====> Epoch: 35 Average loss: 20.1448\n",
      "epoch: 35, beta=0, frac_anom=0.01\n",
      "Train Epoch: 36 [0/8040 (0%)]\tLoss: 20.341156\n",
      "Train Epoch: 36 [1200/8040 (15%)]\tLoss: 19.404606\n",
      "Train Epoch: 36 [2400/8040 (30%)]\tLoss: 20.556425\n",
      "Train Epoch: 36 [3600/8040 (45%)]\tLoss: 19.235828\n",
      "Train Epoch: 36 [4800/8040 (60%)]\tLoss: 19.877014\n",
      "Train Epoch: 36 [6000/8040 (75%)]\tLoss: 19.499154\n",
      "Train Epoch: 36 [7200/8040 (90%)]\tLoss: 19.903050\n",
      "====> Epoch: 36 Average loss: 20.1172\n",
      "epoch: 36, beta=0, frac_anom=0.01\n",
      "Train Epoch: 37 [0/8040 (0%)]\tLoss: 18.547245\n",
      "Train Epoch: 37 [1200/8040 (15%)]\tLoss: 20.542375\n",
      "Train Epoch: 37 [2400/8040 (30%)]\tLoss: 21.264331\n",
      "Train Epoch: 37 [3600/8040 (45%)]\tLoss: 18.953255\n",
      "Train Epoch: 37 [4800/8040 (60%)]\tLoss: 19.860034\n",
      "Train Epoch: 37 [6000/8040 (75%)]\tLoss: 20.728943\n",
      "Train Epoch: 37 [7200/8040 (90%)]\tLoss: 19.472262\n",
      "====> Epoch: 37 Average loss: 20.0671\n",
      "epoch: 37, beta=0, frac_anom=0.01\n",
      "Train Epoch: 38 [0/8040 (0%)]\tLoss: 20.370524\n",
      "Train Epoch: 38 [1200/8040 (15%)]\tLoss: 19.424677\n",
      "Train Epoch: 38 [2400/8040 (30%)]\tLoss: 20.216364\n",
      "Train Epoch: 38 [3600/8040 (45%)]\tLoss: 20.862724\n",
      "Train Epoch: 38 [4800/8040 (60%)]\tLoss: 20.264461\n",
      "Train Epoch: 38 [6000/8040 (75%)]\tLoss: 20.478044\n",
      "Train Epoch: 38 [7200/8040 (90%)]\tLoss: 18.801691\n",
      "====> Epoch: 38 Average loss: 20.0256\n",
      "epoch: 38, beta=0, frac_anom=0.01\n",
      "Train Epoch: 39 [0/8040 (0%)]\tLoss: 19.915680\n",
      "Train Epoch: 39 [1200/8040 (15%)]\tLoss: 20.144151\n",
      "Train Epoch: 39 [2400/8040 (30%)]\tLoss: 19.078430\n",
      "Train Epoch: 39 [3600/8040 (45%)]\tLoss: 19.397827\n",
      "Train Epoch: 39 [4800/8040 (60%)]\tLoss: 19.083419\n",
      "Train Epoch: 39 [6000/8040 (75%)]\tLoss: 21.307802\n",
      "Train Epoch: 39 [7200/8040 (90%)]\tLoss: 20.687667\n",
      "====> Epoch: 39 Average loss: 19.9878\n",
      "epoch: 39, beta=0, frac_anom=0.01\n",
      "Train Epoch: 40 [0/8040 (0%)]\tLoss: 21.037526\n",
      "Train Epoch: 40 [1200/8040 (15%)]\tLoss: 20.058877\n",
      "Train Epoch: 40 [2400/8040 (30%)]\tLoss: 19.983484\n",
      "Train Epoch: 40 [3600/8040 (45%)]\tLoss: 19.164016\n",
      "Train Epoch: 40 [4800/8040 (60%)]\tLoss: 19.351978\n",
      "Train Epoch: 40 [6000/8040 (75%)]\tLoss: 19.984062\n",
      "Train Epoch: 40 [7200/8040 (90%)]\tLoss: 20.396588\n",
      "====> Epoch: 40 Average loss: 19.9724\n",
      "epoch: 40, beta=0, frac_anom=0.01\n",
      "Train Epoch: 41 [0/8040 (0%)]\tLoss: 19.554565\n",
      "Train Epoch: 41 [1200/8040 (15%)]\tLoss: 20.328164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 41 [2400/8040 (30%)]\tLoss: 19.894092\n",
      "Train Epoch: 41 [3600/8040 (45%)]\tLoss: 19.388831\n",
      "Train Epoch: 41 [4800/8040 (60%)]\tLoss: 19.553412\n",
      "Train Epoch: 41 [6000/8040 (75%)]\tLoss: 20.624235\n",
      "Train Epoch: 41 [7200/8040 (90%)]\tLoss: 18.735948\n",
      "====> Epoch: 41 Average loss: 19.9840\n",
      "epoch: 41, beta=0, frac_anom=0.01\n",
      "Train Epoch: 42 [0/8040 (0%)]\tLoss: 19.440204\n",
      "Train Epoch: 42 [1200/8040 (15%)]\tLoss: 19.895844\n",
      "Train Epoch: 42 [2400/8040 (30%)]\tLoss: 20.735217\n",
      "Train Epoch: 42 [3600/8040 (45%)]\tLoss: 19.614445\n",
      "Train Epoch: 42 [4800/8040 (60%)]\tLoss: 19.929234\n",
      "Train Epoch: 42 [6000/8040 (75%)]\tLoss: 19.438692\n",
      "Train Epoch: 42 [7200/8040 (90%)]\tLoss: 20.778741\n",
      "====> Epoch: 42 Average loss: 20.0140\n",
      "epoch: 42, beta=0, frac_anom=0.01\n",
      "Train Epoch: 43 [0/8040 (0%)]\tLoss: 19.120079\n",
      "Train Epoch: 43 [1200/8040 (15%)]\tLoss: 18.712948\n",
      "Train Epoch: 43 [2400/8040 (30%)]\tLoss: 20.282322\n",
      "Train Epoch: 43 [3600/8040 (45%)]\tLoss: 19.281696\n",
      "Train Epoch: 43 [4800/8040 (60%)]\tLoss: 19.838883\n",
      "Train Epoch: 43 [6000/8040 (75%)]\tLoss: 19.875179\n",
      "Train Epoch: 43 [7200/8040 (90%)]\tLoss: 20.357340\n",
      "====> Epoch: 43 Average loss: 19.9193\n",
      "epoch: 43, beta=0, frac_anom=0.01\n",
      "Train Epoch: 44 [0/8040 (0%)]\tLoss: 19.515399\n",
      "Train Epoch: 44 [1200/8040 (15%)]\tLoss: 19.552452\n",
      "Train Epoch: 44 [2400/8040 (30%)]\tLoss: 20.305745\n",
      "Train Epoch: 44 [3600/8040 (45%)]\tLoss: 19.948690\n",
      "Train Epoch: 44 [4800/8040 (60%)]\tLoss: 19.989850\n",
      "Train Epoch: 44 [6000/8040 (75%)]\tLoss: 19.138029\n",
      "Train Epoch: 44 [7200/8040 (90%)]\tLoss: 19.348157\n",
      "====> Epoch: 44 Average loss: 19.8712\n",
      "epoch: 44, beta=0, frac_anom=0.01\n",
      "Train Epoch: 45 [0/8040 (0%)]\tLoss: 20.395846\n",
      "Train Epoch: 45 [1200/8040 (15%)]\tLoss: 19.284501\n",
      "Train Epoch: 45 [2400/8040 (30%)]\tLoss: 19.754722\n",
      "Train Epoch: 45 [3600/8040 (45%)]\tLoss: 20.959894\n",
      "Train Epoch: 45 [4800/8040 (60%)]\tLoss: 20.531761\n",
      "Train Epoch: 45 [6000/8040 (75%)]\tLoss: 20.552450\n",
      "Train Epoch: 45 [7200/8040 (90%)]\tLoss: 19.388196\n",
      "====> Epoch: 45 Average loss: 19.8340\n",
      "epoch: 45, beta=0, frac_anom=0.01\n",
      "Train Epoch: 46 [0/8040 (0%)]\tLoss: 19.529751\n",
      "Train Epoch: 46 [1200/8040 (15%)]\tLoss: 20.015853\n",
      "Train Epoch: 46 [2400/8040 (30%)]\tLoss: 19.033771\n",
      "Train Epoch: 46 [3600/8040 (45%)]\tLoss: 18.556217\n",
      "Train Epoch: 46 [4800/8040 (60%)]\tLoss: 20.596621\n",
      "Train Epoch: 46 [6000/8040 (75%)]\tLoss: 19.957115\n",
      "Train Epoch: 46 [7200/8040 (90%)]\tLoss: 20.825547\n",
      "====> Epoch: 46 Average loss: 19.8756\n",
      "epoch: 46, beta=0, frac_anom=0.01\n",
      "Train Epoch: 47 [0/8040 (0%)]\tLoss: 18.554974\n",
      "Train Epoch: 47 [1200/8040 (15%)]\tLoss: 19.049760\n",
      "Train Epoch: 47 [2400/8040 (30%)]\tLoss: 21.666252\n",
      "Train Epoch: 47 [3600/8040 (45%)]\tLoss: 20.523315\n",
      "Train Epoch: 47 [4800/8040 (60%)]\tLoss: 20.593575\n",
      "Train Epoch: 47 [6000/8040 (75%)]\tLoss: 20.148057\n",
      "Train Epoch: 47 [7200/8040 (90%)]\tLoss: 20.296830\n",
      "====> Epoch: 47 Average loss: 19.8285\n",
      "epoch: 47, beta=0, frac_anom=0.01\n",
      "Train Epoch: 48 [0/8040 (0%)]\tLoss: 18.844175\n",
      "Train Epoch: 48 [1200/8040 (15%)]\tLoss: 18.771688\n",
      "Train Epoch: 48 [2400/8040 (30%)]\tLoss: 19.236033\n",
      "Train Epoch: 48 [3600/8040 (45%)]\tLoss: 20.404692\n",
      "Train Epoch: 48 [4800/8040 (60%)]\tLoss: 20.223067\n",
      "Train Epoch: 48 [6000/8040 (75%)]\tLoss: 19.990499\n",
      "Train Epoch: 48 [7200/8040 (90%)]\tLoss: 18.722166\n",
      "====> Epoch: 48 Average loss: 19.8123\n",
      "epoch: 48, beta=0, frac_anom=0.01\n",
      "Train Epoch: 49 [0/8040 (0%)]\tLoss: 20.745070\n",
      "Train Epoch: 49 [1200/8040 (15%)]\tLoss: 19.699117\n",
      "Train Epoch: 49 [2400/8040 (30%)]\tLoss: 19.541038\n",
      "Train Epoch: 49 [3600/8040 (45%)]\tLoss: 19.467806\n",
      "Train Epoch: 49 [4800/8040 (60%)]\tLoss: 20.569875\n",
      "Train Epoch: 49 [6000/8040 (75%)]\tLoss: 20.375226\n",
      "Train Epoch: 49 [7200/8040 (90%)]\tLoss: 20.168286\n",
      "====> Epoch: 49 Average loss: 19.7389\n",
      "epoch: 49, beta=0, frac_anom=0.01\n",
      "Train Epoch: 50 [0/8040 (0%)]\tLoss: 19.185256\n",
      "Train Epoch: 50 [1200/8040 (15%)]\tLoss: 19.649202\n",
      "Train Epoch: 50 [2400/8040 (30%)]\tLoss: 20.575456\n",
      "Train Epoch: 50 [3600/8040 (45%)]\tLoss: 19.248035\n",
      "Train Epoch: 50 [4800/8040 (60%)]\tLoss: 19.638155\n",
      "Train Epoch: 50 [6000/8040 (75%)]\tLoss: 20.552791\n",
      "Train Epoch: 50 [7200/8040 (90%)]\tLoss: 20.563831\n",
      "====> Epoch: 50 Average loss: 19.7025\n",
      "epoch: 50, beta=0, frac_anom=0.01\n",
      "Train Epoch: 51 [0/8040 (0%)]\tLoss: 20.875346\n",
      "Train Epoch: 51 [1200/8040 (15%)]\tLoss: 20.666919\n",
      "Train Epoch: 51 [2400/8040 (30%)]\tLoss: 19.884237\n",
      "Train Epoch: 51 [3600/8040 (45%)]\tLoss: 20.082383\n",
      "Train Epoch: 51 [4800/8040 (60%)]\tLoss: 19.355229\n",
      "Train Epoch: 51 [6000/8040 (75%)]\tLoss: 20.995087\n",
      "Train Epoch: 51 [7200/8040 (90%)]\tLoss: 18.961489\n",
      "====> Epoch: 51 Average loss: 19.7250\n",
      "epoch: 51, beta=0, frac_anom=0.01\n",
      "Train Epoch: 52 [0/8040 (0%)]\tLoss: 20.359855\n",
      "Train Epoch: 52 [1200/8040 (15%)]\tLoss: 19.691158\n",
      "Train Epoch: 52 [2400/8040 (30%)]\tLoss: 19.405668\n",
      "Train Epoch: 52 [3600/8040 (45%)]\tLoss: 18.458525\n",
      "Train Epoch: 52 [4800/8040 (60%)]\tLoss: 20.530489\n",
      "Train Epoch: 52 [6000/8040 (75%)]\tLoss: 19.852026\n",
      "Train Epoch: 52 [7200/8040 (90%)]\tLoss: 19.638987\n",
      "====> Epoch: 52 Average loss: 19.6927\n",
      "epoch: 52, beta=0, frac_anom=0.01\n",
      "Train Epoch: 53 [0/8040 (0%)]\tLoss: 18.870406\n",
      "Train Epoch: 53 [1200/8040 (15%)]\tLoss: 20.994161\n",
      "Train Epoch: 53 [2400/8040 (30%)]\tLoss: 20.414274\n",
      "Train Epoch: 53 [3600/8040 (45%)]\tLoss: 20.155255\n",
      "Train Epoch: 53 [4800/8040 (60%)]\tLoss: 20.296602\n",
      "Train Epoch: 53 [6000/8040 (75%)]\tLoss: 19.578033\n",
      "Train Epoch: 53 [7200/8040 (90%)]\tLoss: 20.284129\n",
      "====> Epoch: 53 Average loss: 19.6572\n",
      "epoch: 53, beta=0, frac_anom=0.01\n",
      "Train Epoch: 54 [0/8040 (0%)]\tLoss: 19.559318\n",
      "Train Epoch: 54 [1200/8040 (15%)]\tLoss: 20.815356\n",
      "Train Epoch: 54 [2400/8040 (30%)]\tLoss: 19.664284\n",
      "Train Epoch: 54 [3600/8040 (45%)]\tLoss: 19.702765\n",
      "Train Epoch: 54 [4800/8040 (60%)]\tLoss: 18.753052\n",
      "Train Epoch: 54 [6000/8040 (75%)]\tLoss: 19.113914\n",
      "Train Epoch: 54 [7200/8040 (90%)]\tLoss: 19.141071\n",
      "====> Epoch: 54 Average loss: 19.6455\n",
      "epoch: 54, beta=0, frac_anom=0.01\n",
      "Train Epoch: 55 [0/8040 (0%)]\tLoss: 20.293127\n",
      "Train Epoch: 55 [1200/8040 (15%)]\tLoss: 19.558993\n",
      "Train Epoch: 55 [2400/8040 (30%)]\tLoss: 19.879293\n",
      "Train Epoch: 55 [3600/8040 (45%)]\tLoss: 19.342596\n",
      "Train Epoch: 55 [4800/8040 (60%)]\tLoss: 19.227645\n",
      "Train Epoch: 55 [6000/8040 (75%)]\tLoss: 18.706278\n",
      "Train Epoch: 55 [7200/8040 (90%)]\tLoss: 20.031006\n",
      "====> Epoch: 55 Average loss: 19.6061\n",
      "epoch: 55, beta=0, frac_anom=0.01\n",
      "Train Epoch: 56 [0/8040 (0%)]\tLoss: 19.196969\n",
      "Train Epoch: 56 [1200/8040 (15%)]\tLoss: 18.591724\n",
      "Train Epoch: 56 [2400/8040 (30%)]\tLoss: 18.947156\n",
      "Train Epoch: 56 [3600/8040 (45%)]\tLoss: 20.703628\n",
      "Train Epoch: 56 [4800/8040 (60%)]\tLoss: 18.720496\n",
      "Train Epoch: 56 [6000/8040 (75%)]\tLoss: 19.912659\n",
      "Train Epoch: 56 [7200/8040 (90%)]\tLoss: 19.329767\n",
      "====> Epoch: 56 Average loss: 19.5655\n",
      "epoch: 56, beta=0, frac_anom=0.01\n",
      "Train Epoch: 57 [0/8040 (0%)]\tLoss: 18.572573\n",
      "Train Epoch: 57 [1200/8040 (15%)]\tLoss: 19.504952\n",
      "Train Epoch: 57 [2400/8040 (30%)]\tLoss: 20.104179\n",
      "Train Epoch: 57 [3600/8040 (45%)]\tLoss: 18.731746\n",
      "Train Epoch: 57 [4800/8040 (60%)]\tLoss: 20.198271\n",
      "Train Epoch: 57 [6000/8040 (75%)]\tLoss: 20.335443\n",
      "Train Epoch: 57 [7200/8040 (90%)]\tLoss: 19.176501\n",
      "====> Epoch: 57 Average loss: 19.5568\n",
      "epoch: 57, beta=0, frac_anom=0.01\n",
      "Train Epoch: 58 [0/8040 (0%)]\tLoss: 19.579427\n",
      "Train Epoch: 58 [1200/8040 (15%)]\tLoss: 19.496753\n",
      "Train Epoch: 58 [2400/8040 (30%)]\tLoss: 19.341791\n",
      "Train Epoch: 58 [3600/8040 (45%)]\tLoss: 20.467375\n",
      "Train Epoch: 58 [4800/8040 (60%)]\tLoss: 20.920518\n",
      "Train Epoch: 58 [6000/8040 (75%)]\tLoss: 20.336206\n",
      "Train Epoch: 58 [7200/8040 (90%)]\tLoss: 19.649158\n",
      "====> Epoch: 58 Average loss: 19.5984\n",
      "epoch: 58, beta=0, frac_anom=0.01\n",
      "Train Epoch: 59 [0/8040 (0%)]\tLoss: 19.713816\n",
      "Train Epoch: 59 [1200/8040 (15%)]\tLoss: 18.609033\n",
      "Train Epoch: 59 [2400/8040 (30%)]\tLoss: 19.192993\n",
      "Train Epoch: 59 [3600/8040 (45%)]\tLoss: 20.849622\n",
      "Train Epoch: 59 [4800/8040 (60%)]\tLoss: 19.678259\n",
      "Train Epoch: 59 [6000/8040 (75%)]\tLoss: 19.242883\n",
      "Train Epoch: 59 [7200/8040 (90%)]\tLoss: 18.640991\n",
      "====> Epoch: 59 Average loss: 19.6175\n",
      "epoch: 59, beta=0, frac_anom=0.01\n",
      "Train Epoch: 60 [0/8040 (0%)]\tLoss: 19.521075\n",
      "Train Epoch: 60 [1200/8040 (15%)]\tLoss: 19.345744\n",
      "Train Epoch: 60 [2400/8040 (30%)]\tLoss: 19.271842\n",
      "Train Epoch: 60 [3600/8040 (45%)]\tLoss: 20.165263\n",
      "Train Epoch: 60 [4800/8040 (60%)]\tLoss: 19.897616\n",
      "Train Epoch: 60 [6000/8040 (75%)]\tLoss: 18.973623\n",
      "Train Epoch: 60 [7200/8040 (90%)]\tLoss: 19.103811\n",
      "====> Epoch: 60 Average loss: 19.5129\n",
      "epoch: 60, beta=0, frac_anom=0.01\n",
      "Train Epoch: 61 [0/8040 (0%)]\tLoss: 19.170884\n",
      "Train Epoch: 61 [1200/8040 (15%)]\tLoss: 20.029283\n",
      "Train Epoch: 61 [2400/8040 (30%)]\tLoss: 20.950840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 61 [3600/8040 (45%)]\tLoss: 19.671436\n",
      "Train Epoch: 61 [4800/8040 (60%)]\tLoss: 18.980664\n",
      "Train Epoch: 61 [6000/8040 (75%)]\tLoss: 19.520911\n",
      "Train Epoch: 61 [7200/8040 (90%)]\tLoss: 19.580583\n",
      "====> Epoch: 61 Average loss: 19.5392\n",
      "epoch: 61, beta=0, frac_anom=0.01\n",
      "Train Epoch: 62 [0/8040 (0%)]\tLoss: 18.602865\n",
      "Train Epoch: 62 [1200/8040 (15%)]\tLoss: 20.155284\n",
      "Train Epoch: 62 [2400/8040 (30%)]\tLoss: 19.840063\n",
      "Train Epoch: 62 [3600/8040 (45%)]\tLoss: 18.450126\n",
      "Train Epoch: 62 [4800/8040 (60%)]\tLoss: 18.942208\n",
      "Train Epoch: 62 [6000/8040 (75%)]\tLoss: 19.739492\n",
      "Train Epoch: 62 [7200/8040 (90%)]\tLoss: 20.729201\n",
      "====> Epoch: 62 Average loss: 19.4764\n",
      "epoch: 62, beta=0, frac_anom=0.01\n",
      "Train Epoch: 63 [0/8040 (0%)]\tLoss: 19.889266\n",
      "Train Epoch: 63 [1200/8040 (15%)]\tLoss: 19.016243\n",
      "Train Epoch: 63 [2400/8040 (30%)]\tLoss: 19.799141\n",
      "Train Epoch: 63 [3600/8040 (45%)]\tLoss: 19.383000\n",
      "Train Epoch: 63 [4800/8040 (60%)]\tLoss: 18.863902\n",
      "Train Epoch: 63 [6000/8040 (75%)]\tLoss: 20.162738\n",
      "Train Epoch: 63 [7200/8040 (90%)]\tLoss: 18.515194\n",
      "====> Epoch: 63 Average loss: 19.5310\n",
      "epoch: 63, beta=0, frac_anom=0.01\n",
      "Train Epoch: 64 [0/8040 (0%)]\tLoss: 20.093174\n",
      "Train Epoch: 64 [1200/8040 (15%)]\tLoss: 19.209355\n",
      "Train Epoch: 64 [2400/8040 (30%)]\tLoss: 20.241530\n",
      "Train Epoch: 64 [3600/8040 (45%)]\tLoss: 18.836523\n",
      "Train Epoch: 64 [4800/8040 (60%)]\tLoss: 19.616842\n",
      "Train Epoch: 64 [6000/8040 (75%)]\tLoss: 19.024854\n",
      "Train Epoch: 64 [7200/8040 (90%)]\tLoss: 18.933858\n",
      "====> Epoch: 64 Average loss: 19.5307\n",
      "epoch: 64, beta=0, frac_anom=0.01\n",
      "Train Epoch: 65 [0/8040 (0%)]\tLoss: 20.167074\n",
      "Train Epoch: 65 [1200/8040 (15%)]\tLoss: 19.355341\n",
      "Train Epoch: 65 [2400/8040 (30%)]\tLoss: 19.788835\n",
      "Train Epoch: 65 [3600/8040 (45%)]\tLoss: 18.914933\n",
      "Train Epoch: 65 [4800/8040 (60%)]\tLoss: 20.661654\n",
      "Train Epoch: 65 [6000/8040 (75%)]\tLoss: 20.493156\n",
      "Train Epoch: 65 [7200/8040 (90%)]\tLoss: 19.424945\n",
      "====> Epoch: 65 Average loss: 19.5545\n",
      "epoch: 65, beta=0, frac_anom=0.01\n",
      "Train Epoch: 66 [0/8040 (0%)]\tLoss: 20.106368\n",
      "Train Epoch: 66 [1200/8040 (15%)]\tLoss: 18.440511\n",
      "Train Epoch: 66 [2400/8040 (30%)]\tLoss: 18.691508\n",
      "Train Epoch: 66 [3600/8040 (45%)]\tLoss: 19.784788\n",
      "Train Epoch: 66 [4800/8040 (60%)]\tLoss: 18.281683\n",
      "Train Epoch: 66 [6000/8040 (75%)]\tLoss: 18.779742\n",
      "Train Epoch: 66 [7200/8040 (90%)]\tLoss: 20.079305\n",
      "====> Epoch: 66 Average loss: 19.4436\n",
      "epoch: 66, beta=0, frac_anom=0.01\n",
      "Train Epoch: 67 [0/8040 (0%)]\tLoss: 20.770121\n",
      "Train Epoch: 67 [1200/8040 (15%)]\tLoss: 19.166665\n",
      "Train Epoch: 67 [2400/8040 (30%)]\tLoss: 19.524599\n",
      "Train Epoch: 67 [3600/8040 (45%)]\tLoss: 18.548478\n",
      "Train Epoch: 67 [4800/8040 (60%)]\tLoss: 18.801324\n",
      "Train Epoch: 67 [6000/8040 (75%)]\tLoss: 20.581535\n",
      "Train Epoch: 67 [7200/8040 (90%)]\tLoss: 20.782214\n",
      "====> Epoch: 67 Average loss: 19.4469\n",
      "epoch: 67, beta=0, frac_anom=0.01\n",
      "Train Epoch: 68 [0/8040 (0%)]\tLoss: 18.113304\n",
      "Train Epoch: 68 [1200/8040 (15%)]\tLoss: 19.657768\n",
      "Train Epoch: 68 [2400/8040 (30%)]\tLoss: 20.201194\n",
      "Train Epoch: 68 [3600/8040 (45%)]\tLoss: 19.281199\n",
      "Train Epoch: 68 [4800/8040 (60%)]\tLoss: 19.198033\n",
      "Train Epoch: 68 [6000/8040 (75%)]\tLoss: 20.169265\n",
      "Train Epoch: 68 [7200/8040 (90%)]\tLoss: 19.597607\n",
      "====> Epoch: 68 Average loss: 19.4048\n",
      "epoch: 68, beta=0, frac_anom=0.01\n",
      "Train Epoch: 69 [0/8040 (0%)]\tLoss: 19.636556\n",
      "Train Epoch: 69 [1200/8040 (15%)]\tLoss: 18.825712\n",
      "Train Epoch: 69 [2400/8040 (30%)]\tLoss: 18.925800\n",
      "Train Epoch: 69 [3600/8040 (45%)]\tLoss: 19.469775\n",
      "Train Epoch: 69 [4800/8040 (60%)]\tLoss: 19.811015\n",
      "Train Epoch: 69 [6000/8040 (75%)]\tLoss: 19.803495\n",
      "Train Epoch: 69 [7200/8040 (90%)]\tLoss: 20.916772\n",
      "====> Epoch: 69 Average loss: 19.4122\n",
      "epoch: 69, beta=0, frac_anom=0.01\n",
      "Train Epoch: 70 [0/8040 (0%)]\tLoss: 20.132300\n",
      "Train Epoch: 70 [1200/8040 (15%)]\tLoss: 19.594572\n",
      "Train Epoch: 70 [2400/8040 (30%)]\tLoss: 19.880210\n",
      "Train Epoch: 70 [3600/8040 (45%)]\tLoss: 19.589360\n",
      "Train Epoch: 70 [4800/8040 (60%)]\tLoss: 19.307540\n",
      "Train Epoch: 70 [6000/8040 (75%)]\tLoss: 19.241813\n",
      "Train Epoch: 70 [7200/8040 (90%)]\tLoss: 18.485193\n",
      "====> Epoch: 70 Average loss: 19.4016\n",
      "epoch: 70, beta=0, frac_anom=0.01\n",
      "Train Epoch: 71 [0/8040 (0%)]\tLoss: 18.753019\n",
      "Train Epoch: 71 [1200/8040 (15%)]\tLoss: 19.057935\n",
      "Train Epoch: 71 [2400/8040 (30%)]\tLoss: 19.471218\n",
      "Train Epoch: 71 [3600/8040 (45%)]\tLoss: 19.362374\n",
      "Train Epoch: 71 [4800/8040 (60%)]\tLoss: 20.394686\n",
      "Train Epoch: 71 [6000/8040 (75%)]\tLoss: 19.674017\n",
      "Train Epoch: 71 [7200/8040 (90%)]\tLoss: 19.536440\n",
      "====> Epoch: 71 Average loss: 19.4307\n",
      "epoch: 71, beta=0, frac_anom=0.01\n",
      "Train Epoch: 72 [0/8040 (0%)]\tLoss: 18.155680\n",
      "Train Epoch: 72 [1200/8040 (15%)]\tLoss: 18.992181\n",
      "Train Epoch: 72 [2400/8040 (30%)]\tLoss: 18.098901\n",
      "Train Epoch: 72 [3600/8040 (45%)]\tLoss: 20.191766\n",
      "Train Epoch: 72 [4800/8040 (60%)]\tLoss: 19.296790\n",
      "Train Epoch: 72 [6000/8040 (75%)]\tLoss: 18.913308\n",
      "Train Epoch: 72 [7200/8040 (90%)]\tLoss: 18.841819\n",
      "====> Epoch: 72 Average loss: 19.4027\n",
      "epoch: 72, beta=0, frac_anom=0.01\n",
      "Train Epoch: 73 [0/8040 (0%)]\tLoss: 18.611975\n",
      "Train Epoch: 73 [1200/8040 (15%)]\tLoss: 20.452909\n",
      "Train Epoch: 73 [2400/8040 (30%)]\tLoss: 19.331097\n",
      "Train Epoch: 73 [3600/8040 (45%)]\tLoss: 19.715336\n",
      "Train Epoch: 73 [4800/8040 (60%)]\tLoss: 19.247624\n",
      "Train Epoch: 73 [6000/8040 (75%)]\tLoss: 19.208561\n",
      "Train Epoch: 73 [7200/8040 (90%)]\tLoss: 19.068986\n",
      "====> Epoch: 73 Average loss: 19.3939\n",
      "epoch: 73, beta=0, frac_anom=0.01\n",
      "Train Epoch: 74 [0/8040 (0%)]\tLoss: 19.225948\n",
      "Train Epoch: 74 [1200/8040 (15%)]\tLoss: 19.802287\n",
      "Train Epoch: 74 [2400/8040 (30%)]\tLoss: 19.044759\n",
      "Train Epoch: 74 [3600/8040 (45%)]\tLoss: 19.692902\n",
      "Train Epoch: 74 [4800/8040 (60%)]\tLoss: 19.262598\n",
      "Train Epoch: 74 [6000/8040 (75%)]\tLoss: 19.303286\n",
      "Train Epoch: 74 [7200/8040 (90%)]\tLoss: 19.732568\n",
      "====> Epoch: 74 Average loss: 19.3712\n",
      "epoch: 74, beta=0, frac_anom=0.01\n",
      "Train Epoch: 75 [0/8040 (0%)]\tLoss: 19.336088\n",
      "Train Epoch: 75 [1200/8040 (15%)]\tLoss: 19.250663\n",
      "Train Epoch: 75 [2400/8040 (30%)]\tLoss: 19.881934\n",
      "Train Epoch: 75 [3600/8040 (45%)]\tLoss: 19.400321\n",
      "Train Epoch: 75 [4800/8040 (60%)]\tLoss: 19.632406\n",
      "Train Epoch: 75 [6000/8040 (75%)]\tLoss: 19.731081\n",
      "Train Epoch: 75 [7200/8040 (90%)]\tLoss: 19.606512\n",
      "====> Epoch: 75 Average loss: 19.3596\n",
      "epoch: 75, beta=0, frac_anom=0.01\n",
      "Train Epoch: 76 [0/8040 (0%)]\tLoss: 18.656032\n",
      "Train Epoch: 76 [1200/8040 (15%)]\tLoss: 19.006785\n",
      "Train Epoch: 76 [2400/8040 (30%)]\tLoss: 19.856014\n",
      "Train Epoch: 76 [3600/8040 (45%)]\tLoss: 19.337581\n",
      "Train Epoch: 76 [4800/8040 (60%)]\tLoss: 19.266671\n",
      "Train Epoch: 76 [6000/8040 (75%)]\tLoss: 17.657682\n",
      "Train Epoch: 76 [7200/8040 (90%)]\tLoss: 20.121745\n",
      "====> Epoch: 76 Average loss: 19.3114\n",
      "epoch: 76, beta=0, frac_anom=0.01\n",
      "Train Epoch: 77 [0/8040 (0%)]\tLoss: 18.530282\n",
      "Train Epoch: 77 [1200/8040 (15%)]\tLoss: 19.186192\n",
      "Train Epoch: 77 [2400/8040 (30%)]\tLoss: 19.553695\n",
      "Train Epoch: 77 [3600/8040 (45%)]\tLoss: 18.968787\n",
      "Train Epoch: 77 [4800/8040 (60%)]\tLoss: 19.353210\n",
      "Train Epoch: 77 [6000/8040 (75%)]\tLoss: 20.099186\n",
      "Train Epoch: 77 [7200/8040 (90%)]\tLoss: 18.920485\n",
      "====> Epoch: 77 Average loss: 19.3157\n",
      "epoch: 77, beta=0, frac_anom=0.01\n",
      "Train Epoch: 78 [0/8040 (0%)]\tLoss: 18.900590\n",
      "Train Epoch: 78 [1200/8040 (15%)]\tLoss: 19.306588\n",
      "Train Epoch: 78 [2400/8040 (30%)]\tLoss: 18.391949\n",
      "Train Epoch: 78 [3600/8040 (45%)]\tLoss: 19.021057\n",
      "Train Epoch: 78 [4800/8040 (60%)]\tLoss: 19.713468\n",
      "Train Epoch: 78 [6000/8040 (75%)]\tLoss: 20.320290\n",
      "Train Epoch: 78 [7200/8040 (90%)]\tLoss: 18.536499\n",
      "====> Epoch: 78 Average loss: 19.3282\n",
      "epoch: 78, beta=0, frac_anom=0.01\n",
      "Train Epoch: 79 [0/8040 (0%)]\tLoss: 19.354224\n",
      "Train Epoch: 79 [1200/8040 (15%)]\tLoss: 18.892493\n",
      "Train Epoch: 79 [2400/8040 (30%)]\tLoss: 18.746114\n",
      "Train Epoch: 79 [3600/8040 (45%)]\tLoss: 19.310207\n",
      "Train Epoch: 79 [4800/8040 (60%)]\tLoss: 19.710347\n",
      "Train Epoch: 79 [6000/8040 (75%)]\tLoss: 19.256887\n",
      "Train Epoch: 79 [7200/8040 (90%)]\tLoss: 18.754187\n",
      "====> Epoch: 79 Average loss: 19.3613\n",
      "epoch: 79, beta=0, frac_anom=0.01\n",
      "Train Epoch: 80 [0/8040 (0%)]\tLoss: 18.557694\n",
      "Train Epoch: 80 [1200/8040 (15%)]\tLoss: 19.587191\n",
      "Train Epoch: 80 [2400/8040 (30%)]\tLoss: 19.058451\n",
      "Train Epoch: 80 [3600/8040 (45%)]\tLoss: 18.676131\n",
      "Train Epoch: 80 [4800/8040 (60%)]\tLoss: 18.406327\n",
      "Train Epoch: 80 [6000/8040 (75%)]\tLoss: 19.044623\n",
      "Train Epoch: 80 [7200/8040 (90%)]\tLoss: 18.485948\n",
      "====> Epoch: 80 Average loss: 19.2697\n",
      "epoch: 80, beta=0, frac_anom=0.01\n",
      "Train Epoch: 81 [0/8040 (0%)]\tLoss: 19.174025\n",
      "Train Epoch: 81 [1200/8040 (15%)]\tLoss: 17.687649\n",
      "Train Epoch: 81 [2400/8040 (30%)]\tLoss: 19.587699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 81 [3600/8040 (45%)]\tLoss: 18.778170\n",
      "Train Epoch: 81 [4800/8040 (60%)]\tLoss: 18.782463\n",
      "Train Epoch: 81 [6000/8040 (75%)]\tLoss: 19.882623\n",
      "Train Epoch: 81 [7200/8040 (90%)]\tLoss: 18.347770\n",
      "====> Epoch: 81 Average loss: 19.2871\n",
      "epoch: 81, beta=0, frac_anom=0.01\n",
      "Train Epoch: 82 [0/8040 (0%)]\tLoss: 19.831462\n",
      "Train Epoch: 82 [1200/8040 (15%)]\tLoss: 19.696334\n",
      "Train Epoch: 82 [2400/8040 (30%)]\tLoss: 20.263900\n",
      "Train Epoch: 82 [3600/8040 (45%)]\tLoss: 18.760396\n",
      "Train Epoch: 82 [4800/8040 (60%)]\tLoss: 18.648570\n",
      "Train Epoch: 82 [6000/8040 (75%)]\tLoss: 19.766467\n",
      "Train Epoch: 82 [7200/8040 (90%)]\tLoss: 19.593880\n",
      "====> Epoch: 82 Average loss: 19.3113\n",
      "epoch: 82, beta=0, frac_anom=0.01\n",
      "Train Epoch: 83 [0/8040 (0%)]\tLoss: 19.785647\n",
      "Train Epoch: 83 [1200/8040 (15%)]\tLoss: 18.921472\n",
      "Train Epoch: 83 [2400/8040 (30%)]\tLoss: 18.481635\n",
      "Train Epoch: 83 [3600/8040 (45%)]\tLoss: 20.361627\n",
      "Train Epoch: 83 [4800/8040 (60%)]\tLoss: 19.574862\n",
      "Train Epoch: 83 [6000/8040 (75%)]\tLoss: 18.840474\n",
      "Train Epoch: 83 [7200/8040 (90%)]\tLoss: 18.903190\n",
      "====> Epoch: 83 Average loss: 19.2489\n",
      "epoch: 83, beta=0, frac_anom=0.01\n",
      "Train Epoch: 84 [0/8040 (0%)]\tLoss: 20.288082\n",
      "Train Epoch: 84 [1200/8040 (15%)]\tLoss: 19.723409\n",
      "Train Epoch: 84 [2400/8040 (30%)]\tLoss: 20.121322\n",
      "Train Epoch: 84 [3600/8040 (45%)]\tLoss: 19.318009\n",
      "Train Epoch: 84 [4800/8040 (60%)]\tLoss: 19.750199\n",
      "Train Epoch: 84 [6000/8040 (75%)]\tLoss: 18.345782\n",
      "Train Epoch: 84 [7200/8040 (90%)]\tLoss: 19.351168\n",
      "====> Epoch: 84 Average loss: 19.2947\n",
      "epoch: 84, beta=0, frac_anom=0.01\n",
      "Train Epoch: 85 [0/8040 (0%)]\tLoss: 18.871751\n",
      "Train Epoch: 85 [1200/8040 (15%)]\tLoss: 18.950191\n",
      "Train Epoch: 85 [2400/8040 (30%)]\tLoss: 19.419045\n",
      "Train Epoch: 85 [3600/8040 (45%)]\tLoss: 18.210681\n",
      "Train Epoch: 85 [4800/8040 (60%)]\tLoss: 19.250155\n",
      "Train Epoch: 85 [6000/8040 (75%)]\tLoss: 19.305249\n",
      "Train Epoch: 85 [7200/8040 (90%)]\tLoss: 20.457715\n",
      "====> Epoch: 85 Average loss: 19.2763\n",
      "epoch: 85, beta=0, frac_anom=0.01\n",
      "Train Epoch: 86 [0/8040 (0%)]\tLoss: 19.556392\n",
      "Train Epoch: 86 [1200/8040 (15%)]\tLoss: 19.639235\n",
      "Train Epoch: 86 [2400/8040 (30%)]\tLoss: 18.195272\n",
      "Train Epoch: 86 [3600/8040 (45%)]\tLoss: 19.148657\n",
      "Train Epoch: 86 [4800/8040 (60%)]\tLoss: 19.318760\n",
      "Train Epoch: 86 [6000/8040 (75%)]\tLoss: 19.415641\n",
      "Train Epoch: 86 [7200/8040 (90%)]\tLoss: 19.381124\n",
      "====> Epoch: 86 Average loss: 19.2961\n",
      "epoch: 86, beta=0, frac_anom=0.01\n",
      "Train Epoch: 87 [0/8040 (0%)]\tLoss: 19.232420\n",
      "Train Epoch: 87 [1200/8040 (15%)]\tLoss: 18.502690\n",
      "Train Epoch: 87 [2400/8040 (30%)]\tLoss: 20.183244\n",
      "Train Epoch: 87 [3600/8040 (45%)]\tLoss: 19.144710\n",
      "Train Epoch: 87 [4800/8040 (60%)]\tLoss: 18.444759\n",
      "Train Epoch: 87 [6000/8040 (75%)]\tLoss: 19.234753\n",
      "Train Epoch: 87 [7200/8040 (90%)]\tLoss: 19.288232\n",
      "====> Epoch: 87 Average loss: 19.2778\n",
      "epoch: 87, beta=0, frac_anom=0.01\n",
      "Train Epoch: 88 [0/8040 (0%)]\tLoss: 18.906120\n",
      "Train Epoch: 88 [1200/8040 (15%)]\tLoss: 18.679799\n",
      "Train Epoch: 88 [2400/8040 (30%)]\tLoss: 19.603939\n",
      "Train Epoch: 88 [3600/8040 (45%)]\tLoss: 20.035531\n",
      "Train Epoch: 88 [4800/8040 (60%)]\tLoss: 17.910313\n",
      "Train Epoch: 88 [6000/8040 (75%)]\tLoss: 18.382715\n",
      "Train Epoch: 88 [7200/8040 (90%)]\tLoss: 19.434924\n",
      "====> Epoch: 88 Average loss: 19.2681\n",
      "epoch: 88, beta=0, frac_anom=0.01\n",
      "Train Epoch: 89 [0/8040 (0%)]\tLoss: 18.335437\n",
      "Train Epoch: 89 [1200/8040 (15%)]\tLoss: 18.386027\n",
      "Train Epoch: 89 [2400/8040 (30%)]\tLoss: 19.683040\n",
      "Train Epoch: 89 [3600/8040 (45%)]\tLoss: 19.011497\n",
      "Train Epoch: 89 [4800/8040 (60%)]\tLoss: 19.179618\n",
      "Train Epoch: 89 [6000/8040 (75%)]\tLoss: 19.865729\n",
      "Train Epoch: 89 [7200/8040 (90%)]\tLoss: 17.736108\n",
      "====> Epoch: 89 Average loss: 19.2554\n",
      "epoch: 89, beta=0, frac_anom=0.01\n",
      "Train Epoch: 90 [0/8040 (0%)]\tLoss: 18.401526\n",
      "Train Epoch: 90 [1200/8040 (15%)]\tLoss: 18.752596\n",
      "Train Epoch: 90 [2400/8040 (30%)]\tLoss: 20.003021\n",
      "Train Epoch: 90 [3600/8040 (45%)]\tLoss: 19.397642\n",
      "Train Epoch: 90 [4800/8040 (60%)]\tLoss: 18.939288\n",
      "Train Epoch: 90 [6000/8040 (75%)]\tLoss: 19.014838\n",
      "Train Epoch: 90 [7200/8040 (90%)]\tLoss: 18.261784\n",
      "====> Epoch: 90 Average loss: 19.2283\n",
      "epoch: 90, beta=0, frac_anom=0.01\n",
      "Train Epoch: 91 [0/8040 (0%)]\tLoss: 19.445243\n",
      "Train Epoch: 91 [1200/8040 (15%)]\tLoss: 19.117987\n",
      "Train Epoch: 91 [2400/8040 (30%)]\tLoss: 19.475734\n",
      "Train Epoch: 91 [3600/8040 (45%)]\tLoss: 18.628137\n",
      "Train Epoch: 91 [4800/8040 (60%)]\tLoss: 19.061820\n",
      "Train Epoch: 91 [6000/8040 (75%)]\tLoss: 20.927437\n",
      "Train Epoch: 91 [7200/8040 (90%)]\tLoss: 18.960935\n",
      "====> Epoch: 91 Average loss: 19.2154\n",
      "epoch: 91, beta=0, frac_anom=0.01\n",
      "Train Epoch: 92 [0/8040 (0%)]\tLoss: 18.653741\n",
      "Train Epoch: 92 [1200/8040 (15%)]\tLoss: 19.551477\n",
      "Train Epoch: 92 [2400/8040 (30%)]\tLoss: 19.813845\n",
      "Train Epoch: 92 [3600/8040 (45%)]\tLoss: 18.791111\n",
      "Train Epoch: 92 [4800/8040 (60%)]\tLoss: 20.077456\n",
      "Train Epoch: 92 [6000/8040 (75%)]\tLoss: 19.269167\n",
      "Train Epoch: 92 [7200/8040 (90%)]\tLoss: 18.942590\n",
      "====> Epoch: 92 Average loss: 19.2035\n",
      "epoch: 92, beta=0, frac_anom=0.01\n",
      "Train Epoch: 93 [0/8040 (0%)]\tLoss: 19.690271\n",
      "Train Epoch: 93 [1200/8040 (15%)]\tLoss: 19.785333\n",
      "Train Epoch: 93 [2400/8040 (30%)]\tLoss: 19.919324\n",
      "Train Epoch: 93 [3600/8040 (45%)]\tLoss: 18.687821\n",
      "Train Epoch: 93 [4800/8040 (60%)]\tLoss: 18.910006\n",
      "Train Epoch: 93 [6000/8040 (75%)]\tLoss: 17.902492\n",
      "Train Epoch: 93 [7200/8040 (90%)]\tLoss: 18.891980\n",
      "====> Epoch: 93 Average loss: 19.1675\n",
      "epoch: 93, beta=0, frac_anom=0.01\n",
      "Train Epoch: 94 [0/8040 (0%)]\tLoss: 18.668781\n",
      "Train Epoch: 94 [1200/8040 (15%)]\tLoss: 19.002954\n",
      "Train Epoch: 94 [2400/8040 (30%)]\tLoss: 19.745426\n",
      "Train Epoch: 94 [3600/8040 (45%)]\tLoss: 18.050631\n",
      "Train Epoch: 94 [4800/8040 (60%)]\tLoss: 18.147620\n",
      "Train Epoch: 94 [6000/8040 (75%)]\tLoss: 18.416663\n",
      "Train Epoch: 94 [7200/8040 (90%)]\tLoss: 19.355078\n",
      "====> Epoch: 94 Average loss: 19.1879\n",
      "epoch: 94, beta=0, frac_anom=0.01\n",
      "Train Epoch: 95 [0/8040 (0%)]\tLoss: 19.589113\n",
      "Train Epoch: 95 [1200/8040 (15%)]\tLoss: 20.284005\n",
      "Train Epoch: 95 [2400/8040 (30%)]\tLoss: 19.100885\n",
      "Train Epoch: 95 [3600/8040 (45%)]\tLoss: 19.487427\n",
      "Train Epoch: 95 [4800/8040 (60%)]\tLoss: 18.984540\n",
      "Train Epoch: 95 [6000/8040 (75%)]\tLoss: 19.272868\n",
      "Train Epoch: 95 [7200/8040 (90%)]\tLoss: 18.797534\n",
      "====> Epoch: 95 Average loss: 19.2257\n",
      "epoch: 95, beta=0, frac_anom=0.01\n",
      "Train Epoch: 96 [0/8040 (0%)]\tLoss: 19.282680\n",
      "Train Epoch: 96 [1200/8040 (15%)]\tLoss: 19.615232\n",
      "Train Epoch: 96 [2400/8040 (30%)]\tLoss: 19.267537\n",
      "Train Epoch: 96 [3600/8040 (45%)]\tLoss: 19.373895\n",
      "Train Epoch: 96 [4800/8040 (60%)]\tLoss: 18.131075\n",
      "Train Epoch: 96 [6000/8040 (75%)]\tLoss: 19.006576\n",
      "Train Epoch: 96 [7200/8040 (90%)]\tLoss: 19.178703\n",
      "====> Epoch: 96 Average loss: 19.2101\n",
      "epoch: 96, beta=0, frac_anom=0.01\n",
      "Train Epoch: 97 [0/8040 (0%)]\tLoss: 19.030774\n",
      "Train Epoch: 97 [1200/8040 (15%)]\tLoss: 18.936141\n",
      "Train Epoch: 97 [2400/8040 (30%)]\tLoss: 18.583590\n",
      "Train Epoch: 97 [3600/8040 (45%)]\tLoss: 18.386833\n",
      "Train Epoch: 97 [4800/8040 (60%)]\tLoss: 18.411792\n",
      "Train Epoch: 97 [6000/8040 (75%)]\tLoss: 18.478813\n",
      "Train Epoch: 97 [7200/8040 (90%)]\tLoss: 18.666449\n",
      "====> Epoch: 97 Average loss: 19.1590\n",
      "epoch: 97, beta=0, frac_anom=0.01\n",
      "Train Epoch: 98 [0/8040 (0%)]\tLoss: 19.307772\n",
      "Train Epoch: 98 [1200/8040 (15%)]\tLoss: 19.175157\n",
      "Train Epoch: 98 [2400/8040 (30%)]\tLoss: 19.537616\n",
      "Train Epoch: 98 [3600/8040 (45%)]\tLoss: 18.676298\n",
      "Train Epoch: 98 [4800/8040 (60%)]\tLoss: 19.549422\n",
      "Train Epoch: 98 [6000/8040 (75%)]\tLoss: 19.691416\n",
      "Train Epoch: 98 [7200/8040 (90%)]\tLoss: 18.505892\n",
      "====> Epoch: 98 Average loss: 19.1441\n",
      "epoch: 98, beta=0, frac_anom=0.01\n",
      "Train Epoch: 99 [0/8040 (0%)]\tLoss: 19.407141\n",
      "Train Epoch: 99 [1200/8040 (15%)]\tLoss: 18.778849\n",
      "Train Epoch: 99 [2400/8040 (30%)]\tLoss: 18.653076\n",
      "Train Epoch: 99 [3600/8040 (45%)]\tLoss: 18.418722\n",
      "Train Epoch: 99 [4800/8040 (60%)]\tLoss: 19.139968\n",
      "Train Epoch: 99 [6000/8040 (75%)]\tLoss: 18.759007\n",
      "Train Epoch: 99 [7200/8040 (90%)]\tLoss: 18.997803\n",
      "====> Epoch: 99 Average loss: 19.1416\n",
      "epoch: 99, beta=0, frac_anom=0.01\n",
      "Train Epoch: 100 [0/8040 (0%)]\tLoss: 19.379769\n",
      "Train Epoch: 100 [1200/8040 (15%)]\tLoss: 18.857115\n",
      "Train Epoch: 100 [2400/8040 (30%)]\tLoss: 17.939138\n",
      "Train Epoch: 100 [3600/8040 (45%)]\tLoss: 19.022876\n",
      "Train Epoch: 100 [4800/8040 (60%)]\tLoss: 19.080135\n",
      "Train Epoch: 100 [6000/8040 (75%)]\tLoss: 18.546438\n",
      "Train Epoch: 100 [7200/8040 (90%)]\tLoss: 19.870103\n",
      "====> Epoch: 100 Average loss: 19.1458\n",
      "epoch: 100, beta=0, frac_anom=0.01\n",
      "Train Epoch: 101 [0/8040 (0%)]\tLoss: 20.373397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 101 [1200/8040 (15%)]\tLoss: 19.805526\n",
      "Train Epoch: 101 [2400/8040 (30%)]\tLoss: 19.589994\n",
      "Train Epoch: 101 [3600/8040 (45%)]\tLoss: 19.880282\n",
      "Train Epoch: 101 [4800/8040 (60%)]\tLoss: 18.967605\n",
      "Train Epoch: 101 [6000/8040 (75%)]\tLoss: 17.923364\n",
      "Train Epoch: 101 [7200/8040 (90%)]\tLoss: 19.178971\n",
      "====> Epoch: 101 Average loss: 19.1825\n",
      "epoch: 101, beta=0, frac_anom=0.01\n",
      "Train Epoch: 102 [0/8040 (0%)]\tLoss: 18.304441\n",
      "Train Epoch: 102 [1200/8040 (15%)]\tLoss: 19.722384\n",
      "Train Epoch: 102 [2400/8040 (30%)]\tLoss: 19.981104\n",
      "Train Epoch: 102 [3600/8040 (45%)]\tLoss: 19.623775\n",
      "Train Epoch: 102 [4800/8040 (60%)]\tLoss: 18.493254\n",
      "Train Epoch: 102 [6000/8040 (75%)]\tLoss: 18.449768\n",
      "Train Epoch: 102 [7200/8040 (90%)]\tLoss: 19.067588\n",
      "====> Epoch: 102 Average loss: 19.1359\n",
      "epoch: 102, beta=0, frac_anom=0.01\n",
      "Train Epoch: 103 [0/8040 (0%)]\tLoss: 20.234530\n",
      "Train Epoch: 103 [1200/8040 (15%)]\tLoss: 19.145756\n",
      "Train Epoch: 103 [2400/8040 (30%)]\tLoss: 19.622166\n",
      "Train Epoch: 103 [3600/8040 (45%)]\tLoss: 19.643182\n",
      "Train Epoch: 103 [4800/8040 (60%)]\tLoss: 18.878825\n",
      "Train Epoch: 103 [6000/8040 (75%)]\tLoss: 18.104818\n",
      "Train Epoch: 103 [7200/8040 (90%)]\tLoss: 19.107623\n",
      "====> Epoch: 103 Average loss: 19.1060\n",
      "epoch: 103, beta=0, frac_anom=0.01\n",
      "Train Epoch: 104 [0/8040 (0%)]\tLoss: 19.636576\n",
      "Train Epoch: 104 [1200/8040 (15%)]\tLoss: 18.359819\n",
      "Train Epoch: 104 [2400/8040 (30%)]\tLoss: 18.882487\n",
      "Train Epoch: 104 [3600/8040 (45%)]\tLoss: 19.389331\n",
      "Train Epoch: 104 [4800/8040 (60%)]\tLoss: 18.873657\n",
      "Train Epoch: 104 [6000/8040 (75%)]\tLoss: 19.796395\n",
      "Train Epoch: 104 [7200/8040 (90%)]\tLoss: 19.187569\n",
      "====> Epoch: 104 Average loss: 19.1270\n",
      "epoch: 104, beta=0, frac_anom=0.01\n",
      "Train Epoch: 105 [0/8040 (0%)]\tLoss: 18.234080\n",
      "Train Epoch: 105 [1200/8040 (15%)]\tLoss: 18.911694\n",
      "Train Epoch: 105 [2400/8040 (30%)]\tLoss: 19.487219\n",
      "Train Epoch: 105 [3600/8040 (45%)]\tLoss: 19.064478\n",
      "Train Epoch: 105 [4800/8040 (60%)]\tLoss: 18.184786\n",
      "Train Epoch: 105 [6000/8040 (75%)]\tLoss: 19.129761\n",
      "Train Epoch: 105 [7200/8040 (90%)]\tLoss: 20.032340\n",
      "====> Epoch: 105 Average loss: 19.0745\n",
      "epoch: 105, beta=0, frac_anom=0.01\n",
      "Train Epoch: 106 [0/8040 (0%)]\tLoss: 20.195654\n",
      "Train Epoch: 106 [1200/8040 (15%)]\tLoss: 19.211910\n",
      "Train Epoch: 106 [2400/8040 (30%)]\tLoss: 19.673580\n",
      "Train Epoch: 106 [3600/8040 (45%)]\tLoss: 19.451194\n",
      "Train Epoch: 106 [4800/8040 (60%)]\tLoss: 19.658315\n",
      "Train Epoch: 106 [6000/8040 (75%)]\tLoss: 19.291305\n",
      "Train Epoch: 106 [7200/8040 (90%)]\tLoss: 19.243880\n",
      "====> Epoch: 106 Average loss: 19.1105\n",
      "epoch: 106, beta=0, frac_anom=0.01\n",
      "Train Epoch: 107 [0/8040 (0%)]\tLoss: 19.410514\n",
      "Train Epoch: 107 [1200/8040 (15%)]\tLoss: 18.814813\n",
      "Train Epoch: 107 [2400/8040 (30%)]\tLoss: 19.006396\n",
      "Train Epoch: 107 [3600/8040 (45%)]\tLoss: 19.620866\n",
      "Train Epoch: 107 [4800/8040 (60%)]\tLoss: 18.169666\n",
      "Train Epoch: 107 [6000/8040 (75%)]\tLoss: 19.892741\n",
      "Train Epoch: 107 [7200/8040 (90%)]\tLoss: 19.614024\n",
      "====> Epoch: 107 Average loss: 19.0643\n",
      "epoch: 107, beta=0, frac_anom=0.01\n",
      "Train Epoch: 108 [0/8040 (0%)]\tLoss: 19.290295\n",
      "Train Epoch: 108 [1200/8040 (15%)]\tLoss: 19.001811\n",
      "Train Epoch: 108 [2400/8040 (30%)]\tLoss: 18.689699\n",
      "Train Epoch: 108 [3600/8040 (45%)]\tLoss: 18.932552\n",
      "Train Epoch: 108 [4800/8040 (60%)]\tLoss: 19.725916\n",
      "Train Epoch: 108 [6000/8040 (75%)]\tLoss: 19.324923\n",
      "Train Epoch: 108 [7200/8040 (90%)]\tLoss: 19.919360\n",
      "====> Epoch: 108 Average loss: 19.0749\n",
      "epoch: 108, beta=0, frac_anom=0.01\n",
      "Train Epoch: 109 [0/8040 (0%)]\tLoss: 20.040568\n",
      "Train Epoch: 109 [1200/8040 (15%)]\tLoss: 19.688735\n",
      "Train Epoch: 109 [2400/8040 (30%)]\tLoss: 18.821212\n",
      "Train Epoch: 109 [3600/8040 (45%)]\tLoss: 18.538411\n",
      "Train Epoch: 109 [4800/8040 (60%)]\tLoss: 19.267350\n",
      "Train Epoch: 109 [6000/8040 (75%)]\tLoss: 19.228381\n",
      "Train Epoch: 109 [7200/8040 (90%)]\tLoss: 18.925496\n",
      "====> Epoch: 109 Average loss: 19.0902\n",
      "epoch: 109, beta=0, frac_anom=0.01\n",
      "Train Epoch: 110 [0/8040 (0%)]\tLoss: 18.170022\n",
      "Train Epoch: 110 [1200/8040 (15%)]\tLoss: 19.446330\n",
      "Train Epoch: 110 [2400/8040 (30%)]\tLoss: 19.232998\n",
      "Train Epoch: 110 [3600/8040 (45%)]\tLoss: 18.873324\n",
      "Train Epoch: 110 [4800/8040 (60%)]\tLoss: 18.703088\n",
      "Train Epoch: 110 [6000/8040 (75%)]\tLoss: 19.818937\n",
      "Train Epoch: 110 [7200/8040 (90%)]\tLoss: 19.347097\n",
      "====> Epoch: 110 Average loss: 19.0708\n",
      "epoch: 110, beta=0, frac_anom=0.01\n",
      "Train Epoch: 111 [0/8040 (0%)]\tLoss: 20.098739\n",
      "Train Epoch: 111 [1200/8040 (15%)]\tLoss: 18.893207\n",
      "Train Epoch: 111 [2400/8040 (30%)]\tLoss: 19.614852\n",
      "Train Epoch: 111 [3600/8040 (45%)]\tLoss: 18.770410\n",
      "Train Epoch: 111 [4800/8040 (60%)]\tLoss: 18.934165\n",
      "Train Epoch: 111 [6000/8040 (75%)]\tLoss: 18.847355\n",
      "Train Epoch: 111 [7200/8040 (90%)]\tLoss: 17.795900\n",
      "====> Epoch: 111 Average loss: 19.0936\n",
      "epoch: 111, beta=0, frac_anom=0.01\n",
      "Train Epoch: 112 [0/8040 (0%)]\tLoss: 19.343184\n",
      "Train Epoch: 112 [1200/8040 (15%)]\tLoss: 18.958834\n",
      "Train Epoch: 112 [2400/8040 (30%)]\tLoss: 19.393309\n",
      "Train Epoch: 112 [3600/8040 (45%)]\tLoss: 18.610382\n",
      "Train Epoch: 112 [4800/8040 (60%)]\tLoss: 19.475277\n",
      "Train Epoch: 112 [6000/8040 (75%)]\tLoss: 19.019368\n",
      "Train Epoch: 112 [7200/8040 (90%)]\tLoss: 18.020740\n",
      "====> Epoch: 112 Average loss: 19.0328\n",
      "epoch: 112, beta=0, frac_anom=0.01\n",
      "Train Epoch: 113 [0/8040 (0%)]\tLoss: 19.327285\n",
      "Train Epoch: 113 [1200/8040 (15%)]\tLoss: 18.912439\n",
      "Train Epoch: 113 [2400/8040 (30%)]\tLoss: 19.971783\n",
      "Train Epoch: 113 [3600/8040 (45%)]\tLoss: 19.016105\n",
      "Train Epoch: 113 [4800/8040 (60%)]\tLoss: 18.659741\n",
      "Train Epoch: 113 [6000/8040 (75%)]\tLoss: 19.501554\n",
      "Train Epoch: 113 [7200/8040 (90%)]\tLoss: 19.022764\n",
      "====> Epoch: 113 Average loss: 19.0474\n",
      "epoch: 113, beta=0, frac_anom=0.01\n",
      "Train Epoch: 114 [0/8040 (0%)]\tLoss: 18.150317\n",
      "Train Epoch: 114 [1200/8040 (15%)]\tLoss: 19.357501\n",
      "Train Epoch: 114 [2400/8040 (30%)]\tLoss: 19.692812\n",
      "Train Epoch: 114 [3600/8040 (45%)]\tLoss: 20.056061\n",
      "Train Epoch: 114 [4800/8040 (60%)]\tLoss: 19.161692\n",
      "Train Epoch: 114 [6000/8040 (75%)]\tLoss: 19.133521\n",
      "Train Epoch: 114 [7200/8040 (90%)]\tLoss: 18.636320\n",
      "====> Epoch: 114 Average loss: 19.0632\n",
      "epoch: 114, beta=0, frac_anom=0.01\n",
      "Train Epoch: 115 [0/8040 (0%)]\tLoss: 19.225354\n",
      "Train Epoch: 115 [1200/8040 (15%)]\tLoss: 18.354506\n",
      "Train Epoch: 115 [2400/8040 (30%)]\tLoss: 18.641270\n",
      "Train Epoch: 115 [3600/8040 (45%)]\tLoss: 18.668577\n",
      "Train Epoch: 115 [4800/8040 (60%)]\tLoss: 18.613163\n",
      "Train Epoch: 115 [6000/8040 (75%)]\tLoss: 19.193502\n",
      "Train Epoch: 115 [7200/8040 (90%)]\tLoss: 17.694613\n",
      "====> Epoch: 115 Average loss: 19.0749\n",
      "epoch: 115, beta=0, frac_anom=0.01\n",
      "Train Epoch: 116 [0/8040 (0%)]\tLoss: 19.278011\n",
      "Train Epoch: 116 [1200/8040 (15%)]\tLoss: 19.048389\n",
      "Train Epoch: 116 [2400/8040 (30%)]\tLoss: 19.254004\n",
      "Train Epoch: 116 [3600/8040 (45%)]\tLoss: 18.912514\n",
      "Train Epoch: 116 [4800/8040 (60%)]\tLoss: 18.321983\n",
      "Train Epoch: 116 [6000/8040 (75%)]\tLoss: 19.590112\n",
      "Train Epoch: 116 [7200/8040 (90%)]\tLoss: 18.831706\n",
      "====> Epoch: 116 Average loss: 19.0952\n",
      "epoch: 116, beta=0, frac_anom=0.01\n",
      "Train Epoch: 117 [0/8040 (0%)]\tLoss: 18.529104\n",
      "Train Epoch: 117 [1200/8040 (15%)]\tLoss: 18.537791\n",
      "Train Epoch: 117 [2400/8040 (30%)]\tLoss: 18.383921\n",
      "Train Epoch: 117 [3600/8040 (45%)]\tLoss: 19.381889\n",
      "Train Epoch: 117 [4800/8040 (60%)]\tLoss: 19.927268\n",
      "Train Epoch: 117 [6000/8040 (75%)]\tLoss: 17.796960\n",
      "Train Epoch: 117 [7200/8040 (90%)]\tLoss: 17.937594\n",
      "====> Epoch: 117 Average loss: 19.0561\n",
      "epoch: 117, beta=0, frac_anom=0.01\n",
      "Train Epoch: 118 [0/8040 (0%)]\tLoss: 18.619800\n",
      "Train Epoch: 118 [1200/8040 (15%)]\tLoss: 19.128918\n",
      "Train Epoch: 118 [2400/8040 (30%)]\tLoss: 18.476908\n",
      "Train Epoch: 118 [3600/8040 (45%)]\tLoss: 19.474788\n",
      "Train Epoch: 118 [4800/8040 (60%)]\tLoss: 19.576668\n",
      "Train Epoch: 118 [6000/8040 (75%)]\tLoss: 19.575918\n",
      "Train Epoch: 118 [7200/8040 (90%)]\tLoss: 18.833140\n",
      "====> Epoch: 118 Average loss: 19.0044\n",
      "epoch: 118, beta=0, frac_anom=0.01\n",
      "Train Epoch: 119 [0/8040 (0%)]\tLoss: 19.540794\n",
      "Train Epoch: 119 [1200/8040 (15%)]\tLoss: 19.557886\n",
      "Train Epoch: 119 [2400/8040 (30%)]\tLoss: 18.757131\n",
      "Train Epoch: 119 [3600/8040 (45%)]\tLoss: 18.612809\n",
      "Train Epoch: 119 [4800/8040 (60%)]\tLoss: 19.847723\n",
      "Train Epoch: 119 [6000/8040 (75%)]\tLoss: 19.303821\n",
      "Train Epoch: 119 [7200/8040 (90%)]\tLoss: 18.410799\n",
      "====> Epoch: 119 Average loss: 19.0256\n",
      "epoch: 119, beta=0, frac_anom=0.01\n",
      "Train Epoch: 120 [0/8040 (0%)]\tLoss: 18.601276\n",
      "Train Epoch: 120 [1200/8040 (15%)]\tLoss: 19.717293\n",
      "Train Epoch: 120 [2400/8040 (30%)]\tLoss: 19.112288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 120 [3600/8040 (45%)]\tLoss: 19.386194\n",
      "Train Epoch: 120 [4800/8040 (60%)]\tLoss: 18.514150\n",
      "Train Epoch: 120 [6000/8040 (75%)]\tLoss: 19.559143\n",
      "Train Epoch: 120 [7200/8040 (90%)]\tLoss: 18.815853\n",
      "====> Epoch: 120 Average loss: 19.0900\n",
      "epoch: 120, beta=0, frac_anom=0.01\n",
      "Train Epoch: 121 [0/8040 (0%)]\tLoss: 17.481864\n",
      "Train Epoch: 121 [1200/8040 (15%)]\tLoss: 18.340963\n",
      "Train Epoch: 121 [2400/8040 (30%)]\tLoss: 18.578682\n",
      "Train Epoch: 121 [3600/8040 (45%)]\tLoss: 18.916113\n",
      "Train Epoch: 121 [4800/8040 (60%)]\tLoss: 19.863009\n",
      "Train Epoch: 121 [6000/8040 (75%)]\tLoss: 17.515706\n",
      "Train Epoch: 121 [7200/8040 (90%)]\tLoss: 18.966284\n",
      "====> Epoch: 121 Average loss: 19.0257\n",
      "epoch: 121, beta=0, frac_anom=0.01\n",
      "Train Epoch: 122 [0/8040 (0%)]\tLoss: 18.790531\n",
      "Train Epoch: 122 [1200/8040 (15%)]\tLoss: 18.256205\n",
      "Train Epoch: 122 [2400/8040 (30%)]\tLoss: 18.671248\n",
      "Train Epoch: 122 [3600/8040 (45%)]\tLoss: 18.868213\n",
      "Train Epoch: 122 [4800/8040 (60%)]\tLoss: 19.113131\n",
      "Train Epoch: 122 [6000/8040 (75%)]\tLoss: 19.976583\n",
      "Train Epoch: 122 [7200/8040 (90%)]\tLoss: 19.640108\n",
      "====> Epoch: 122 Average loss: 18.9723\n",
      "epoch: 122, beta=0, frac_anom=0.01\n",
      "Train Epoch: 123 [0/8040 (0%)]\tLoss: 18.572038\n",
      "Train Epoch: 123 [1200/8040 (15%)]\tLoss: 17.577356\n",
      "Train Epoch: 123 [2400/8040 (30%)]\tLoss: 18.044967\n",
      "Train Epoch: 123 [3600/8040 (45%)]\tLoss: 19.261330\n",
      "Train Epoch: 123 [4800/8040 (60%)]\tLoss: 17.993896\n",
      "Train Epoch: 123 [6000/8040 (75%)]\tLoss: 18.143441\n",
      "Train Epoch: 123 [7200/8040 (90%)]\tLoss: 19.323210\n",
      "====> Epoch: 123 Average loss: 19.0269\n",
      "epoch: 123, beta=0, frac_anom=0.01\n",
      "Train Epoch: 124 [0/8040 (0%)]\tLoss: 18.339069\n",
      "Train Epoch: 124 [1200/8040 (15%)]\tLoss: 20.261804\n",
      "Train Epoch: 124 [2400/8040 (30%)]\tLoss: 18.560667\n",
      "Train Epoch: 124 [3600/8040 (45%)]\tLoss: 17.712380\n",
      "Train Epoch: 124 [4800/8040 (60%)]\tLoss: 18.463603\n",
      "Train Epoch: 124 [6000/8040 (75%)]\tLoss: 19.404362\n",
      "Train Epoch: 124 [7200/8040 (90%)]\tLoss: 17.720089\n",
      "====> Epoch: 124 Average loss: 18.9988\n",
      "epoch: 124, beta=0, frac_anom=0.01\n",
      "Train Epoch: 125 [0/8040 (0%)]\tLoss: 19.905109\n",
      "Train Epoch: 125 [1200/8040 (15%)]\tLoss: 19.142904\n",
      "Train Epoch: 125 [2400/8040 (30%)]\tLoss: 18.773067\n",
      "Train Epoch: 125 [3600/8040 (45%)]\tLoss: 18.952443\n",
      "Train Epoch: 125 [4800/8040 (60%)]\tLoss: 19.322807\n",
      "Train Epoch: 125 [6000/8040 (75%)]\tLoss: 18.852116\n",
      "Train Epoch: 125 [7200/8040 (90%)]\tLoss: 19.113601\n",
      "====> Epoch: 125 Average loss: 19.0267\n",
      "epoch: 125, beta=0, frac_anom=0.01\n",
      "Train Epoch: 126 [0/8040 (0%)]\tLoss: 19.631700\n",
      "Train Epoch: 126 [1200/8040 (15%)]\tLoss: 18.606224\n",
      "Train Epoch: 126 [2400/8040 (30%)]\tLoss: 19.045581\n",
      "Train Epoch: 126 [3600/8040 (45%)]\tLoss: 19.403375\n",
      "Train Epoch: 126 [4800/8040 (60%)]\tLoss: 18.240328\n",
      "Train Epoch: 126 [6000/8040 (75%)]\tLoss: 19.004301\n",
      "Train Epoch: 126 [7200/8040 (90%)]\tLoss: 17.502266\n",
      "====> Epoch: 126 Average loss: 18.9949\n",
      "epoch: 126, beta=0, frac_anom=0.01\n",
      "Train Epoch: 127 [0/8040 (0%)]\tLoss: 18.751611\n",
      "Train Epoch: 127 [1200/8040 (15%)]\tLoss: 19.592932\n",
      "Train Epoch: 127 [2400/8040 (30%)]\tLoss: 19.278625\n",
      "Train Epoch: 127 [3600/8040 (45%)]\tLoss: 18.436788\n",
      "Train Epoch: 127 [4800/8040 (60%)]\tLoss: 19.173513\n",
      "Train Epoch: 127 [6000/8040 (75%)]\tLoss: 19.722742\n",
      "Train Epoch: 127 [7200/8040 (90%)]\tLoss: 19.507635\n",
      "====> Epoch: 127 Average loss: 19.0044\n",
      "epoch: 127, beta=0, frac_anom=0.01\n",
      "Train Epoch: 128 [0/8040 (0%)]\tLoss: 20.370603\n",
      "Train Epoch: 128 [1200/8040 (15%)]\tLoss: 19.504653\n",
      "Train Epoch: 128 [2400/8040 (30%)]\tLoss: 17.889364\n",
      "Train Epoch: 128 [3600/8040 (45%)]\tLoss: 17.821265\n",
      "Train Epoch: 128 [4800/8040 (60%)]\tLoss: 18.888298\n",
      "Train Epoch: 128 [6000/8040 (75%)]\tLoss: 19.430143\n",
      "Train Epoch: 128 [7200/8040 (90%)]\tLoss: 19.498883\n",
      "====> Epoch: 128 Average loss: 18.9665\n",
      "epoch: 128, beta=0, frac_anom=0.01\n",
      "Train Epoch: 129 [0/8040 (0%)]\tLoss: 18.284351\n",
      "Train Epoch: 129 [1200/8040 (15%)]\tLoss: 18.853585\n",
      "Train Epoch: 129 [2400/8040 (30%)]\tLoss: 19.003833\n",
      "Train Epoch: 129 [3600/8040 (45%)]\tLoss: 19.112233\n",
      "Train Epoch: 129 [4800/8040 (60%)]\tLoss: 19.438059\n",
      "Train Epoch: 129 [6000/8040 (75%)]\tLoss: 19.072355\n",
      "Train Epoch: 129 [7200/8040 (90%)]\tLoss: 19.326750\n",
      "====> Epoch: 129 Average loss: 18.9985\n",
      "epoch: 129, beta=0, frac_anom=0.01\n",
      "Train Epoch: 130 [0/8040 (0%)]\tLoss: 18.845894\n",
      "Train Epoch: 130 [1200/8040 (15%)]\tLoss: 19.158836\n",
      "Train Epoch: 130 [2400/8040 (30%)]\tLoss: 19.474015\n",
      "Train Epoch: 130 [3600/8040 (45%)]\tLoss: 19.898258\n",
      "Train Epoch: 130 [4800/8040 (60%)]\tLoss: 18.523547\n",
      "Train Epoch: 130 [6000/8040 (75%)]\tLoss: 18.921025\n",
      "Train Epoch: 130 [7200/8040 (90%)]\tLoss: 18.718347\n",
      "====> Epoch: 130 Average loss: 18.9810\n",
      "epoch: 130, beta=0, frac_anom=0.01\n",
      "Train Epoch: 131 [0/8040 (0%)]\tLoss: 18.066597\n",
      "Train Epoch: 131 [1200/8040 (15%)]\tLoss: 18.949719\n",
      "Train Epoch: 131 [2400/8040 (30%)]\tLoss: 18.704020\n",
      "Train Epoch: 131 [3600/8040 (45%)]\tLoss: 19.612708\n",
      "Train Epoch: 131 [4800/8040 (60%)]\tLoss: 19.331413\n",
      "Train Epoch: 131 [6000/8040 (75%)]\tLoss: 18.797921\n",
      "Train Epoch: 131 [7200/8040 (90%)]\tLoss: 17.633512\n",
      "====> Epoch: 131 Average loss: 18.9727\n",
      "epoch: 131, beta=0, frac_anom=0.01\n",
      "Train Epoch: 132 [0/8040 (0%)]\tLoss: 19.253221\n",
      "Train Epoch: 132 [1200/8040 (15%)]\tLoss: 19.393127\n",
      "Train Epoch: 132 [2400/8040 (30%)]\tLoss: 17.990706\n",
      "Train Epoch: 132 [3600/8040 (45%)]\tLoss: 19.539500\n",
      "Train Epoch: 132 [4800/8040 (60%)]\tLoss: 19.382011\n",
      "Train Epoch: 132 [6000/8040 (75%)]\tLoss: 18.935938\n",
      "Train Epoch: 132 [7200/8040 (90%)]\tLoss: 19.516736\n",
      "====> Epoch: 132 Average loss: 18.9567\n",
      "epoch: 132, beta=0, frac_anom=0.01\n",
      "Train Epoch: 133 [0/8040 (0%)]\tLoss: 18.781818\n",
      "Train Epoch: 133 [1200/8040 (15%)]\tLoss: 18.654610\n",
      "Train Epoch: 133 [2400/8040 (30%)]\tLoss: 19.401422\n",
      "Train Epoch: 133 [3600/8040 (45%)]\tLoss: 19.043982\n",
      "Train Epoch: 133 [4800/8040 (60%)]\tLoss: 19.276172\n",
      "Train Epoch: 133 [6000/8040 (75%)]\tLoss: 19.849453\n",
      "Train Epoch: 133 [7200/8040 (90%)]\tLoss: 18.939492\n",
      "====> Epoch: 133 Average loss: 18.9880\n",
      "epoch: 133, beta=0, frac_anom=0.01\n",
      "Train Epoch: 134 [0/8040 (0%)]\tLoss: 19.848623\n",
      "Train Epoch: 134 [1200/8040 (15%)]\tLoss: 19.199215\n",
      "Train Epoch: 134 [2400/8040 (30%)]\tLoss: 18.501786\n",
      "Train Epoch: 134 [3600/8040 (45%)]\tLoss: 19.211841\n",
      "Train Epoch: 134 [4800/8040 (60%)]\tLoss: 18.536884\n",
      "Train Epoch: 134 [6000/8040 (75%)]\tLoss: 17.960248\n",
      "Train Epoch: 134 [7200/8040 (90%)]\tLoss: 20.003644\n",
      "====> Epoch: 134 Average loss: 18.9760\n",
      "epoch: 134, beta=0, frac_anom=0.01\n",
      "Train Epoch: 135 [0/8040 (0%)]\tLoss: 18.719287\n",
      "Train Epoch: 135 [1200/8040 (15%)]\tLoss: 19.064067\n",
      "Train Epoch: 135 [2400/8040 (30%)]\tLoss: 19.765845\n",
      "Train Epoch: 135 [3600/8040 (45%)]\tLoss: 19.104659\n",
      "Train Epoch: 135 [4800/8040 (60%)]\tLoss: 18.598956\n",
      "Train Epoch: 135 [6000/8040 (75%)]\tLoss: 18.436497\n",
      "Train Epoch: 135 [7200/8040 (90%)]\tLoss: 19.493463\n",
      "====> Epoch: 135 Average loss: 18.9692\n",
      "epoch: 135, beta=0, frac_anom=0.01\n",
      "Train Epoch: 136 [0/8040 (0%)]\tLoss: 19.002124\n",
      "Train Epoch: 136 [1200/8040 (15%)]\tLoss: 19.123476\n",
      "Train Epoch: 136 [2400/8040 (30%)]\tLoss: 19.507241\n",
      "Train Epoch: 136 [3600/8040 (45%)]\tLoss: 18.768127\n",
      "Train Epoch: 136 [4800/8040 (60%)]\tLoss: 19.213055\n",
      "Train Epoch: 136 [6000/8040 (75%)]\tLoss: 19.029773\n",
      "Train Epoch: 136 [7200/8040 (90%)]\tLoss: 19.085555\n",
      "====> Epoch: 136 Average loss: 18.9865\n",
      "epoch: 136, beta=0, frac_anom=0.01\n",
      "Train Epoch: 137 [0/8040 (0%)]\tLoss: 19.941333\n",
      "Train Epoch: 137 [1200/8040 (15%)]\tLoss: 18.608305\n",
      "Train Epoch: 137 [2400/8040 (30%)]\tLoss: 19.262970\n",
      "Train Epoch: 137 [3600/8040 (45%)]\tLoss: 19.340108\n",
      "Train Epoch: 137 [4800/8040 (60%)]\tLoss: 19.776221\n",
      "Train Epoch: 137 [6000/8040 (75%)]\tLoss: 19.845353\n",
      "Train Epoch: 137 [7200/8040 (90%)]\tLoss: 18.442625\n",
      "====> Epoch: 137 Average loss: 18.9723\n",
      "epoch: 137, beta=0, frac_anom=0.01\n",
      "Train Epoch: 138 [0/8040 (0%)]\tLoss: 19.442680\n",
      "Train Epoch: 138 [1200/8040 (15%)]\tLoss: 19.585535\n",
      "Train Epoch: 138 [2400/8040 (30%)]\tLoss: 19.528410\n",
      "Train Epoch: 138 [3600/8040 (45%)]\tLoss: 18.073372\n",
      "Train Epoch: 138 [4800/8040 (60%)]\tLoss: 19.375435\n",
      "Train Epoch: 138 [6000/8040 (75%)]\tLoss: 19.247111\n",
      "Train Epoch: 138 [7200/8040 (90%)]\tLoss: 19.216650\n",
      "====> Epoch: 138 Average loss: 18.9199\n",
      "epoch: 138, beta=0, frac_anom=0.01\n",
      "Train Epoch: 139 [0/8040 (0%)]\tLoss: 18.841526\n",
      "Train Epoch: 139 [1200/8040 (15%)]\tLoss: 19.142078\n",
      "Train Epoch: 139 [2400/8040 (30%)]\tLoss: 18.360189\n",
      "Train Epoch: 139 [3600/8040 (45%)]\tLoss: 19.235638\n",
      "Train Epoch: 139 [4800/8040 (60%)]\tLoss: 19.161410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 139 [6000/8040 (75%)]\tLoss: 19.350633\n",
      "Train Epoch: 139 [7200/8040 (90%)]\tLoss: 19.074565\n",
      "====> Epoch: 139 Average loss: 18.9918\n",
      "epoch: 139, beta=0, frac_anom=0.01\n",
      "Train Epoch: 140 [0/8040 (0%)]\tLoss: 20.028357\n",
      "Train Epoch: 140 [1200/8040 (15%)]\tLoss: 20.684741\n",
      "Train Epoch: 140 [2400/8040 (30%)]\tLoss: 19.405758\n",
      "Train Epoch: 140 [3600/8040 (45%)]\tLoss: 18.203656\n",
      "Train Epoch: 140 [4800/8040 (60%)]\tLoss: 19.894149\n",
      "Train Epoch: 140 [6000/8040 (75%)]\tLoss: 18.825920\n",
      "Train Epoch: 140 [7200/8040 (90%)]\tLoss: 19.698022\n",
      "====> Epoch: 140 Average loss: 18.9407\n",
      "epoch: 140, beta=0, frac_anom=0.01\n",
      "Train Epoch: 141 [0/8040 (0%)]\tLoss: 18.166597\n",
      "Train Epoch: 141 [1200/8040 (15%)]\tLoss: 19.057947\n",
      "Train Epoch: 141 [2400/8040 (30%)]\tLoss: 18.822815\n",
      "Train Epoch: 141 [3600/8040 (45%)]\tLoss: 19.640694\n",
      "Train Epoch: 141 [4800/8040 (60%)]\tLoss: 18.448930\n",
      "Train Epoch: 141 [6000/8040 (75%)]\tLoss: 18.769702\n",
      "Train Epoch: 141 [7200/8040 (90%)]\tLoss: 18.152256\n",
      "====> Epoch: 141 Average loss: 18.9174\n",
      "epoch: 141, beta=0, frac_anom=0.01\n",
      "Train Epoch: 142 [0/8040 (0%)]\tLoss: 19.092247\n",
      "Train Epoch: 142 [1200/8040 (15%)]\tLoss: 19.438631\n",
      "Train Epoch: 142 [2400/8040 (30%)]\tLoss: 19.053481\n",
      "Train Epoch: 142 [3600/8040 (45%)]\tLoss: 18.748108\n",
      "Train Epoch: 142 [4800/8040 (60%)]\tLoss: 18.783716\n",
      "Train Epoch: 142 [6000/8040 (75%)]\tLoss: 18.946598\n",
      "Train Epoch: 142 [7200/8040 (90%)]\tLoss: 18.842814\n",
      "====> Epoch: 142 Average loss: 18.9631\n",
      "epoch: 142, beta=0, frac_anom=0.01\n",
      "Train Epoch: 143 [0/8040 (0%)]\tLoss: 18.201707\n",
      "Train Epoch: 143 [1200/8040 (15%)]\tLoss: 19.187480\n",
      "Train Epoch: 143 [2400/8040 (30%)]\tLoss: 18.391996\n",
      "Train Epoch: 143 [3600/8040 (45%)]\tLoss: 18.557479\n",
      "Train Epoch: 143 [4800/8040 (60%)]\tLoss: 19.589838\n",
      "Train Epoch: 143 [6000/8040 (75%)]\tLoss: 19.815509\n",
      "Train Epoch: 143 [7200/8040 (90%)]\tLoss: 19.282882\n",
      "====> Epoch: 143 Average loss: 18.9268\n",
      "epoch: 143, beta=0, frac_anom=0.01\n",
      "Train Epoch: 144 [0/8040 (0%)]\tLoss: 17.919151\n",
      "Train Epoch: 144 [1200/8040 (15%)]\tLoss: 19.582992\n",
      "Train Epoch: 144 [2400/8040 (30%)]\tLoss: 18.987927\n",
      "Train Epoch: 144 [3600/8040 (45%)]\tLoss: 19.253931\n",
      "Train Epoch: 144 [4800/8040 (60%)]\tLoss: 18.460166\n",
      "Train Epoch: 144 [6000/8040 (75%)]\tLoss: 19.238218\n",
      "Train Epoch: 144 [7200/8040 (90%)]\tLoss: 19.747457\n",
      "====> Epoch: 144 Average loss: 18.9009\n",
      "epoch: 144, beta=0, frac_anom=0.01\n",
      "Train Epoch: 145 [0/8040 (0%)]\tLoss: 18.574247\n",
      "Train Epoch: 145 [1200/8040 (15%)]\tLoss: 18.127629\n",
      "Train Epoch: 145 [2400/8040 (30%)]\tLoss: 18.581846\n",
      "Train Epoch: 145 [3600/8040 (45%)]\tLoss: 18.384686\n",
      "Train Epoch: 145 [4800/8040 (60%)]\tLoss: 19.298938\n",
      "Train Epoch: 145 [6000/8040 (75%)]\tLoss: 18.293103\n",
      "Train Epoch: 145 [7200/8040 (90%)]\tLoss: 18.941209\n",
      "====> Epoch: 145 Average loss: 18.9141\n",
      "epoch: 145, beta=0, frac_anom=0.01\n",
      "Train Epoch: 146 [0/8040 (0%)]\tLoss: 18.405701\n",
      "Train Epoch: 146 [1200/8040 (15%)]\tLoss: 18.850722\n",
      "Train Epoch: 146 [2400/8040 (30%)]\tLoss: 18.784155\n",
      "Train Epoch: 146 [3600/8040 (45%)]\tLoss: 18.131527\n",
      "Train Epoch: 146 [4800/8040 (60%)]\tLoss: 18.769670\n",
      "Train Epoch: 146 [6000/8040 (75%)]\tLoss: 19.453094\n",
      "Train Epoch: 146 [7200/8040 (90%)]\tLoss: 19.724915\n",
      "====> Epoch: 146 Average loss: 18.8852\n",
      "epoch: 146, beta=0, frac_anom=0.01\n",
      "Train Epoch: 147 [0/8040 (0%)]\tLoss: 19.492794\n",
      "Train Epoch: 147 [1200/8040 (15%)]\tLoss: 18.351469\n",
      "Train Epoch: 147 [2400/8040 (30%)]\tLoss: 18.482513\n",
      "Train Epoch: 147 [3600/8040 (45%)]\tLoss: 19.175692\n",
      "Train Epoch: 147 [4800/8040 (60%)]\tLoss: 19.099711\n",
      "Train Epoch: 147 [6000/8040 (75%)]\tLoss: 18.950411\n",
      "Train Epoch: 147 [7200/8040 (90%)]\tLoss: 19.171464\n",
      "====> Epoch: 147 Average loss: 18.9226\n",
      "epoch: 147, beta=0, frac_anom=0.01\n",
      "Train Epoch: 148 [0/8040 (0%)]\tLoss: 18.349141\n",
      "Train Epoch: 148 [1200/8040 (15%)]\tLoss: 19.087695\n",
      "Train Epoch: 148 [2400/8040 (30%)]\tLoss: 19.153418\n",
      "Train Epoch: 148 [3600/8040 (45%)]\tLoss: 19.154801\n",
      "Train Epoch: 148 [4800/8040 (60%)]\tLoss: 20.151143\n",
      "Train Epoch: 148 [6000/8040 (75%)]\tLoss: 18.618864\n",
      "Train Epoch: 148 [7200/8040 (90%)]\tLoss: 18.880420\n",
      "====> Epoch: 148 Average loss: 18.9806\n",
      "epoch: 148, beta=0, frac_anom=0.01\n",
      "Train Epoch: 149 [0/8040 (0%)]\tLoss: 19.185229\n",
      "Train Epoch: 149 [1200/8040 (15%)]\tLoss: 19.149898\n",
      "Train Epoch: 149 [2400/8040 (30%)]\tLoss: 19.004637\n",
      "Train Epoch: 149 [3600/8040 (45%)]\tLoss: 19.196077\n",
      "Train Epoch: 149 [4800/8040 (60%)]\tLoss: 18.242806\n",
      "Train Epoch: 149 [6000/8040 (75%)]\tLoss: 18.742338\n",
      "Train Epoch: 149 [7200/8040 (90%)]\tLoss: 20.187585\n",
      "====> Epoch: 149 Average loss: 18.9610\n",
      "epoch: 149, beta=0, frac_anom=0.01\n",
      "Train Epoch: 150 [0/8040 (0%)]\tLoss: 18.648389\n",
      "Train Epoch: 150 [1200/8040 (15%)]\tLoss: 18.385722\n",
      "Train Epoch: 150 [2400/8040 (30%)]\tLoss: 17.822957\n",
      "Train Epoch: 150 [3600/8040 (45%)]\tLoss: 17.825854\n",
      "Train Epoch: 150 [4800/8040 (60%)]\tLoss: 18.164276\n",
      "Train Epoch: 150 [6000/8040 (75%)]\tLoss: 18.084892\n",
      "Train Epoch: 150 [7200/8040 (90%)]\tLoss: 18.643207\n",
      "====> Epoch: 150 Average loss: 18.8640\n",
      "epoch: 150, beta=0, frac_anom=0.01\n",
      "====> Test set loss: 13.1121\n",
      "Train Epoch: 1 [0/8040 (0%)]\tLoss: 147.389941\n",
      "Train Epoch: 1 [1200/8040 (15%)]\tLoss: 46.036357\n",
      "Train Epoch: 1 [2400/8040 (30%)]\tLoss: 39.104740\n",
      "Train Epoch: 1 [3600/8040 (45%)]\tLoss: 36.001090\n",
      "Train Epoch: 1 [4800/8040 (60%)]\tLoss: 31.843669\n",
      "Train Epoch: 1 [6000/8040 (75%)]\tLoss: 31.737535\n",
      "Train Epoch: 1 [7200/8040 (90%)]\tLoss: 30.498783\n",
      "====> Epoch: 1 Average loss: 41.2583\n",
      "epoch: 1, beta=0, frac_anom=0.05\n",
      "Train Epoch: 2 [0/8040 (0%)]\tLoss: 30.496960\n",
      "Train Epoch: 2 [1200/8040 (15%)]\tLoss: 32.132037\n",
      "Train Epoch: 2 [2400/8040 (30%)]\tLoss: 31.128805\n",
      "Train Epoch: 2 [3600/8040 (45%)]\tLoss: 28.270886\n",
      "Train Epoch: 2 [4800/8040 (60%)]\tLoss: 31.589549\n",
      "Train Epoch: 2 [6000/8040 (75%)]\tLoss: 28.051038\n",
      "Train Epoch: 2 [7200/8040 (90%)]\tLoss: 31.209123\n",
      "====> Epoch: 2 Average loss: 30.0647\n",
      "epoch: 2, beta=0, frac_anom=0.05\n",
      "Train Epoch: 3 [0/8040 (0%)]\tLoss: 30.044672\n",
      "Train Epoch: 3 [1200/8040 (15%)]\tLoss: 28.470441\n",
      "Train Epoch: 3 [2400/8040 (30%)]\tLoss: 27.445093\n",
      "Train Epoch: 3 [3600/8040 (45%)]\tLoss: 28.505497\n",
      "Train Epoch: 3 [4800/8040 (60%)]\tLoss: 28.233777\n",
      "Train Epoch: 3 [6000/8040 (75%)]\tLoss: 26.851813\n",
      "Train Epoch: 3 [7200/8040 (90%)]\tLoss: 25.800802\n",
      "====> Epoch: 3 Average loss: 27.9314\n",
      "epoch: 3, beta=0, frac_anom=0.05\n",
      "Train Epoch: 4 [0/8040 (0%)]\tLoss: 26.884414\n",
      "Train Epoch: 4 [1200/8040 (15%)]\tLoss: 24.310065\n",
      "Train Epoch: 4 [2400/8040 (30%)]\tLoss: 26.031665\n",
      "Train Epoch: 4 [3600/8040 (45%)]\tLoss: 24.373442\n",
      "Train Epoch: 4 [4800/8040 (60%)]\tLoss: 25.766878\n",
      "Train Epoch: 4 [6000/8040 (75%)]\tLoss: 27.020526\n",
      "Train Epoch: 4 [7200/8040 (90%)]\tLoss: 23.746342\n",
      "====> Epoch: 4 Average loss: 26.2234\n",
      "epoch: 4, beta=0, frac_anom=0.05\n",
      "Train Epoch: 5 [0/8040 (0%)]\tLoss: 25.959786\n",
      "Train Epoch: 5 [1200/8040 (15%)]\tLoss: 23.933374\n",
      "Train Epoch: 5 [2400/8040 (30%)]\tLoss: 23.199939\n",
      "Train Epoch: 5 [3600/8040 (45%)]\tLoss: 24.837752\n",
      "Train Epoch: 5 [4800/8040 (60%)]\tLoss: 27.020266\n",
      "Train Epoch: 5 [6000/8040 (75%)]\tLoss: 26.225456\n",
      "Train Epoch: 5 [7200/8040 (90%)]\tLoss: 25.277991\n",
      "====> Epoch: 5 Average loss: 25.0396\n",
      "epoch: 5, beta=0, frac_anom=0.05\n",
      "Train Epoch: 6 [0/8040 (0%)]\tLoss: 25.043512\n",
      "Train Epoch: 6 [1200/8040 (15%)]\tLoss: 23.910986\n",
      "Train Epoch: 6 [2400/8040 (30%)]\tLoss: 25.360655\n",
      "Train Epoch: 6 [3600/8040 (45%)]\tLoss: 23.864012\n",
      "Train Epoch: 6 [4800/8040 (60%)]\tLoss: 22.928719\n",
      "Train Epoch: 6 [6000/8040 (75%)]\tLoss: 23.134617\n",
      "Train Epoch: 6 [7200/8040 (90%)]\tLoss: 24.085748\n",
      "====> Epoch: 6 Average loss: 24.3609\n",
      "epoch: 6, beta=0, frac_anom=0.05\n",
      "Train Epoch: 7 [0/8040 (0%)]\tLoss: 23.936058\n",
      "Train Epoch: 7 [1200/8040 (15%)]\tLoss: 23.793101\n",
      "Train Epoch: 7 [2400/8040 (30%)]\tLoss: 24.772233\n",
      "Train Epoch: 7 [3600/8040 (45%)]\tLoss: 25.738495\n",
      "Train Epoch: 7 [4800/8040 (60%)]\tLoss: 24.100334\n",
      "Train Epoch: 7 [6000/8040 (75%)]\tLoss: 24.852869\n",
      "Train Epoch: 7 [7200/8040 (90%)]\tLoss: 24.455481\n",
      "====> Epoch: 7 Average loss: 23.8360\n",
      "epoch: 7, beta=0, frac_anom=0.05\n",
      "Train Epoch: 8 [0/8040 (0%)]\tLoss: 23.521965\n",
      "Train Epoch: 8 [1200/8040 (15%)]\tLoss: 23.590462\n",
      "Train Epoch: 8 [2400/8040 (30%)]\tLoss: 26.118070\n",
      "Train Epoch: 8 [3600/8040 (45%)]\tLoss: 25.198757\n",
      "Train Epoch: 8 [4800/8040 (60%)]\tLoss: 23.298456\n",
      "Train Epoch: 8 [6000/8040 (75%)]\tLoss: 23.359408\n",
      "Train Epoch: 8 [7200/8040 (90%)]\tLoss: 23.304118\n",
      "====> Epoch: 8 Average loss: 23.5853\n",
      "epoch: 8, beta=0, frac_anom=0.05\n",
      "Train Epoch: 9 [0/8040 (0%)]\tLoss: 22.412240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [1200/8040 (15%)]\tLoss: 23.605774\n",
      "Train Epoch: 9 [2400/8040 (30%)]\tLoss: 23.154061\n",
      "Train Epoch: 9 [3600/8040 (45%)]\tLoss: 23.235510\n",
      "Train Epoch: 9 [4800/8040 (60%)]\tLoss: 21.806826\n",
      "Train Epoch: 9 [6000/8040 (75%)]\tLoss: 23.245502\n",
      "Train Epoch: 9 [7200/8040 (90%)]\tLoss: 24.160240\n",
      "====> Epoch: 9 Average loss: 23.3005\n",
      "epoch: 9, beta=0, frac_anom=0.05\n",
      "Train Epoch: 10 [0/8040 (0%)]\tLoss: 23.863220\n",
      "Train Epoch: 10 [1200/8040 (15%)]\tLoss: 23.234983\n",
      "Train Epoch: 10 [2400/8040 (30%)]\tLoss: 23.313159\n",
      "Train Epoch: 10 [3600/8040 (45%)]\tLoss: 22.522117\n",
      "Train Epoch: 10 [4800/8040 (60%)]\tLoss: 23.240889\n",
      "Train Epoch: 10 [6000/8040 (75%)]\tLoss: 22.654460\n",
      "Train Epoch: 10 [7200/8040 (90%)]\tLoss: 22.866418\n",
      "====> Epoch: 10 Average loss: 22.9954\n",
      "epoch: 10, beta=0, frac_anom=0.05\n",
      "Train Epoch: 11 [0/8040 (0%)]\tLoss: 23.198535\n",
      "Train Epoch: 11 [1200/8040 (15%)]\tLoss: 23.814811\n",
      "Train Epoch: 11 [2400/8040 (30%)]\tLoss: 22.113536\n",
      "Train Epoch: 11 [3600/8040 (45%)]\tLoss: 23.749656\n",
      "Train Epoch: 11 [4800/8040 (60%)]\tLoss: 22.758813\n",
      "Train Epoch: 11 [6000/8040 (75%)]\tLoss: 23.948281\n",
      "Train Epoch: 11 [7200/8040 (90%)]\tLoss: 24.285771\n",
      "====> Epoch: 11 Average loss: 22.8720\n",
      "epoch: 11, beta=0, frac_anom=0.05\n",
      "Train Epoch: 12 [0/8040 (0%)]\tLoss: 23.096497\n",
      "Train Epoch: 12 [1200/8040 (15%)]\tLoss: 22.453739\n",
      "Train Epoch: 12 [2400/8040 (30%)]\tLoss: 21.848568\n",
      "Train Epoch: 12 [3600/8040 (45%)]\tLoss: 22.530200\n",
      "Train Epoch: 12 [4800/8040 (60%)]\tLoss: 22.909340\n",
      "Train Epoch: 12 [6000/8040 (75%)]\tLoss: 21.612181\n",
      "Train Epoch: 12 [7200/8040 (90%)]\tLoss: 22.662966\n",
      "====> Epoch: 12 Average loss: 22.6360\n",
      "epoch: 12, beta=0, frac_anom=0.05\n",
      "Train Epoch: 13 [0/8040 (0%)]\tLoss: 22.928518\n",
      "Train Epoch: 13 [1200/8040 (15%)]\tLoss: 22.850942\n",
      "Train Epoch: 13 [2400/8040 (30%)]\tLoss: 22.524152\n",
      "Train Epoch: 13 [3600/8040 (45%)]\tLoss: 22.492676\n",
      "Train Epoch: 13 [4800/8040 (60%)]\tLoss: 22.594906\n",
      "Train Epoch: 13 [6000/8040 (75%)]\tLoss: 22.237952\n",
      "Train Epoch: 13 [7200/8040 (90%)]\tLoss: 21.441219\n",
      "====> Epoch: 13 Average loss: 22.4382\n",
      "epoch: 13, beta=0, frac_anom=0.05\n",
      "Train Epoch: 14 [0/8040 (0%)]\tLoss: 23.482747\n",
      "Train Epoch: 14 [1200/8040 (15%)]\tLoss: 22.325907\n",
      "Train Epoch: 14 [2400/8040 (30%)]\tLoss: 22.806881\n",
      "Train Epoch: 14 [3600/8040 (45%)]\tLoss: 23.033512\n",
      "Train Epoch: 14 [4800/8040 (60%)]\tLoss: 21.594397\n",
      "Train Epoch: 14 [6000/8040 (75%)]\tLoss: 21.450826\n",
      "Train Epoch: 14 [7200/8040 (90%)]\tLoss: 20.662006\n",
      "====> Epoch: 14 Average loss: 22.3443\n",
      "epoch: 14, beta=0, frac_anom=0.05\n",
      "Train Epoch: 15 [0/8040 (0%)]\tLoss: 21.164478\n",
      "Train Epoch: 15 [1200/8040 (15%)]\tLoss: 21.692505\n",
      "Train Epoch: 15 [2400/8040 (30%)]\tLoss: 22.559888\n",
      "Train Epoch: 15 [3600/8040 (45%)]\tLoss: 20.105827\n",
      "Train Epoch: 15 [4800/8040 (60%)]\tLoss: 25.001782\n",
      "Train Epoch: 15 [6000/8040 (75%)]\tLoss: 22.258453\n",
      "Train Epoch: 15 [7200/8040 (90%)]\tLoss: 21.248419\n",
      "====> Epoch: 15 Average loss: 22.0752\n",
      "epoch: 15, beta=0, frac_anom=0.05\n",
      "Train Epoch: 16 [0/8040 (0%)]\tLoss: 21.895667\n",
      "Train Epoch: 16 [1200/8040 (15%)]\tLoss: 22.757039\n",
      "Train Epoch: 16 [2400/8040 (30%)]\tLoss: 21.381042\n",
      "Train Epoch: 16 [3600/8040 (45%)]\tLoss: 23.923541\n",
      "Train Epoch: 16 [4800/8040 (60%)]\tLoss: 21.246332\n",
      "Train Epoch: 16 [6000/8040 (75%)]\tLoss: 22.275521\n",
      "Train Epoch: 16 [7200/8040 (90%)]\tLoss: 21.736792\n",
      "====> Epoch: 16 Average loss: 21.9576\n",
      "epoch: 16, beta=0, frac_anom=0.05\n",
      "Train Epoch: 17 [0/8040 (0%)]\tLoss: 21.648012\n",
      "Train Epoch: 17 [1200/8040 (15%)]\tLoss: 21.870490\n",
      "Train Epoch: 17 [2400/8040 (30%)]\tLoss: 21.370976\n",
      "Train Epoch: 17 [3600/8040 (45%)]\tLoss: 21.781608\n",
      "Train Epoch: 17 [4800/8040 (60%)]\tLoss: 21.239028\n",
      "Train Epoch: 17 [6000/8040 (75%)]\tLoss: 21.260533\n",
      "Train Epoch: 17 [7200/8040 (90%)]\tLoss: 20.784843\n",
      "====> Epoch: 17 Average loss: 21.9332\n",
      "epoch: 17, beta=0, frac_anom=0.05\n",
      "Train Epoch: 18 [0/8040 (0%)]\tLoss: 22.949074\n",
      "Train Epoch: 18 [1200/8040 (15%)]\tLoss: 20.422626\n",
      "Train Epoch: 18 [2400/8040 (30%)]\tLoss: 22.495241\n",
      "Train Epoch: 18 [3600/8040 (45%)]\tLoss: 21.816807\n",
      "Train Epoch: 18 [4800/8040 (60%)]\tLoss: 22.024626\n",
      "Train Epoch: 18 [6000/8040 (75%)]\tLoss: 21.780054\n",
      "Train Epoch: 18 [7200/8040 (90%)]\tLoss: 21.723197\n",
      "====> Epoch: 18 Average loss: 21.7458\n",
      "epoch: 18, beta=0, frac_anom=0.05\n",
      "Train Epoch: 19 [0/8040 (0%)]\tLoss: 21.107135\n",
      "Train Epoch: 19 [1200/8040 (15%)]\tLoss: 21.204523\n",
      "Train Epoch: 19 [2400/8040 (30%)]\tLoss: 21.834810\n",
      "Train Epoch: 19 [3600/8040 (45%)]\tLoss: 20.960624\n",
      "Train Epoch: 19 [4800/8040 (60%)]\tLoss: 22.261682\n",
      "Train Epoch: 19 [6000/8040 (75%)]\tLoss: 22.022994\n",
      "Train Epoch: 19 [7200/8040 (90%)]\tLoss: 22.490102\n",
      "====> Epoch: 19 Average loss: 21.6319\n",
      "epoch: 19, beta=0, frac_anom=0.05\n",
      "Train Epoch: 20 [0/8040 (0%)]\tLoss: 22.104669\n",
      "Train Epoch: 20 [1200/8040 (15%)]\tLoss: 21.448816\n",
      "Train Epoch: 20 [2400/8040 (30%)]\tLoss: 21.707894\n",
      "Train Epoch: 20 [3600/8040 (45%)]\tLoss: 22.732436\n",
      "Train Epoch: 20 [4800/8040 (60%)]\tLoss: 19.503408\n",
      "Train Epoch: 20 [6000/8040 (75%)]\tLoss: 22.159412\n",
      "Train Epoch: 20 [7200/8040 (90%)]\tLoss: 21.931793\n",
      "====> Epoch: 20 Average loss: 21.6591\n",
      "epoch: 20, beta=0, frac_anom=0.05\n",
      "Train Epoch: 21 [0/8040 (0%)]\tLoss: 22.440743\n",
      "Train Epoch: 21 [1200/8040 (15%)]\tLoss: 20.970992\n",
      "Train Epoch: 21 [2400/8040 (30%)]\tLoss: 22.245593\n",
      "Train Epoch: 21 [3600/8040 (45%)]\tLoss: 21.757131\n",
      "Train Epoch: 21 [4800/8040 (60%)]\tLoss: 21.229317\n",
      "Train Epoch: 21 [6000/8040 (75%)]\tLoss: 21.339372\n",
      "Train Epoch: 21 [7200/8040 (90%)]\tLoss: 21.238542\n",
      "====> Epoch: 21 Average loss: 21.5201\n",
      "epoch: 21, beta=0, frac_anom=0.05\n",
      "Train Epoch: 22 [0/8040 (0%)]\tLoss: 21.521560\n",
      "Train Epoch: 22 [1200/8040 (15%)]\tLoss: 20.404187\n",
      "Train Epoch: 22 [2400/8040 (30%)]\tLoss: 22.442694\n",
      "Train Epoch: 22 [3600/8040 (45%)]\tLoss: 21.813098\n",
      "Train Epoch: 22 [4800/8040 (60%)]\tLoss: 21.690857\n",
      "Train Epoch: 22 [6000/8040 (75%)]\tLoss: 20.892700\n",
      "Train Epoch: 22 [7200/8040 (90%)]\tLoss: 22.030017\n",
      "====> Epoch: 22 Average loss: 21.3870\n",
      "epoch: 22, beta=0, frac_anom=0.05\n",
      "Train Epoch: 23 [0/8040 (0%)]\tLoss: 21.119238\n",
      "Train Epoch: 23 [1200/8040 (15%)]\tLoss: 24.512793\n",
      "Train Epoch: 23 [2400/8040 (30%)]\tLoss: 20.720662\n",
      "Train Epoch: 23 [3600/8040 (45%)]\tLoss: 22.133242\n",
      "Train Epoch: 23 [4800/8040 (60%)]\tLoss: 22.716327\n",
      "Train Epoch: 23 [6000/8040 (75%)]\tLoss: 20.106130\n",
      "Train Epoch: 23 [7200/8040 (90%)]\tLoss: 21.854218\n",
      "====> Epoch: 23 Average loss: 21.3859\n",
      "epoch: 23, beta=0, frac_anom=0.05\n",
      "Train Epoch: 24 [0/8040 (0%)]\tLoss: 20.852035\n",
      "Train Epoch: 24 [1200/8040 (15%)]\tLoss: 19.510217\n",
      "Train Epoch: 24 [2400/8040 (30%)]\tLoss: 21.092751\n",
      "Train Epoch: 24 [3600/8040 (45%)]\tLoss: 20.576282\n",
      "Train Epoch: 24 [4800/8040 (60%)]\tLoss: 21.151845\n",
      "Train Epoch: 24 [6000/8040 (75%)]\tLoss: 20.888765\n",
      "Train Epoch: 24 [7200/8040 (90%)]\tLoss: 21.106494\n",
      "====> Epoch: 24 Average loss: 21.3222\n",
      "epoch: 24, beta=0, frac_anom=0.05\n",
      "Train Epoch: 25 [0/8040 (0%)]\tLoss: 21.191032\n",
      "Train Epoch: 25 [1200/8040 (15%)]\tLoss: 21.290350\n",
      "Train Epoch: 25 [2400/8040 (30%)]\tLoss: 19.502808\n",
      "Train Epoch: 25 [3600/8040 (45%)]\tLoss: 21.455491\n",
      "Train Epoch: 25 [4800/8040 (60%)]\tLoss: 21.350578\n",
      "Train Epoch: 25 [6000/8040 (75%)]\tLoss: 21.684060\n",
      "Train Epoch: 25 [7200/8040 (90%)]\tLoss: 19.977218\n",
      "====> Epoch: 25 Average loss: 21.1920\n",
      "epoch: 25, beta=0, frac_anom=0.05\n",
      "Train Epoch: 26 [0/8040 (0%)]\tLoss: 21.305450\n",
      "Train Epoch: 26 [1200/8040 (15%)]\tLoss: 20.877246\n",
      "Train Epoch: 26 [2400/8040 (30%)]\tLoss: 20.973523\n",
      "Train Epoch: 26 [3600/8040 (45%)]\tLoss: 21.895829\n",
      "Train Epoch: 26 [4800/8040 (60%)]\tLoss: 22.233374\n",
      "Train Epoch: 26 [6000/8040 (75%)]\tLoss: 21.235087\n",
      "Train Epoch: 26 [7200/8040 (90%)]\tLoss: 21.525193\n",
      "====> Epoch: 26 Average loss: 21.1681\n",
      "epoch: 26, beta=0, frac_anom=0.05\n",
      "Train Epoch: 27 [0/8040 (0%)]\tLoss: 21.778843\n",
      "Train Epoch: 27 [1200/8040 (15%)]\tLoss: 21.085075\n",
      "Train Epoch: 27 [2400/8040 (30%)]\tLoss: 20.300643\n",
      "Train Epoch: 27 [3600/8040 (45%)]\tLoss: 21.551253\n",
      "Train Epoch: 27 [4800/8040 (60%)]\tLoss: 21.326265\n",
      "Train Epoch: 27 [6000/8040 (75%)]\tLoss: 21.641665\n",
      "Train Epoch: 27 [7200/8040 (90%)]\tLoss: 21.371167\n",
      "====> Epoch: 27 Average loss: 21.1712\n",
      "epoch: 27, beta=0, frac_anom=0.05\n",
      "Train Epoch: 28 [0/8040 (0%)]\tLoss: 20.076265\n",
      "Train Epoch: 28 [1200/8040 (15%)]\tLoss: 21.371651\n",
      "Train Epoch: 28 [2400/8040 (30%)]\tLoss: 21.510478\n",
      "Train Epoch: 28 [3600/8040 (45%)]\tLoss: 19.751579\n",
      "Train Epoch: 28 [4800/8040 (60%)]\tLoss: 22.868101\n",
      "Train Epoch: 28 [6000/8040 (75%)]\tLoss: 20.261713\n",
      "Train Epoch: 28 [7200/8040 (90%)]\tLoss: 20.515572\n",
      "====> Epoch: 28 Average loss: 21.0990\n",
      "epoch: 28, beta=0, frac_anom=0.05\n",
      "Train Epoch: 29 [0/8040 (0%)]\tLoss: 21.458732\n",
      "Train Epoch: 29 [1200/8040 (15%)]\tLoss: 20.663118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 29 [2400/8040 (30%)]\tLoss: 21.289164\n",
      "Train Epoch: 29 [3600/8040 (45%)]\tLoss: 21.391284\n",
      "Train Epoch: 29 [4800/8040 (60%)]\tLoss: 19.638025\n",
      "Train Epoch: 29 [6000/8040 (75%)]\tLoss: 21.465782\n",
      "Train Epoch: 29 [7200/8040 (90%)]\tLoss: 21.089895\n",
      "====> Epoch: 29 Average loss: 21.0420\n",
      "epoch: 29, beta=0, frac_anom=0.05\n",
      "Train Epoch: 30 [0/8040 (0%)]\tLoss: 21.676921\n",
      "Train Epoch: 30 [1200/8040 (15%)]\tLoss: 20.153387\n",
      "Train Epoch: 30 [2400/8040 (30%)]\tLoss: 22.657229\n",
      "Train Epoch: 30 [3600/8040 (45%)]\tLoss: 20.703516\n",
      "Train Epoch: 30 [4800/8040 (60%)]\tLoss: 21.065405\n",
      "Train Epoch: 30 [6000/8040 (75%)]\tLoss: 22.207013\n",
      "Train Epoch: 30 [7200/8040 (90%)]\tLoss: 21.516538\n",
      "====> Epoch: 30 Average loss: 20.9943\n",
      "epoch: 30, beta=0, frac_anom=0.05\n",
      "Train Epoch: 31 [0/8040 (0%)]\tLoss: 19.206517\n",
      "Train Epoch: 31 [1200/8040 (15%)]\tLoss: 20.611025\n",
      "Train Epoch: 31 [2400/8040 (30%)]\tLoss: 20.034737\n",
      "Train Epoch: 31 [3600/8040 (45%)]\tLoss: 21.440684\n",
      "Train Epoch: 31 [4800/8040 (60%)]\tLoss: 20.134963\n",
      "Train Epoch: 31 [6000/8040 (75%)]\tLoss: 19.822278\n",
      "Train Epoch: 31 [7200/8040 (90%)]\tLoss: 21.117519\n",
      "====> Epoch: 31 Average loss: 20.9346\n",
      "epoch: 31, beta=0, frac_anom=0.05\n",
      "Train Epoch: 32 [0/8040 (0%)]\tLoss: 20.974432\n",
      "Train Epoch: 32 [1200/8040 (15%)]\tLoss: 22.042948\n",
      "Train Epoch: 32 [2400/8040 (30%)]\tLoss: 20.607697\n",
      "Train Epoch: 32 [3600/8040 (45%)]\tLoss: 21.392574\n",
      "Train Epoch: 32 [4800/8040 (60%)]\tLoss: 20.109281\n",
      "Train Epoch: 32 [6000/8040 (75%)]\tLoss: 18.597673\n",
      "Train Epoch: 32 [7200/8040 (90%)]\tLoss: 22.289467\n",
      "====> Epoch: 32 Average loss: 20.8669\n",
      "epoch: 32, beta=0, frac_anom=0.05\n",
      "Train Epoch: 33 [0/8040 (0%)]\tLoss: 20.218132\n",
      "Train Epoch: 33 [1200/8040 (15%)]\tLoss: 21.775989\n",
      "Train Epoch: 33 [2400/8040 (30%)]\tLoss: 20.738560\n",
      "Train Epoch: 33 [3600/8040 (45%)]\tLoss: 20.373810\n",
      "Train Epoch: 33 [4800/8040 (60%)]\tLoss: 20.702846\n",
      "Train Epoch: 33 [6000/8040 (75%)]\tLoss: 20.844041\n",
      "Train Epoch: 33 [7200/8040 (90%)]\tLoss: 19.807107\n",
      "====> Epoch: 33 Average loss: 20.8693\n",
      "epoch: 33, beta=0, frac_anom=0.05\n",
      "Train Epoch: 34 [0/8040 (0%)]\tLoss: 18.891425\n",
      "Train Epoch: 34 [1200/8040 (15%)]\tLoss: 20.167558\n",
      "Train Epoch: 34 [2400/8040 (30%)]\tLoss: 20.735392\n",
      "Train Epoch: 34 [3600/8040 (45%)]\tLoss: 21.063096\n",
      "Train Epoch: 34 [4800/8040 (60%)]\tLoss: 20.114071\n",
      "Train Epoch: 34 [6000/8040 (75%)]\tLoss: 19.378044\n",
      "Train Epoch: 34 [7200/8040 (90%)]\tLoss: 21.144676\n",
      "====> Epoch: 34 Average loss: 20.8298\n",
      "epoch: 34, beta=0, frac_anom=0.05\n",
      "Train Epoch: 35 [0/8040 (0%)]\tLoss: 20.810620\n",
      "Train Epoch: 35 [1200/8040 (15%)]\tLoss: 20.625783\n",
      "Train Epoch: 35 [2400/8040 (30%)]\tLoss: 20.405609\n",
      "Train Epoch: 35 [3600/8040 (45%)]\tLoss: 19.362008\n",
      "Train Epoch: 35 [4800/8040 (60%)]\tLoss: 22.325975\n",
      "Train Epoch: 35 [6000/8040 (75%)]\tLoss: 20.602651\n",
      "Train Epoch: 35 [7200/8040 (90%)]\tLoss: 21.152460\n",
      "====> Epoch: 35 Average loss: 20.7645\n",
      "epoch: 35, beta=0, frac_anom=0.05\n",
      "Train Epoch: 36 [0/8040 (0%)]\tLoss: 20.523283\n",
      "Train Epoch: 36 [1200/8040 (15%)]\tLoss: 20.495418\n",
      "Train Epoch: 36 [2400/8040 (30%)]\tLoss: 21.460301\n",
      "Train Epoch: 36 [3600/8040 (45%)]\tLoss: 19.874158\n",
      "Train Epoch: 36 [4800/8040 (60%)]\tLoss: 19.873295\n",
      "Train Epoch: 36 [6000/8040 (75%)]\tLoss: 20.810583\n",
      "Train Epoch: 36 [7200/8040 (90%)]\tLoss: 20.264811\n",
      "====> Epoch: 36 Average loss: 20.7172\n",
      "epoch: 36, beta=0, frac_anom=0.05\n",
      "Train Epoch: 37 [0/8040 (0%)]\tLoss: 19.252743\n",
      "Train Epoch: 37 [1200/8040 (15%)]\tLoss: 20.323942\n",
      "Train Epoch: 37 [2400/8040 (30%)]\tLoss: 21.760144\n",
      "Train Epoch: 37 [3600/8040 (45%)]\tLoss: 19.470650\n",
      "Train Epoch: 37 [4800/8040 (60%)]\tLoss: 20.876160\n",
      "Train Epoch: 37 [6000/8040 (75%)]\tLoss: 22.374577\n",
      "Train Epoch: 37 [7200/8040 (90%)]\tLoss: 19.375090\n",
      "====> Epoch: 37 Average loss: 20.6552\n",
      "epoch: 37, beta=0, frac_anom=0.05\n",
      "Train Epoch: 38 [0/8040 (0%)]\tLoss: 22.207479\n",
      "Train Epoch: 38 [1200/8040 (15%)]\tLoss: 19.513180\n",
      "Train Epoch: 38 [2400/8040 (30%)]\tLoss: 20.319019\n",
      "Train Epoch: 38 [3600/8040 (45%)]\tLoss: 21.300020\n",
      "Train Epoch: 38 [4800/8040 (60%)]\tLoss: 20.780371\n",
      "Train Epoch: 38 [6000/8040 (75%)]\tLoss: 20.687714\n",
      "Train Epoch: 38 [7200/8040 (90%)]\tLoss: 19.363285\n",
      "====> Epoch: 38 Average loss: 20.6474\n",
      "epoch: 38, beta=0, frac_anom=0.05\n",
      "Train Epoch: 39 [0/8040 (0%)]\tLoss: 20.479865\n",
      "Train Epoch: 39 [1200/8040 (15%)]\tLoss: 20.753752\n",
      "Train Epoch: 39 [2400/8040 (30%)]\tLoss: 19.792151\n",
      "Train Epoch: 39 [3600/8040 (45%)]\tLoss: 19.401676\n",
      "Train Epoch: 39 [4800/8040 (60%)]\tLoss: 20.755731\n",
      "Train Epoch: 39 [6000/8040 (75%)]\tLoss: 21.864781\n",
      "Train Epoch: 39 [7200/8040 (90%)]\tLoss: 20.489512\n",
      "====> Epoch: 39 Average loss: 20.5841\n",
      "epoch: 39, beta=0, frac_anom=0.05\n",
      "Train Epoch: 40 [0/8040 (0%)]\tLoss: 21.204985\n",
      "Train Epoch: 40 [1200/8040 (15%)]\tLoss: 20.534505\n",
      "Train Epoch: 40 [2400/8040 (30%)]\tLoss: 21.637441\n",
      "Train Epoch: 40 [3600/8040 (45%)]\tLoss: 19.430312\n",
      "Train Epoch: 40 [4800/8040 (60%)]\tLoss: 20.342004\n",
      "Train Epoch: 40 [6000/8040 (75%)]\tLoss: 20.345610\n",
      "Train Epoch: 40 [7200/8040 (90%)]\tLoss: 20.597758\n",
      "====> Epoch: 40 Average loss: 20.5863\n",
      "epoch: 40, beta=0, frac_anom=0.05\n",
      "Train Epoch: 41 [0/8040 (0%)]\tLoss: 20.644926\n",
      "Train Epoch: 41 [1200/8040 (15%)]\tLoss: 20.956974\n",
      "Train Epoch: 41 [2400/8040 (30%)]\tLoss: 20.272795\n",
      "Train Epoch: 41 [3600/8040 (45%)]\tLoss: 20.470642\n",
      "Train Epoch: 41 [4800/8040 (60%)]\tLoss: 19.744405\n",
      "Train Epoch: 41 [6000/8040 (75%)]\tLoss: 20.597388\n",
      "Train Epoch: 41 [7200/8040 (90%)]\tLoss: 19.652836\n",
      "====> Epoch: 41 Average loss: 20.5221\n",
      "epoch: 41, beta=0, frac_anom=0.05\n",
      "Train Epoch: 42 [0/8040 (0%)]\tLoss: 19.358506\n",
      "Train Epoch: 42 [1200/8040 (15%)]\tLoss: 20.130033\n",
      "Train Epoch: 42 [2400/8040 (30%)]\tLoss: 21.342232\n",
      "Train Epoch: 42 [3600/8040 (45%)]\tLoss: 20.573726\n",
      "Train Epoch: 42 [4800/8040 (60%)]\tLoss: 19.752283\n",
      "Train Epoch: 42 [6000/8040 (75%)]\tLoss: 20.582336\n",
      "Train Epoch: 42 [7200/8040 (90%)]\tLoss: 21.171047\n",
      "====> Epoch: 42 Average loss: 20.4945\n",
      "epoch: 42, beta=0, frac_anom=0.05\n",
      "Train Epoch: 43 [0/8040 (0%)]\tLoss: 19.509198\n",
      "Train Epoch: 43 [1200/8040 (15%)]\tLoss: 19.230811\n",
      "Train Epoch: 43 [2400/8040 (30%)]\tLoss: 20.228719\n",
      "Train Epoch: 43 [3600/8040 (45%)]\tLoss: 20.643349\n",
      "Train Epoch: 43 [4800/8040 (60%)]\tLoss: 20.509684\n",
      "Train Epoch: 43 [6000/8040 (75%)]\tLoss: 20.186169\n",
      "Train Epoch: 43 [7200/8040 (90%)]\tLoss: 21.023539\n",
      "====> Epoch: 43 Average loss: 20.4618\n",
      "epoch: 43, beta=0, frac_anom=0.05\n",
      "Train Epoch: 44 [0/8040 (0%)]\tLoss: 20.398568\n",
      "Train Epoch: 44 [1200/8040 (15%)]\tLoss: 19.932817\n",
      "Train Epoch: 44 [2400/8040 (30%)]\tLoss: 21.634351\n",
      "Train Epoch: 44 [3600/8040 (45%)]\tLoss: 20.291622\n",
      "Train Epoch: 44 [4800/8040 (60%)]\tLoss: 20.700899\n",
      "Train Epoch: 44 [6000/8040 (75%)]\tLoss: 19.927852\n",
      "Train Epoch: 44 [7200/8040 (90%)]\tLoss: 19.759241\n",
      "====> Epoch: 44 Average loss: 20.4407\n",
      "epoch: 44, beta=0, frac_anom=0.05\n",
      "Train Epoch: 45 [0/8040 (0%)]\tLoss: 21.847616\n",
      "Train Epoch: 45 [1200/8040 (15%)]\tLoss: 20.032210\n",
      "Train Epoch: 45 [2400/8040 (30%)]\tLoss: 20.641575\n",
      "Train Epoch: 45 [3600/8040 (45%)]\tLoss: 20.842542\n",
      "Train Epoch: 45 [4800/8040 (60%)]\tLoss: 20.403617\n",
      "Train Epoch: 45 [6000/8040 (75%)]\tLoss: 20.483824\n",
      "Train Epoch: 45 [7200/8040 (90%)]\tLoss: 20.535697\n",
      "====> Epoch: 45 Average loss: 20.3963\n",
      "epoch: 45, beta=0, frac_anom=0.05\n",
      "Train Epoch: 46 [0/8040 (0%)]\tLoss: 19.741227\n",
      "Train Epoch: 46 [1200/8040 (15%)]\tLoss: 20.427374\n",
      "Train Epoch: 46 [2400/8040 (30%)]\tLoss: 19.494014\n",
      "Train Epoch: 46 [3600/8040 (45%)]\tLoss: 19.001172\n",
      "Train Epoch: 46 [4800/8040 (60%)]\tLoss: 21.227749\n",
      "Train Epoch: 46 [6000/8040 (75%)]\tLoss: 20.590177\n",
      "Train Epoch: 46 [7200/8040 (90%)]\tLoss: 21.496456\n",
      "====> Epoch: 46 Average loss: 20.4373\n",
      "epoch: 46, beta=0, frac_anom=0.05\n",
      "Train Epoch: 47 [0/8040 (0%)]\tLoss: 18.885514\n",
      "Train Epoch: 47 [1200/8040 (15%)]\tLoss: 19.766325\n",
      "Train Epoch: 47 [2400/8040 (30%)]\tLoss: 21.993951\n",
      "Train Epoch: 47 [3600/8040 (45%)]\tLoss: 20.867346\n",
      "Train Epoch: 47 [4800/8040 (60%)]\tLoss: 21.399626\n",
      "Train Epoch: 47 [6000/8040 (75%)]\tLoss: 20.609810\n",
      "Train Epoch: 47 [7200/8040 (90%)]\tLoss: 21.074664\n",
      "====> Epoch: 47 Average loss: 20.3403\n",
      "epoch: 47, beta=0, frac_anom=0.05\n",
      "Train Epoch: 48 [0/8040 (0%)]\tLoss: 19.224414\n",
      "Train Epoch: 48 [1200/8040 (15%)]\tLoss: 19.440004\n",
      "Train Epoch: 48 [2400/8040 (30%)]\tLoss: 19.599512\n",
      "Train Epoch: 48 [3600/8040 (45%)]\tLoss: 20.684143\n",
      "Train Epoch: 48 [4800/8040 (60%)]\tLoss: 20.534491\n",
      "Train Epoch: 48 [6000/8040 (75%)]\tLoss: 21.157196\n",
      "Train Epoch: 48 [7200/8040 (90%)]\tLoss: 18.811719\n",
      "====> Epoch: 48 Average loss: 20.2880\n",
      "epoch: 48, beta=0, frac_anom=0.05\n",
      "Train Epoch: 49 [0/8040 (0%)]\tLoss: 20.853414\n",
      "Train Epoch: 49 [1200/8040 (15%)]\tLoss: 20.270056\n",
      "Train Epoch: 49 [2400/8040 (30%)]\tLoss: 19.870506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 49 [3600/8040 (45%)]\tLoss: 20.176829\n",
      "Train Epoch: 49 [4800/8040 (60%)]\tLoss: 21.100165\n",
      "Train Epoch: 49 [6000/8040 (75%)]\tLoss: 20.610354\n",
      "Train Epoch: 49 [7200/8040 (90%)]\tLoss: 20.787227\n",
      "====> Epoch: 49 Average loss: 20.3064\n",
      "epoch: 49, beta=0, frac_anom=0.05\n",
      "Train Epoch: 50 [0/8040 (0%)]\tLoss: 20.459753\n",
      "Train Epoch: 50 [1200/8040 (15%)]\tLoss: 19.894912\n",
      "Train Epoch: 50 [2400/8040 (30%)]\tLoss: 21.097463\n",
      "Train Epoch: 50 [3600/8040 (45%)]\tLoss: 18.686764\n",
      "Train Epoch: 50 [4800/8040 (60%)]\tLoss: 19.766899\n",
      "Train Epoch: 50 [6000/8040 (75%)]\tLoss: 22.042934\n",
      "Train Epoch: 50 [7200/8040 (90%)]\tLoss: 20.635990\n",
      "====> Epoch: 50 Average loss: 20.2807\n",
      "epoch: 50, beta=0, frac_anom=0.05\n",
      "Train Epoch: 51 [0/8040 (0%)]\tLoss: 21.239876\n",
      "Train Epoch: 51 [1200/8040 (15%)]\tLoss: 20.872042\n",
      "Train Epoch: 51 [2400/8040 (30%)]\tLoss: 20.292855\n",
      "Train Epoch: 51 [3600/8040 (45%)]\tLoss: 20.645972\n",
      "Train Epoch: 51 [4800/8040 (60%)]\tLoss: 19.110470\n",
      "Train Epoch: 51 [6000/8040 (75%)]\tLoss: 21.760730\n",
      "Train Epoch: 51 [7200/8040 (90%)]\tLoss: 19.632524\n",
      "====> Epoch: 51 Average loss: 20.2677\n",
      "epoch: 51, beta=0, frac_anom=0.05\n",
      "Train Epoch: 52 [0/8040 (0%)]\tLoss: 21.681458\n",
      "Train Epoch: 52 [1200/8040 (15%)]\tLoss: 20.744692\n",
      "Train Epoch: 52 [2400/8040 (30%)]\tLoss: 20.312191\n",
      "Train Epoch: 52 [3600/8040 (45%)]\tLoss: 18.826202\n",
      "Train Epoch: 52 [4800/8040 (60%)]\tLoss: 21.044480\n",
      "Train Epoch: 52 [6000/8040 (75%)]\tLoss: 20.667440\n",
      "Train Epoch: 52 [7200/8040 (90%)]\tLoss: 19.854972\n",
      "====> Epoch: 52 Average loss: 20.1988\n",
      "epoch: 52, beta=0, frac_anom=0.05\n",
      "Train Epoch: 53 [0/8040 (0%)]\tLoss: 20.116882\n",
      "Train Epoch: 53 [1200/8040 (15%)]\tLoss: 21.139677\n",
      "Train Epoch: 53 [2400/8040 (30%)]\tLoss: 21.055648\n",
      "Train Epoch: 53 [3600/8040 (45%)]\tLoss: 20.866410\n",
      "Train Epoch: 53 [4800/8040 (60%)]\tLoss: 20.275739\n",
      "Train Epoch: 53 [6000/8040 (75%)]\tLoss: 20.469442\n",
      "Train Epoch: 53 [7200/8040 (90%)]\tLoss: 20.138027\n",
      "====> Epoch: 53 Average loss: 20.1767\n",
      "epoch: 53, beta=0, frac_anom=0.05\n",
      "Train Epoch: 54 [0/8040 (0%)]\tLoss: 20.182591\n",
      "Train Epoch: 54 [1200/8040 (15%)]\tLoss: 20.635940\n",
      "Train Epoch: 54 [2400/8040 (30%)]\tLoss: 20.423934\n",
      "Train Epoch: 54 [3600/8040 (45%)]\tLoss: 20.025871\n",
      "Train Epoch: 54 [4800/8040 (60%)]\tLoss: 19.006293\n",
      "Train Epoch: 54 [6000/8040 (75%)]\tLoss: 19.803133\n",
      "Train Epoch: 54 [7200/8040 (90%)]\tLoss: 19.086941\n",
      "====> Epoch: 54 Average loss: 20.1927\n",
      "epoch: 54, beta=0, frac_anom=0.05\n",
      "Train Epoch: 55 [0/8040 (0%)]\tLoss: 20.242110\n",
      "Train Epoch: 55 [1200/8040 (15%)]\tLoss: 20.219405\n",
      "Train Epoch: 55 [2400/8040 (30%)]\tLoss: 20.386149\n",
      "Train Epoch: 55 [3600/8040 (45%)]\tLoss: 19.570538\n",
      "Train Epoch: 55 [4800/8040 (60%)]\tLoss: 20.467684\n",
      "Train Epoch: 55 [6000/8040 (75%)]\tLoss: 19.646155\n",
      "Train Epoch: 55 [7200/8040 (90%)]\tLoss: 20.821228\n",
      "====> Epoch: 55 Average loss: 20.1723\n",
      "epoch: 55, beta=0, frac_anom=0.05\n",
      "Train Epoch: 56 [0/8040 (0%)]\tLoss: 19.406653\n",
      "Train Epoch: 56 [1200/8040 (15%)]\tLoss: 18.497587\n",
      "Train Epoch: 56 [2400/8040 (30%)]\tLoss: 19.356571\n",
      "Train Epoch: 56 [3600/8040 (45%)]\tLoss: 21.935730\n",
      "Train Epoch: 56 [4800/8040 (60%)]\tLoss: 19.336336\n",
      "Train Epoch: 56 [6000/8040 (75%)]\tLoss: 20.450942\n",
      "Train Epoch: 56 [7200/8040 (90%)]\tLoss: 20.483905\n",
      "====> Epoch: 56 Average loss: 20.1450\n",
      "epoch: 56, beta=0, frac_anom=0.05\n",
      "Train Epoch: 57 [0/8040 (0%)]\tLoss: 18.620597\n",
      "Train Epoch: 57 [1200/8040 (15%)]\tLoss: 20.433179\n",
      "Train Epoch: 57 [2400/8040 (30%)]\tLoss: 21.127051\n",
      "Train Epoch: 57 [3600/8040 (45%)]\tLoss: 19.268343\n",
      "Train Epoch: 57 [4800/8040 (60%)]\tLoss: 20.760317\n",
      "Train Epoch: 57 [6000/8040 (75%)]\tLoss: 21.083339\n",
      "Train Epoch: 57 [7200/8040 (90%)]\tLoss: 19.381677\n",
      "====> Epoch: 57 Average loss: 20.0846\n",
      "epoch: 57, beta=0, frac_anom=0.05\n",
      "Train Epoch: 58 [0/8040 (0%)]\tLoss: 19.590275\n",
      "Train Epoch: 58 [1200/8040 (15%)]\tLoss: 20.169763\n",
      "Train Epoch: 58 [2400/8040 (30%)]\tLoss: 20.667521\n",
      "Train Epoch: 58 [3600/8040 (45%)]\tLoss: 20.625464\n",
      "Train Epoch: 58 [4800/8040 (60%)]\tLoss: 20.505695\n",
      "Train Epoch: 58 [6000/8040 (75%)]\tLoss: 20.338741\n",
      "Train Epoch: 58 [7200/8040 (90%)]\tLoss: 20.089172\n",
      "====> Epoch: 58 Average loss: 20.1540\n",
      "epoch: 58, beta=0, frac_anom=0.05\n",
      "Train Epoch: 59 [0/8040 (0%)]\tLoss: 20.444633\n",
      "Train Epoch: 59 [1200/8040 (15%)]\tLoss: 19.302625\n",
      "Train Epoch: 59 [2400/8040 (30%)]\tLoss: 19.938413\n",
      "Train Epoch: 59 [3600/8040 (45%)]\tLoss: 20.937150\n",
      "Train Epoch: 59 [4800/8040 (60%)]\tLoss: 20.000909\n",
      "Train Epoch: 59 [6000/8040 (75%)]\tLoss: 19.486479\n",
      "Train Epoch: 59 [7200/8040 (90%)]\tLoss: 18.920679\n",
      "====> Epoch: 59 Average loss: 20.1057\n",
      "epoch: 59, beta=0, frac_anom=0.05\n",
      "Train Epoch: 60 [0/8040 (0%)]\tLoss: 19.838639\n",
      "Train Epoch: 60 [1200/8040 (15%)]\tLoss: 20.103705\n",
      "Train Epoch: 60 [2400/8040 (30%)]\tLoss: 19.738623\n",
      "Train Epoch: 60 [3600/8040 (45%)]\tLoss: 20.645984\n",
      "Train Epoch: 60 [4800/8040 (60%)]\tLoss: 20.260219\n",
      "Train Epoch: 60 [6000/8040 (75%)]\tLoss: 19.329708\n",
      "Train Epoch: 60 [7200/8040 (90%)]\tLoss: 19.818323\n",
      "====> Epoch: 60 Average loss: 20.0783\n",
      "epoch: 60, beta=0, frac_anom=0.05\n",
      "Train Epoch: 61 [0/8040 (0%)]\tLoss: 19.574270\n",
      "Train Epoch: 61 [1200/8040 (15%)]\tLoss: 20.300185\n",
      "Train Epoch: 61 [2400/8040 (30%)]\tLoss: 21.644877\n",
      "Train Epoch: 61 [3600/8040 (45%)]\tLoss: 20.275488\n",
      "Train Epoch: 61 [4800/8040 (60%)]\tLoss: 19.598024\n",
      "Train Epoch: 61 [6000/8040 (75%)]\tLoss: 19.850675\n",
      "Train Epoch: 61 [7200/8040 (90%)]\tLoss: 19.872670\n",
      "====> Epoch: 61 Average loss: 20.0919\n",
      "epoch: 61, beta=0, frac_anom=0.05\n",
      "Train Epoch: 62 [0/8040 (0%)]\tLoss: 19.522333\n",
      "Train Epoch: 62 [1200/8040 (15%)]\tLoss: 20.153741\n",
      "Train Epoch: 62 [2400/8040 (30%)]\tLoss: 20.088550\n",
      "Train Epoch: 62 [3600/8040 (45%)]\tLoss: 19.916980\n",
      "Train Epoch: 62 [4800/8040 (60%)]\tLoss: 19.181419\n",
      "Train Epoch: 62 [6000/8040 (75%)]\tLoss: 20.401465\n",
      "Train Epoch: 62 [7200/8040 (90%)]\tLoss: 21.268542\n",
      "====> Epoch: 62 Average loss: 20.0004\n",
      "epoch: 62, beta=0, frac_anom=0.05\n",
      "Train Epoch: 63 [0/8040 (0%)]\tLoss: 20.180709\n",
      "Train Epoch: 63 [1200/8040 (15%)]\tLoss: 20.113737\n",
      "Train Epoch: 63 [2400/8040 (30%)]\tLoss: 19.596480\n",
      "Train Epoch: 63 [3600/8040 (45%)]\tLoss: 19.211839\n",
      "Train Epoch: 63 [4800/8040 (60%)]\tLoss: 19.580371\n",
      "Train Epoch: 63 [6000/8040 (75%)]\tLoss: 20.846267\n",
      "Train Epoch: 63 [7200/8040 (90%)]\tLoss: 19.367851\n",
      "====> Epoch: 63 Average loss: 20.0316\n",
      "epoch: 63, beta=0, frac_anom=0.05\n",
      "Train Epoch: 64 [0/8040 (0%)]\tLoss: 20.750488\n",
      "Train Epoch: 64 [1200/8040 (15%)]\tLoss: 19.432660\n",
      "Train Epoch: 64 [2400/8040 (30%)]\tLoss: 20.330632\n",
      "Train Epoch: 64 [3600/8040 (45%)]\tLoss: 19.192784\n",
      "Train Epoch: 64 [4800/8040 (60%)]\tLoss: 20.320504\n",
      "Train Epoch: 64 [6000/8040 (75%)]\tLoss: 19.119366\n",
      "Train Epoch: 64 [7200/8040 (90%)]\tLoss: 19.969869\n",
      "====> Epoch: 64 Average loss: 20.0040\n",
      "epoch: 64, beta=0, frac_anom=0.05\n",
      "Train Epoch: 65 [0/8040 (0%)]\tLoss: 20.423792\n",
      "Train Epoch: 65 [1200/8040 (15%)]\tLoss: 20.118632\n",
      "Train Epoch: 65 [2400/8040 (30%)]\tLoss: 20.410010\n",
      "Train Epoch: 65 [3600/8040 (45%)]\tLoss: 19.446667\n",
      "Train Epoch: 65 [4800/8040 (60%)]\tLoss: 21.261072\n",
      "Train Epoch: 65 [6000/8040 (75%)]\tLoss: 20.306600\n",
      "Train Epoch: 65 [7200/8040 (90%)]\tLoss: 19.857306\n",
      "====> Epoch: 65 Average loss: 20.0257\n",
      "epoch: 65, beta=0, frac_anom=0.05\n",
      "Train Epoch: 66 [0/8040 (0%)]\tLoss: 20.374357\n",
      "Train Epoch: 66 [1200/8040 (15%)]\tLoss: 18.960478\n",
      "Train Epoch: 66 [2400/8040 (30%)]\tLoss: 19.377454\n",
      "Train Epoch: 66 [3600/8040 (45%)]\tLoss: 19.943070\n",
      "Train Epoch: 66 [4800/8040 (60%)]\tLoss: 19.562634\n",
      "Train Epoch: 66 [6000/8040 (75%)]\tLoss: 18.812748\n",
      "Train Epoch: 66 [7200/8040 (90%)]\tLoss: 19.891797\n",
      "====> Epoch: 66 Average loss: 19.9682\n",
      "epoch: 66, beta=0, frac_anom=0.05\n",
      "Train Epoch: 67 [0/8040 (0%)]\tLoss: 21.391986\n",
      "Train Epoch: 67 [1200/8040 (15%)]\tLoss: 19.629757\n",
      "Train Epoch: 67 [2400/8040 (30%)]\tLoss: 19.717480\n",
      "Train Epoch: 67 [3600/8040 (45%)]\tLoss: 19.436365\n",
      "Train Epoch: 67 [4800/8040 (60%)]\tLoss: 19.144861\n",
      "Train Epoch: 67 [6000/8040 (75%)]\tLoss: 20.968660\n",
      "Train Epoch: 67 [7200/8040 (90%)]\tLoss: 22.096665\n",
      "====> Epoch: 67 Average loss: 20.0052\n",
      "epoch: 67, beta=0, frac_anom=0.05\n",
      "Train Epoch: 68 [0/8040 (0%)]\tLoss: 18.331626\n",
      "Train Epoch: 68 [1200/8040 (15%)]\tLoss: 19.653904\n",
      "Train Epoch: 68 [2400/8040 (30%)]\tLoss: 20.530709\n",
      "Train Epoch: 68 [3600/8040 (45%)]\tLoss: 19.922843\n",
      "Train Epoch: 68 [4800/8040 (60%)]\tLoss: 19.253815\n",
      "Train Epoch: 68 [6000/8040 (75%)]\tLoss: 21.401278\n",
      "Train Epoch: 68 [7200/8040 (90%)]\tLoss: 20.139712\n",
      "====> Epoch: 68 Average loss: 19.9559\n",
      "epoch: 68, beta=0, frac_anom=0.05\n",
      "Train Epoch: 69 [0/8040 (0%)]\tLoss: 19.943990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 69 [1200/8040 (15%)]\tLoss: 20.001839\n",
      "Train Epoch: 69 [2400/8040 (30%)]\tLoss: 19.422101\n",
      "Train Epoch: 69 [3600/8040 (45%)]\tLoss: 19.590727\n",
      "Train Epoch: 69 [4800/8040 (60%)]\tLoss: 20.433667\n",
      "Train Epoch: 69 [6000/8040 (75%)]\tLoss: 20.683012\n",
      "Train Epoch: 69 [7200/8040 (90%)]\tLoss: 20.564583\n",
      "====> Epoch: 69 Average loss: 19.9221\n",
      "epoch: 69, beta=0, frac_anom=0.05\n",
      "Train Epoch: 70 [0/8040 (0%)]\tLoss: 20.457245\n",
      "Train Epoch: 70 [1200/8040 (15%)]\tLoss: 19.674898\n",
      "Train Epoch: 70 [2400/8040 (30%)]\tLoss: 20.682945\n",
      "Train Epoch: 70 [3600/8040 (45%)]\tLoss: 19.722575\n",
      "Train Epoch: 70 [4800/8040 (60%)]\tLoss: 18.927708\n",
      "Train Epoch: 70 [6000/8040 (75%)]\tLoss: 19.852563\n",
      "Train Epoch: 70 [7200/8040 (90%)]\tLoss: 18.592092\n",
      "====> Epoch: 70 Average loss: 19.9043\n",
      "epoch: 70, beta=0, frac_anom=0.05\n",
      "Train Epoch: 71 [0/8040 (0%)]\tLoss: 19.151078\n",
      "Train Epoch: 71 [1200/8040 (15%)]\tLoss: 19.916423\n",
      "Train Epoch: 71 [2400/8040 (30%)]\tLoss: 19.331401\n",
      "Train Epoch: 71 [3600/8040 (45%)]\tLoss: 20.166943\n",
      "Train Epoch: 71 [4800/8040 (60%)]\tLoss: 20.916813\n",
      "Train Epoch: 71 [6000/8040 (75%)]\tLoss: 19.735400\n",
      "Train Epoch: 71 [7200/8040 (90%)]\tLoss: 20.617190\n",
      "====> Epoch: 71 Average loss: 19.9687\n",
      "epoch: 71, beta=0, frac_anom=0.05\n",
      "Train Epoch: 72 [0/8040 (0%)]\tLoss: 18.587793\n",
      "Train Epoch: 72 [1200/8040 (15%)]\tLoss: 19.165814\n",
      "Train Epoch: 72 [2400/8040 (30%)]\tLoss: 18.750444\n",
      "Train Epoch: 72 [3600/8040 (45%)]\tLoss: 20.756624\n",
      "Train Epoch: 72 [4800/8040 (60%)]\tLoss: 19.911967\n",
      "Train Epoch: 72 [6000/8040 (75%)]\tLoss: 19.336308\n",
      "Train Epoch: 72 [7200/8040 (90%)]\tLoss: 20.376619\n",
      "====> Epoch: 72 Average loss: 19.8913\n",
      "epoch: 72, beta=0, frac_anom=0.05\n",
      "Train Epoch: 73 [0/8040 (0%)]\tLoss: 19.610667\n",
      "Train Epoch: 73 [1200/8040 (15%)]\tLoss: 20.678202\n",
      "Train Epoch: 73 [2400/8040 (30%)]\tLoss: 20.250525\n",
      "Train Epoch: 73 [3600/8040 (45%)]\tLoss: 19.793502\n",
      "Train Epoch: 73 [4800/8040 (60%)]\tLoss: 19.579856\n",
      "Train Epoch: 73 [6000/8040 (75%)]\tLoss: 19.876373\n",
      "Train Epoch: 73 [7200/8040 (90%)]\tLoss: 19.565149\n",
      "====> Epoch: 73 Average loss: 19.9170\n",
      "epoch: 73, beta=0, frac_anom=0.05\n",
      "Train Epoch: 74 [0/8040 (0%)]\tLoss: 19.683055\n",
      "Train Epoch: 74 [1200/8040 (15%)]\tLoss: 21.233813\n",
      "Train Epoch: 74 [2400/8040 (30%)]\tLoss: 20.192173\n",
      "Train Epoch: 74 [3600/8040 (45%)]\tLoss: 20.913900\n",
      "Train Epoch: 74 [4800/8040 (60%)]\tLoss: 19.438517\n",
      "Train Epoch: 74 [6000/8040 (75%)]\tLoss: 18.819210\n",
      "Train Epoch: 74 [7200/8040 (90%)]\tLoss: 19.415763\n",
      "====> Epoch: 74 Average loss: 19.8932\n",
      "epoch: 74, beta=0, frac_anom=0.05\n",
      "Train Epoch: 75 [0/8040 (0%)]\tLoss: 20.092509\n",
      "Train Epoch: 75 [1200/8040 (15%)]\tLoss: 19.227281\n",
      "Train Epoch: 75 [2400/8040 (30%)]\tLoss: 20.580314\n",
      "Train Epoch: 75 [3600/8040 (45%)]\tLoss: 19.549188\n",
      "Train Epoch: 75 [4800/8040 (60%)]\tLoss: 20.030644\n",
      "Train Epoch: 75 [6000/8040 (75%)]\tLoss: 19.774223\n",
      "Train Epoch: 75 [7200/8040 (90%)]\tLoss: 20.700161\n",
      "====> Epoch: 75 Average loss: 19.8797\n",
      "epoch: 75, beta=0, frac_anom=0.05\n",
      "Train Epoch: 76 [0/8040 (0%)]\tLoss: 19.245251\n",
      "Train Epoch: 76 [1200/8040 (15%)]\tLoss: 18.903951\n",
      "Train Epoch: 76 [2400/8040 (30%)]\tLoss: 20.076898\n",
      "Train Epoch: 76 [3600/8040 (45%)]\tLoss: 19.999011\n",
      "Train Epoch: 76 [4800/8040 (60%)]\tLoss: 19.458016\n",
      "Train Epoch: 76 [6000/8040 (75%)]\tLoss: 18.411918\n",
      "Train Epoch: 76 [7200/8040 (90%)]\tLoss: 20.469588\n",
      "====> Epoch: 76 Average loss: 19.8230\n",
      "epoch: 76, beta=0, frac_anom=0.05\n",
      "Train Epoch: 77 [0/8040 (0%)]\tLoss: 18.322986\n",
      "Train Epoch: 77 [1200/8040 (15%)]\tLoss: 20.110811\n",
      "Train Epoch: 77 [2400/8040 (30%)]\tLoss: 19.732328\n",
      "Train Epoch: 77 [3600/8040 (45%)]\tLoss: 19.746981\n",
      "Train Epoch: 77 [4800/8040 (60%)]\tLoss: 19.865165\n",
      "Train Epoch: 77 [6000/8040 (75%)]\tLoss: 20.529885\n",
      "Train Epoch: 77 [7200/8040 (90%)]\tLoss: 19.882353\n",
      "====> Epoch: 77 Average loss: 19.8125\n",
      "epoch: 77, beta=0, frac_anom=0.05\n",
      "Train Epoch: 78 [0/8040 (0%)]\tLoss: 19.212724\n",
      "Train Epoch: 78 [1200/8040 (15%)]\tLoss: 19.532707\n",
      "Train Epoch: 78 [2400/8040 (30%)]\tLoss: 18.993854\n",
      "Train Epoch: 78 [3600/8040 (45%)]\tLoss: 19.309401\n",
      "Train Epoch: 78 [4800/8040 (60%)]\tLoss: 19.828166\n",
      "Train Epoch: 78 [6000/8040 (75%)]\tLoss: 20.535974\n",
      "Train Epoch: 78 [7200/8040 (90%)]\tLoss: 19.157680\n",
      "====> Epoch: 78 Average loss: 19.8176\n",
      "epoch: 78, beta=0, frac_anom=0.05\n",
      "Train Epoch: 79 [0/8040 (0%)]\tLoss: 19.528625\n",
      "Train Epoch: 79 [1200/8040 (15%)]\tLoss: 19.692000\n",
      "Train Epoch: 79 [2400/8040 (30%)]\tLoss: 19.167515\n",
      "Train Epoch: 79 [3600/8040 (45%)]\tLoss: 19.202917\n",
      "Train Epoch: 79 [4800/8040 (60%)]\tLoss: 19.777745\n",
      "Train Epoch: 79 [6000/8040 (75%)]\tLoss: 20.658415\n",
      "Train Epoch: 79 [7200/8040 (90%)]\tLoss: 18.902083\n",
      "====> Epoch: 79 Average loss: 19.8708\n",
      "epoch: 79, beta=0, frac_anom=0.05\n",
      "Train Epoch: 80 [0/8040 (0%)]\tLoss: 19.072054\n",
      "Train Epoch: 80 [1200/8040 (15%)]\tLoss: 19.820404\n",
      "Train Epoch: 80 [2400/8040 (30%)]\tLoss: 19.489181\n",
      "Train Epoch: 80 [3600/8040 (45%)]\tLoss: 19.709444\n",
      "Train Epoch: 80 [4800/8040 (60%)]\tLoss: 18.517269\n",
      "Train Epoch: 80 [6000/8040 (75%)]\tLoss: 19.701976\n",
      "Train Epoch: 80 [7200/8040 (90%)]\tLoss: 19.130469\n",
      "====> Epoch: 80 Average loss: 19.7937\n",
      "epoch: 80, beta=0, frac_anom=0.05\n",
      "Train Epoch: 81 [0/8040 (0%)]\tLoss: 19.593642\n",
      "Train Epoch: 81 [1200/8040 (15%)]\tLoss: 18.426371\n",
      "Train Epoch: 81 [2400/8040 (30%)]\tLoss: 20.430141\n",
      "Train Epoch: 81 [3600/8040 (45%)]\tLoss: 19.480202\n",
      "Train Epoch: 81 [4800/8040 (60%)]\tLoss: 19.456510\n",
      "Train Epoch: 81 [6000/8040 (75%)]\tLoss: 20.103695\n",
      "Train Epoch: 81 [7200/8040 (90%)]\tLoss: 19.180827\n",
      "====> Epoch: 81 Average loss: 19.7624\n",
      "epoch: 81, beta=0, frac_anom=0.05\n",
      "Train Epoch: 82 [0/8040 (0%)]\tLoss: 20.801172\n",
      "Train Epoch: 82 [1200/8040 (15%)]\tLoss: 19.799068\n",
      "Train Epoch: 82 [2400/8040 (30%)]\tLoss: 20.330391\n",
      "Train Epoch: 82 [3600/8040 (45%)]\tLoss: 20.589663\n",
      "Train Epoch: 82 [4800/8040 (60%)]\tLoss: 18.571309\n",
      "Train Epoch: 82 [6000/8040 (75%)]\tLoss: 19.927342\n",
      "Train Epoch: 82 [7200/8040 (90%)]\tLoss: 20.574078\n",
      "====> Epoch: 82 Average loss: 19.7311\n",
      "epoch: 82, beta=0, frac_anom=0.05\n",
      "Train Epoch: 83 [0/8040 (0%)]\tLoss: 20.129940\n",
      "Train Epoch: 83 [1200/8040 (15%)]\tLoss: 19.166471\n",
      "Train Epoch: 83 [2400/8040 (30%)]\tLoss: 18.911995\n",
      "Train Epoch: 83 [3600/8040 (45%)]\tLoss: 21.114919\n",
      "Train Epoch: 83 [4800/8040 (60%)]\tLoss: 20.257458\n",
      "Train Epoch: 83 [6000/8040 (75%)]\tLoss: 19.277649\n",
      "Train Epoch: 83 [7200/8040 (90%)]\tLoss: 20.266848\n",
      "====> Epoch: 83 Average loss: 19.7885\n",
      "epoch: 83, beta=0, frac_anom=0.05\n",
      "Train Epoch: 84 [0/8040 (0%)]\tLoss: 20.315739\n",
      "Train Epoch: 84 [1200/8040 (15%)]\tLoss: 19.914937\n",
      "Train Epoch: 84 [2400/8040 (30%)]\tLoss: 21.018315\n",
      "Train Epoch: 84 [3600/8040 (45%)]\tLoss: 19.585954\n",
      "Train Epoch: 84 [4800/8040 (60%)]\tLoss: 20.545957\n",
      "Train Epoch: 84 [6000/8040 (75%)]\tLoss: 19.444509\n",
      "Train Epoch: 84 [7200/8040 (90%)]\tLoss: 19.447445\n",
      "====> Epoch: 84 Average loss: 19.7793\n",
      "epoch: 84, beta=0, frac_anom=0.05\n",
      "Train Epoch: 85 [0/8040 (0%)]\tLoss: 18.747270\n",
      "Train Epoch: 85 [1200/8040 (15%)]\tLoss: 19.283565\n",
      "Train Epoch: 85 [2400/8040 (30%)]\tLoss: 20.123108\n",
      "Train Epoch: 85 [3600/8040 (45%)]\tLoss: 18.659192\n",
      "Train Epoch: 85 [4800/8040 (60%)]\tLoss: 19.820125\n",
      "Train Epoch: 85 [6000/8040 (75%)]\tLoss: 20.056986\n",
      "Train Epoch: 85 [7200/8040 (90%)]\tLoss: 20.867303\n",
      "====> Epoch: 85 Average loss: 19.7758\n",
      "epoch: 85, beta=0, frac_anom=0.05\n",
      "Train Epoch: 86 [0/8040 (0%)]\tLoss: 20.973633\n",
      "Train Epoch: 86 [1200/8040 (15%)]\tLoss: 20.099609\n",
      "Train Epoch: 86 [2400/8040 (30%)]\tLoss: 18.647727\n",
      "Train Epoch: 86 [3600/8040 (45%)]\tLoss: 19.805912\n",
      "Train Epoch: 86 [4800/8040 (60%)]\tLoss: 19.816077\n",
      "Train Epoch: 86 [6000/8040 (75%)]\tLoss: 19.859859\n",
      "Train Epoch: 86 [7200/8040 (90%)]\tLoss: 20.052905\n",
      "====> Epoch: 86 Average loss: 19.7555\n",
      "epoch: 86, beta=0, frac_anom=0.05\n",
      "Train Epoch: 87 [0/8040 (0%)]\tLoss: 19.555825\n",
      "Train Epoch: 87 [1200/8040 (15%)]\tLoss: 19.393034\n",
      "Train Epoch: 87 [2400/8040 (30%)]\tLoss: 20.889549\n",
      "Train Epoch: 87 [3600/8040 (45%)]\tLoss: 20.239954\n",
      "Train Epoch: 87 [4800/8040 (60%)]\tLoss: 18.387569\n",
      "Train Epoch: 87 [6000/8040 (75%)]\tLoss: 19.114317\n",
      "Train Epoch: 87 [7200/8040 (90%)]\tLoss: 19.797034\n",
      "====> Epoch: 87 Average loss: 19.7298\n",
      "epoch: 87, beta=0, frac_anom=0.05\n",
      "Train Epoch: 88 [0/8040 (0%)]\tLoss: 18.990601\n",
      "Train Epoch: 88 [1200/8040 (15%)]\tLoss: 18.915041\n",
      "Train Epoch: 88 [2400/8040 (30%)]\tLoss: 19.864299\n",
      "Train Epoch: 88 [3600/8040 (45%)]\tLoss: 19.993962\n",
      "Train Epoch: 88 [4800/8040 (60%)]\tLoss: 18.617920\n",
      "Train Epoch: 88 [6000/8040 (75%)]\tLoss: 18.563888\n",
      "Train Epoch: 88 [7200/8040 (90%)]\tLoss: 20.172099\n",
      "====> Epoch: 88 Average loss: 19.7589\n",
      "epoch: 88, beta=0, frac_anom=0.05\n",
      "Train Epoch: 89 [0/8040 (0%)]\tLoss: 18.280221\n",
      "Train Epoch: 89 [1200/8040 (15%)]\tLoss: 18.920227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 89 [2400/8040 (30%)]\tLoss: 19.913735\n",
      "Train Epoch: 89 [3600/8040 (45%)]\tLoss: 19.661348\n",
      "Train Epoch: 89 [4800/8040 (60%)]\tLoss: 19.665365\n",
      "Train Epoch: 89 [6000/8040 (75%)]\tLoss: 20.776078\n",
      "Train Epoch: 89 [7200/8040 (90%)]\tLoss: 18.094216\n",
      "====> Epoch: 89 Average loss: 19.7530\n",
      "epoch: 89, beta=0, frac_anom=0.05\n",
      "Train Epoch: 90 [0/8040 (0%)]\tLoss: 18.662720\n",
      "Train Epoch: 90 [1200/8040 (15%)]\tLoss: 19.328739\n",
      "Train Epoch: 90 [2400/8040 (30%)]\tLoss: 20.450407\n",
      "Train Epoch: 90 [3600/8040 (45%)]\tLoss: 19.911629\n",
      "Train Epoch: 90 [4800/8040 (60%)]\tLoss: 19.480927\n",
      "Train Epoch: 90 [6000/8040 (75%)]\tLoss: 19.079897\n",
      "Train Epoch: 90 [7200/8040 (90%)]\tLoss: 19.195837\n",
      "====> Epoch: 90 Average loss: 19.7157\n",
      "epoch: 90, beta=0, frac_anom=0.05\n",
      "Train Epoch: 91 [0/8040 (0%)]\tLoss: 20.389136\n",
      "Train Epoch: 91 [1200/8040 (15%)]\tLoss: 19.274662\n",
      "Train Epoch: 91 [2400/8040 (30%)]\tLoss: 20.014380\n",
      "Train Epoch: 91 [3600/8040 (45%)]\tLoss: 19.547015\n",
      "Train Epoch: 91 [4800/8040 (60%)]\tLoss: 19.004647\n",
      "Train Epoch: 91 [6000/8040 (75%)]\tLoss: 21.017147\n",
      "Train Epoch: 91 [7200/8040 (90%)]\tLoss: 19.099984\n",
      "====> Epoch: 91 Average loss: 19.6902\n",
      "epoch: 91, beta=0, frac_anom=0.05\n",
      "Train Epoch: 92 [0/8040 (0%)]\tLoss: 19.360933\n",
      "Train Epoch: 92 [1200/8040 (15%)]\tLoss: 19.759957\n",
      "Train Epoch: 92 [2400/8040 (30%)]\tLoss: 20.433299\n",
      "Train Epoch: 92 [3600/8040 (45%)]\tLoss: 19.079403\n",
      "Train Epoch: 92 [4800/8040 (60%)]\tLoss: 20.781812\n",
      "Train Epoch: 92 [6000/8040 (75%)]\tLoss: 18.980452\n",
      "Train Epoch: 92 [7200/8040 (90%)]\tLoss: 19.627226\n",
      "====> Epoch: 92 Average loss: 19.6966\n",
      "epoch: 92, beta=0, frac_anom=0.05\n",
      "Train Epoch: 93 [0/8040 (0%)]\tLoss: 19.634119\n",
      "Train Epoch: 93 [1200/8040 (15%)]\tLoss: 20.160449\n",
      "Train Epoch: 93 [2400/8040 (30%)]\tLoss: 19.972302\n",
      "Train Epoch: 93 [3600/8040 (45%)]\tLoss: 19.643073\n",
      "Train Epoch: 93 [4800/8040 (60%)]\tLoss: 19.148621\n",
      "Train Epoch: 93 [6000/8040 (75%)]\tLoss: 17.416282\n",
      "Train Epoch: 93 [7200/8040 (90%)]\tLoss: 19.577873\n",
      "====> Epoch: 93 Average loss: 19.6563\n",
      "epoch: 93, beta=0, frac_anom=0.05\n",
      "Train Epoch: 94 [0/8040 (0%)]\tLoss: 18.923997\n",
      "Train Epoch: 94 [1200/8040 (15%)]\tLoss: 19.045980\n",
      "Train Epoch: 94 [2400/8040 (30%)]\tLoss: 19.700997\n",
      "Train Epoch: 94 [3600/8040 (45%)]\tLoss: 18.519316\n",
      "Train Epoch: 94 [4800/8040 (60%)]\tLoss: 18.555573\n",
      "Train Epoch: 94 [6000/8040 (75%)]\tLoss: 18.402275\n",
      "Train Epoch: 94 [7200/8040 (90%)]\tLoss: 20.120396\n",
      "====> Epoch: 94 Average loss: 19.6264\n",
      "epoch: 94, beta=0, frac_anom=0.05\n",
      "Train Epoch: 95 [0/8040 (0%)]\tLoss: 19.814056\n",
      "Train Epoch: 95 [1200/8040 (15%)]\tLoss: 20.543327\n",
      "Train Epoch: 95 [2400/8040 (30%)]\tLoss: 19.074080\n",
      "Train Epoch: 95 [3600/8040 (45%)]\tLoss: 19.795752\n",
      "Train Epoch: 95 [4800/8040 (60%)]\tLoss: 19.424483\n",
      "Train Epoch: 95 [6000/8040 (75%)]\tLoss: 19.405625\n",
      "Train Epoch: 95 [7200/8040 (90%)]\tLoss: 19.675802\n",
      "====> Epoch: 95 Average loss: 19.6742\n",
      "epoch: 95, beta=0, frac_anom=0.05\n",
      "Train Epoch: 96 [0/8040 (0%)]\tLoss: 19.673877\n",
      "Train Epoch: 96 [1200/8040 (15%)]\tLoss: 19.891703\n",
      "Train Epoch: 96 [2400/8040 (30%)]\tLoss: 19.605273\n",
      "Train Epoch: 96 [3600/8040 (45%)]\tLoss: 19.469539\n",
      "Train Epoch: 96 [4800/8040 (60%)]\tLoss: 18.575142\n",
      "Train Epoch: 96 [6000/8040 (75%)]\tLoss: 19.308754\n",
      "Train Epoch: 96 [7200/8040 (90%)]\tLoss: 19.935724\n",
      "====> Epoch: 96 Average loss: 19.6792\n",
      "epoch: 96, beta=0, frac_anom=0.05\n",
      "Train Epoch: 97 [0/8040 (0%)]\tLoss: 19.216199\n",
      "Train Epoch: 97 [1200/8040 (15%)]\tLoss: 19.612488\n",
      "Train Epoch: 97 [2400/8040 (30%)]\tLoss: 18.775012\n",
      "Train Epoch: 97 [3600/8040 (45%)]\tLoss: 19.068180\n",
      "Train Epoch: 97 [4800/8040 (60%)]\tLoss: 18.643724\n",
      "Train Epoch: 97 [6000/8040 (75%)]\tLoss: 18.506315\n",
      "Train Epoch: 97 [7200/8040 (90%)]\tLoss: 19.403345\n",
      "====> Epoch: 97 Average loss: 19.6483\n",
      "epoch: 97, beta=0, frac_anom=0.05\n",
      "Train Epoch: 98 [0/8040 (0%)]\tLoss: 20.143689\n",
      "Train Epoch: 98 [1200/8040 (15%)]\tLoss: 19.461452\n",
      "Train Epoch: 98 [2400/8040 (30%)]\tLoss: 19.194071\n",
      "Train Epoch: 98 [3600/8040 (45%)]\tLoss: 20.096765\n",
      "Train Epoch: 98 [4800/8040 (60%)]\tLoss: 20.122095\n",
      "Train Epoch: 98 [6000/8040 (75%)]\tLoss: 20.390474\n",
      "Train Epoch: 98 [7200/8040 (90%)]\tLoss: 19.009835\n",
      "====> Epoch: 98 Average loss: 19.6713\n",
      "epoch: 98, beta=0, frac_anom=0.05\n",
      "Train Epoch: 99 [0/8040 (0%)]\tLoss: 19.591207\n",
      "Train Epoch: 99 [1200/8040 (15%)]\tLoss: 19.336383\n",
      "Train Epoch: 99 [2400/8040 (30%)]\tLoss: 19.432438\n",
      "Train Epoch: 99 [3600/8040 (45%)]\tLoss: 19.248403\n",
      "Train Epoch: 99 [4800/8040 (60%)]\tLoss: 20.256978\n",
      "Train Epoch: 99 [6000/8040 (75%)]\tLoss: 19.425647\n",
      "Train Epoch: 99 [7200/8040 (90%)]\tLoss: 19.381630\n",
      "====> Epoch: 99 Average loss: 19.6587\n",
      "epoch: 99, beta=0, frac_anom=0.05\n",
      "Train Epoch: 100 [0/8040 (0%)]\tLoss: 19.104598\n",
      "Train Epoch: 100 [1200/8040 (15%)]\tLoss: 19.177563\n",
      "Train Epoch: 100 [2400/8040 (30%)]\tLoss: 18.117572\n",
      "Train Epoch: 100 [3600/8040 (45%)]\tLoss: 20.310738\n",
      "Train Epoch: 100 [4800/8040 (60%)]\tLoss: 20.311517\n",
      "Train Epoch: 100 [6000/8040 (75%)]\tLoss: 19.269023\n",
      "Train Epoch: 100 [7200/8040 (90%)]\tLoss: 20.269067\n",
      "====> Epoch: 100 Average loss: 19.6372\n",
      "epoch: 100, beta=0, frac_anom=0.05\n",
      "Train Epoch: 101 [0/8040 (0%)]\tLoss: 20.941897\n",
      "Train Epoch: 101 [1200/8040 (15%)]\tLoss: 20.011975\n",
      "Train Epoch: 101 [2400/8040 (30%)]\tLoss: 19.692346\n",
      "Train Epoch: 101 [3600/8040 (45%)]\tLoss: 20.384475\n",
      "Train Epoch: 101 [4800/8040 (60%)]\tLoss: 19.836206\n",
      "Train Epoch: 101 [6000/8040 (75%)]\tLoss: 18.183976\n",
      "Train Epoch: 101 [7200/8040 (90%)]\tLoss: 19.556978\n",
      "====> Epoch: 101 Average loss: 19.6086\n",
      "epoch: 101, beta=0, frac_anom=0.05\n",
      "Train Epoch: 102 [0/8040 (0%)]\tLoss: 18.582609\n",
      "Train Epoch: 102 [1200/8040 (15%)]\tLoss: 19.819922\n",
      "Train Epoch: 102 [2400/8040 (30%)]\tLoss: 20.269961\n",
      "Train Epoch: 102 [3600/8040 (45%)]\tLoss: 20.040733\n",
      "Train Epoch: 102 [4800/8040 (60%)]\tLoss: 18.934379\n",
      "Train Epoch: 102 [6000/8040 (75%)]\tLoss: 18.928601\n",
      "Train Epoch: 102 [7200/8040 (90%)]\tLoss: 19.694379\n",
      "====> Epoch: 102 Average loss: 19.5993\n",
      "epoch: 102, beta=0, frac_anom=0.05\n",
      "Train Epoch: 103 [0/8040 (0%)]\tLoss: 20.587549\n",
      "Train Epoch: 103 [1200/8040 (15%)]\tLoss: 19.338106\n",
      "Train Epoch: 103 [2400/8040 (30%)]\tLoss: 20.166180\n",
      "Train Epoch: 103 [3600/8040 (45%)]\tLoss: 20.287419\n",
      "Train Epoch: 103 [4800/8040 (60%)]\tLoss: 19.286568\n",
      "Train Epoch: 103 [6000/8040 (75%)]\tLoss: 18.307898\n",
      "Train Epoch: 103 [7200/8040 (90%)]\tLoss: 19.662467\n",
      "====> Epoch: 103 Average loss: 19.6042\n",
      "epoch: 103, beta=0, frac_anom=0.05\n",
      "Train Epoch: 104 [0/8040 (0%)]\tLoss: 20.352568\n",
      "Train Epoch: 104 [1200/8040 (15%)]\tLoss: 19.136782\n",
      "Train Epoch: 104 [2400/8040 (30%)]\tLoss: 19.404348\n",
      "Train Epoch: 104 [3600/8040 (45%)]\tLoss: 19.545559\n",
      "Train Epoch: 104 [4800/8040 (60%)]\tLoss: 19.881071\n",
      "Train Epoch: 104 [6000/8040 (75%)]\tLoss: 20.066528\n",
      "Train Epoch: 104 [7200/8040 (90%)]\tLoss: 20.033561\n",
      "====> Epoch: 104 Average loss: 19.5917\n",
      "epoch: 104, beta=0, frac_anom=0.05\n",
      "Train Epoch: 105 [0/8040 (0%)]\tLoss: 18.851013\n",
      "Train Epoch: 105 [1200/8040 (15%)]\tLoss: 19.165995\n",
      "Train Epoch: 105 [2400/8040 (30%)]\tLoss: 19.710510\n",
      "Train Epoch: 105 [3600/8040 (45%)]\tLoss: 19.334609\n",
      "Train Epoch: 105 [4800/8040 (60%)]\tLoss: 18.889250\n",
      "Train Epoch: 105 [6000/8040 (75%)]\tLoss: 19.769364\n",
      "Train Epoch: 105 [7200/8040 (90%)]\tLoss: 20.703825\n",
      "====> Epoch: 105 Average loss: 19.6386\n",
      "epoch: 105, beta=0, frac_anom=0.05\n",
      "Train Epoch: 106 [0/8040 (0%)]\tLoss: 19.989832\n",
      "Train Epoch: 106 [1200/8040 (15%)]\tLoss: 19.710350\n",
      "Train Epoch: 106 [2400/8040 (30%)]\tLoss: 20.294275\n",
      "Train Epoch: 106 [3600/8040 (45%)]\tLoss: 19.831659\n",
      "Train Epoch: 106 [4800/8040 (60%)]\tLoss: 19.878764\n",
      "Train Epoch: 106 [6000/8040 (75%)]\tLoss: 20.382404\n",
      "Train Epoch: 106 [7200/8040 (90%)]\tLoss: 19.456679\n",
      "====> Epoch: 106 Average loss: 19.6102\n",
      "epoch: 106, beta=0, frac_anom=0.05\n",
      "Train Epoch: 107 [0/8040 (0%)]\tLoss: 19.663143\n",
      "Train Epoch: 107 [1200/8040 (15%)]\tLoss: 19.249797\n",
      "Train Epoch: 107 [2400/8040 (30%)]\tLoss: 19.244499\n",
      "Train Epoch: 107 [3600/8040 (45%)]\tLoss: 19.484143\n",
      "Train Epoch: 107 [4800/8040 (60%)]\tLoss: 18.637939\n",
      "Train Epoch: 107 [6000/8040 (75%)]\tLoss: 20.456222\n",
      "Train Epoch: 107 [7200/8040 (90%)]\tLoss: 20.434017\n",
      "====> Epoch: 107 Average loss: 19.5581\n",
      "epoch: 107, beta=0, frac_anom=0.05\n",
      "Train Epoch: 108 [0/8040 (0%)]\tLoss: 19.265692\n",
      "Train Epoch: 108 [1200/8040 (15%)]\tLoss: 18.923110\n",
      "Train Epoch: 108 [2400/8040 (30%)]\tLoss: 18.979873\n",
      "Train Epoch: 108 [3600/8040 (45%)]\tLoss: 19.325411\n",
      "Train Epoch: 108 [4800/8040 (60%)]\tLoss: 20.320125\n",
      "Train Epoch: 108 [6000/8040 (75%)]\tLoss: 19.852521\n",
      "Train Epoch: 108 [7200/8040 (90%)]\tLoss: 20.079323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 108 Average loss: 19.5420\n",
      "epoch: 108, beta=0, frac_anom=0.05\n",
      "Train Epoch: 109 [0/8040 (0%)]\tLoss: 20.415084\n",
      "Train Epoch: 109 [1200/8040 (15%)]\tLoss: 19.807176\n",
      "Train Epoch: 109 [2400/8040 (30%)]\tLoss: 20.069456\n",
      "Train Epoch: 109 [3600/8040 (45%)]\tLoss: 18.865491\n",
      "Train Epoch: 109 [4800/8040 (60%)]\tLoss: 19.901123\n",
      "Train Epoch: 109 [6000/8040 (75%)]\tLoss: 19.162044\n",
      "Train Epoch: 109 [7200/8040 (90%)]\tLoss: 18.918524\n",
      "====> Epoch: 109 Average loss: 19.5919\n",
      "epoch: 109, beta=0, frac_anom=0.05\n",
      "Train Epoch: 110 [0/8040 (0%)]\tLoss: 18.780452\n",
      "Train Epoch: 110 [1200/8040 (15%)]\tLoss: 19.797502\n",
      "Train Epoch: 110 [2400/8040 (30%)]\tLoss: 19.758726\n",
      "Train Epoch: 110 [3600/8040 (45%)]\tLoss: 19.962016\n",
      "Train Epoch: 110 [4800/8040 (60%)]\tLoss: 19.564197\n",
      "Train Epoch: 110 [6000/8040 (75%)]\tLoss: 20.187573\n",
      "Train Epoch: 110 [7200/8040 (90%)]\tLoss: 19.741821\n",
      "====> Epoch: 110 Average loss: 19.5505\n",
      "epoch: 110, beta=0, frac_anom=0.05\n",
      "Train Epoch: 111 [0/8040 (0%)]\tLoss: 20.670703\n",
      "Train Epoch: 111 [1200/8040 (15%)]\tLoss: 18.500651\n",
      "Train Epoch: 111 [2400/8040 (30%)]\tLoss: 20.878505\n",
      "Train Epoch: 111 [3600/8040 (45%)]\tLoss: 18.427669\n",
      "Train Epoch: 111 [4800/8040 (60%)]\tLoss: 19.354924\n",
      "Train Epoch: 111 [6000/8040 (75%)]\tLoss: 19.407867\n",
      "Train Epoch: 111 [7200/8040 (90%)]\tLoss: 18.094979\n",
      "====> Epoch: 111 Average loss: 19.5165\n",
      "epoch: 111, beta=0, frac_anom=0.05\n",
      "Train Epoch: 112 [0/8040 (0%)]\tLoss: 20.092997\n",
      "Train Epoch: 112 [1200/8040 (15%)]\tLoss: 20.059648\n",
      "Train Epoch: 112 [2400/8040 (30%)]\tLoss: 19.632349\n",
      "Train Epoch: 112 [3600/8040 (45%)]\tLoss: 18.875472\n",
      "Train Epoch: 112 [4800/8040 (60%)]\tLoss: 20.171086\n",
      "Train Epoch: 112 [6000/8040 (75%)]\tLoss: 19.366862\n",
      "Train Epoch: 112 [7200/8040 (90%)]\tLoss: 18.189156\n",
      "====> Epoch: 112 Average loss: 19.5120\n",
      "epoch: 112, beta=0, frac_anom=0.05\n",
      "Train Epoch: 113 [0/8040 (0%)]\tLoss: 19.429258\n",
      "Train Epoch: 113 [1200/8040 (15%)]\tLoss: 19.698777\n",
      "Train Epoch: 113 [2400/8040 (30%)]\tLoss: 20.907542\n",
      "Train Epoch: 113 [3600/8040 (45%)]\tLoss: 18.896956\n",
      "Train Epoch: 113 [4800/8040 (60%)]\tLoss: 18.464138\n",
      "Train Epoch: 113 [6000/8040 (75%)]\tLoss: 19.996289\n",
      "Train Epoch: 113 [7200/8040 (90%)]\tLoss: 20.155819\n",
      "====> Epoch: 113 Average loss: 19.5510\n",
      "epoch: 113, beta=0, frac_anom=0.05\n",
      "Train Epoch: 114 [0/8040 (0%)]\tLoss: 18.729618\n",
      "Train Epoch: 114 [1200/8040 (15%)]\tLoss: 19.718803\n",
      "Train Epoch: 114 [2400/8040 (30%)]\tLoss: 20.611430\n",
      "Train Epoch: 114 [3600/8040 (45%)]\tLoss: 20.288971\n",
      "Train Epoch: 114 [4800/8040 (60%)]\tLoss: 20.278188\n",
      "Train Epoch: 114 [6000/8040 (75%)]\tLoss: 19.643158\n",
      "Train Epoch: 114 [7200/8040 (90%)]\tLoss: 19.224247\n",
      "====> Epoch: 114 Average loss: 19.5239\n",
      "epoch: 114, beta=0, frac_anom=0.05\n",
      "Train Epoch: 115 [0/8040 (0%)]\tLoss: 19.770522\n",
      "Train Epoch: 115 [1200/8040 (15%)]\tLoss: 19.248197\n",
      "Train Epoch: 115 [2400/8040 (30%)]\tLoss: 18.664445\n",
      "Train Epoch: 115 [3600/8040 (45%)]\tLoss: 19.398580\n",
      "Train Epoch: 115 [4800/8040 (60%)]\tLoss: 19.439952\n",
      "Train Epoch: 115 [6000/8040 (75%)]\tLoss: 20.052087\n",
      "Train Epoch: 115 [7200/8040 (90%)]\tLoss: 18.987872\n",
      "====> Epoch: 115 Average loss: 19.5638\n",
      "epoch: 115, beta=0, frac_anom=0.05\n",
      "Train Epoch: 116 [0/8040 (0%)]\tLoss: 19.144651\n",
      "Train Epoch: 116 [1200/8040 (15%)]\tLoss: 19.898271\n",
      "Train Epoch: 116 [2400/8040 (30%)]\tLoss: 19.272441\n",
      "Train Epoch: 116 [3600/8040 (45%)]\tLoss: 19.075981\n",
      "Train Epoch: 116 [4800/8040 (60%)]\tLoss: 18.521989\n",
      "Train Epoch: 116 [6000/8040 (75%)]\tLoss: 20.475513\n",
      "Train Epoch: 116 [7200/8040 (90%)]\tLoss: 19.426544\n",
      "====> Epoch: 116 Average loss: 19.5388\n",
      "epoch: 116, beta=0, frac_anom=0.05\n",
      "Train Epoch: 117 [0/8040 (0%)]\tLoss: 18.848848\n",
      "Train Epoch: 117 [1200/8040 (15%)]\tLoss: 19.237048\n",
      "Train Epoch: 117 [2400/8040 (30%)]\tLoss: 19.134334\n",
      "Train Epoch: 117 [3600/8040 (45%)]\tLoss: 19.381903\n",
      "Train Epoch: 117 [4800/8040 (60%)]\tLoss: 19.761660\n",
      "Train Epoch: 117 [6000/8040 (75%)]\tLoss: 18.865407\n",
      "Train Epoch: 117 [7200/8040 (90%)]\tLoss: 18.505239\n",
      "====> Epoch: 117 Average loss: 19.5422\n",
      "epoch: 117, beta=0, frac_anom=0.05\n",
      "Train Epoch: 118 [0/8040 (0%)]\tLoss: 18.568941\n",
      "Train Epoch: 118 [1200/8040 (15%)]\tLoss: 19.406677\n",
      "Train Epoch: 118 [2400/8040 (30%)]\tLoss: 19.739787\n",
      "Train Epoch: 118 [3600/8040 (45%)]\tLoss: 19.623161\n",
      "Train Epoch: 118 [4800/8040 (60%)]\tLoss: 19.976855\n",
      "Train Epoch: 118 [6000/8040 (75%)]\tLoss: 20.172620\n",
      "Train Epoch: 118 [7200/8040 (90%)]\tLoss: 19.691378\n",
      "====> Epoch: 118 Average loss: 19.5303\n",
      "epoch: 118, beta=0, frac_anom=0.05\n",
      "Train Epoch: 119 [0/8040 (0%)]\tLoss: 19.458655\n",
      "Train Epoch: 119 [1200/8040 (15%)]\tLoss: 19.389412\n",
      "Train Epoch: 119 [2400/8040 (30%)]\tLoss: 20.096753\n",
      "Train Epoch: 119 [3600/8040 (45%)]\tLoss: 19.575028\n",
      "Train Epoch: 119 [4800/8040 (60%)]\tLoss: 20.439400\n",
      "Train Epoch: 119 [6000/8040 (75%)]\tLoss: 19.108254\n",
      "Train Epoch: 119 [7200/8040 (90%)]\tLoss: 19.044051\n",
      "====> Epoch: 119 Average loss: 19.5099\n",
      "epoch: 119, beta=0, frac_anom=0.05\n",
      "Train Epoch: 120 [0/8040 (0%)]\tLoss: 18.986023\n",
      "Train Epoch: 120 [1200/8040 (15%)]\tLoss: 19.996708\n",
      "Train Epoch: 120 [2400/8040 (30%)]\tLoss: 19.349182\n",
      "Train Epoch: 120 [3600/8040 (45%)]\tLoss: 19.516862\n",
      "Train Epoch: 120 [4800/8040 (60%)]\tLoss: 19.250283\n",
      "Train Epoch: 120 [6000/8040 (75%)]\tLoss: 19.824874\n",
      "Train Epoch: 120 [7200/8040 (90%)]\tLoss: 19.670351\n",
      "====> Epoch: 120 Average loss: 19.5413\n",
      "epoch: 120, beta=0, frac_anom=0.05\n",
      "Train Epoch: 121 [0/8040 (0%)]\tLoss: 17.818532\n",
      "Train Epoch: 121 [1200/8040 (15%)]\tLoss: 18.969822\n",
      "Train Epoch: 121 [2400/8040 (30%)]\tLoss: 18.626955\n",
      "Train Epoch: 121 [3600/8040 (45%)]\tLoss: 20.019853\n",
      "Train Epoch: 121 [4800/8040 (60%)]\tLoss: 19.980514\n",
      "Train Epoch: 121 [6000/8040 (75%)]\tLoss: 18.370874\n",
      "Train Epoch: 121 [7200/8040 (90%)]\tLoss: 19.334987\n",
      "====> Epoch: 121 Average loss: 19.4834\n",
      "epoch: 121, beta=0, frac_anom=0.05\n",
      "Train Epoch: 122 [0/8040 (0%)]\tLoss: 19.480688\n",
      "Train Epoch: 122 [1200/8040 (15%)]\tLoss: 18.927541\n",
      "Train Epoch: 122 [2400/8040 (30%)]\tLoss: 19.250759\n",
      "Train Epoch: 122 [3600/8040 (45%)]\tLoss: 19.331720\n",
      "Train Epoch: 122 [4800/8040 (60%)]\tLoss: 19.771318\n",
      "Train Epoch: 122 [6000/8040 (75%)]\tLoss: 20.041215\n",
      "Train Epoch: 122 [7200/8040 (90%)]\tLoss: 19.821220\n",
      "====> Epoch: 122 Average loss: 19.4757\n",
      "epoch: 122, beta=0, frac_anom=0.05\n",
      "Train Epoch: 123 [0/8040 (0%)]\tLoss: 19.295976\n",
      "Train Epoch: 123 [1200/8040 (15%)]\tLoss: 18.313578\n",
      "Train Epoch: 123 [2400/8040 (30%)]\tLoss: 18.099038\n",
      "Train Epoch: 123 [3600/8040 (45%)]\tLoss: 20.162573\n",
      "Train Epoch: 123 [4800/8040 (60%)]\tLoss: 19.390507\n",
      "Train Epoch: 123 [6000/8040 (75%)]\tLoss: 18.453135\n",
      "Train Epoch: 123 [7200/8040 (90%)]\tLoss: 19.626807\n",
      "====> Epoch: 123 Average loss: 19.5092\n",
      "epoch: 123, beta=0, frac_anom=0.05\n",
      "Train Epoch: 124 [0/8040 (0%)]\tLoss: 19.147760\n",
      "Train Epoch: 124 [1200/8040 (15%)]\tLoss: 20.989266\n",
      "Train Epoch: 124 [2400/8040 (30%)]\tLoss: 19.145199\n",
      "Train Epoch: 124 [3600/8040 (45%)]\tLoss: 18.499711\n",
      "Train Epoch: 124 [4800/8040 (60%)]\tLoss: 19.531337\n",
      "Train Epoch: 124 [6000/8040 (75%)]\tLoss: 19.456246\n",
      "Train Epoch: 124 [7200/8040 (90%)]\tLoss: 19.306706\n",
      "====> Epoch: 124 Average loss: 19.4623\n",
      "epoch: 124, beta=0, frac_anom=0.05\n",
      "Train Epoch: 125 [0/8040 (0%)]\tLoss: 20.051019\n",
      "Train Epoch: 125 [1200/8040 (15%)]\tLoss: 19.919706\n",
      "Train Epoch: 125 [2400/8040 (30%)]\tLoss: 19.153536\n",
      "Train Epoch: 125 [3600/8040 (45%)]\tLoss: 19.422815\n",
      "Train Epoch: 125 [4800/8040 (60%)]\tLoss: 19.737150\n",
      "Train Epoch: 125 [6000/8040 (75%)]\tLoss: 19.731346\n",
      "Train Epoch: 125 [7200/8040 (90%)]\tLoss: 18.866272\n",
      "====> Epoch: 125 Average loss: 19.4413\n",
      "epoch: 125, beta=0, frac_anom=0.05\n",
      "Train Epoch: 126 [0/8040 (0%)]\tLoss: 19.591553\n",
      "Train Epoch: 126 [1200/8040 (15%)]\tLoss: 18.891736\n",
      "Train Epoch: 126 [2400/8040 (30%)]\tLoss: 18.967232\n",
      "Train Epoch: 126 [3600/8040 (45%)]\tLoss: 20.321041\n",
      "Train Epoch: 126 [4800/8040 (60%)]\tLoss: 19.238605\n",
      "Train Epoch: 126 [6000/8040 (75%)]\tLoss: 19.037207\n",
      "Train Epoch: 126 [7200/8040 (90%)]\tLoss: 18.767574\n",
      "====> Epoch: 126 Average loss: 19.4465\n",
      "epoch: 126, beta=0, frac_anom=0.05\n",
      "Train Epoch: 127 [0/8040 (0%)]\tLoss: 19.009776\n",
      "Train Epoch: 127 [1200/8040 (15%)]\tLoss: 19.974111\n",
      "Train Epoch: 127 [2400/8040 (30%)]\tLoss: 19.873629\n",
      "Train Epoch: 127 [3600/8040 (45%)]\tLoss: 18.949750\n",
      "Train Epoch: 127 [4800/8040 (60%)]\tLoss: 19.121655\n",
      "Train Epoch: 127 [6000/8040 (75%)]\tLoss: 19.387339\n",
      "Train Epoch: 127 [7200/8040 (90%)]\tLoss: 19.895070\n",
      "====> Epoch: 127 Average loss: 19.4367\n",
      "epoch: 127, beta=0, frac_anom=0.05\n",
      "Train Epoch: 128 [0/8040 (0%)]\tLoss: 20.651027\n",
      "Train Epoch: 128 [1200/8040 (15%)]\tLoss: 20.510840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 128 [2400/8040 (30%)]\tLoss: 19.139722\n",
      "Train Epoch: 128 [3600/8040 (45%)]\tLoss: 18.461047\n",
      "Train Epoch: 128 [4800/8040 (60%)]\tLoss: 18.961605\n",
      "Train Epoch: 128 [6000/8040 (75%)]\tLoss: 20.270799\n",
      "Train Epoch: 128 [7200/8040 (90%)]\tLoss: 19.665139\n",
      "====> Epoch: 128 Average loss: 19.4527\n",
      "epoch: 128, beta=0, frac_anom=0.05\n",
      "Train Epoch: 129 [0/8040 (0%)]\tLoss: 18.520028\n",
      "Train Epoch: 129 [1200/8040 (15%)]\tLoss: 19.203695\n",
      "Train Epoch: 129 [2400/8040 (30%)]\tLoss: 18.667810\n",
      "Train Epoch: 129 [3600/8040 (45%)]\tLoss: 19.363806\n",
      "Train Epoch: 129 [4800/8040 (60%)]\tLoss: 20.266233\n",
      "Train Epoch: 129 [6000/8040 (75%)]\tLoss: 19.280160\n",
      "Train Epoch: 129 [7200/8040 (90%)]\tLoss: 20.004008\n",
      "====> Epoch: 129 Average loss: 19.4415\n",
      "epoch: 129, beta=0, frac_anom=0.05\n",
      "Train Epoch: 130 [0/8040 (0%)]\tLoss: 19.112742\n",
      "Train Epoch: 130 [1200/8040 (15%)]\tLoss: 19.182886\n",
      "Train Epoch: 130 [2400/8040 (30%)]\tLoss: 19.688928\n",
      "Train Epoch: 130 [3600/8040 (45%)]\tLoss: 20.412783\n",
      "Train Epoch: 130 [4800/8040 (60%)]\tLoss: 19.352448\n",
      "Train Epoch: 130 [6000/8040 (75%)]\tLoss: 19.900596\n",
      "Train Epoch: 130 [7200/8040 (90%)]\tLoss: 18.952047\n",
      "====> Epoch: 130 Average loss: 19.4775\n",
      "epoch: 130, beta=0, frac_anom=0.05\n",
      "Train Epoch: 131 [0/8040 (0%)]\tLoss: 18.804110\n",
      "Train Epoch: 131 [1200/8040 (15%)]\tLoss: 19.456543\n",
      "Train Epoch: 131 [2400/8040 (30%)]\tLoss: 18.872825\n",
      "Train Epoch: 131 [3600/8040 (45%)]\tLoss: 20.637630\n",
      "Train Epoch: 131 [4800/8040 (60%)]\tLoss: 19.135059\n",
      "Train Epoch: 131 [6000/8040 (75%)]\tLoss: 18.323924\n",
      "Train Epoch: 131 [7200/8040 (90%)]\tLoss: 18.887821\n",
      "====> Epoch: 131 Average loss: 19.4533\n",
      "epoch: 131, beta=0, frac_anom=0.05\n",
      "Train Epoch: 132 [0/8040 (0%)]\tLoss: 19.374457\n",
      "Train Epoch: 132 [1200/8040 (15%)]\tLoss: 19.386100\n",
      "Train Epoch: 132 [2400/8040 (30%)]\tLoss: 18.661707\n",
      "Train Epoch: 132 [3600/8040 (45%)]\tLoss: 20.221110\n",
      "Train Epoch: 132 [4800/8040 (60%)]\tLoss: 19.688765\n",
      "Train Epoch: 132 [6000/8040 (75%)]\tLoss: 19.163601\n",
      "Train Epoch: 132 [7200/8040 (90%)]\tLoss: 20.262083\n",
      "====> Epoch: 132 Average loss: 19.4302\n",
      "epoch: 132, beta=0, frac_anom=0.05\n",
      "Train Epoch: 133 [0/8040 (0%)]\tLoss: 19.204647\n",
      "Train Epoch: 133 [1200/8040 (15%)]\tLoss: 18.682251\n",
      "Train Epoch: 133 [2400/8040 (30%)]\tLoss: 20.023153\n",
      "Train Epoch: 133 [3600/8040 (45%)]\tLoss: 20.073193\n",
      "Train Epoch: 133 [4800/8040 (60%)]\tLoss: 20.015196\n",
      "Train Epoch: 133 [6000/8040 (75%)]\tLoss: 19.791833\n",
      "Train Epoch: 133 [7200/8040 (90%)]\tLoss: 18.800419\n",
      "====> Epoch: 133 Average loss: 19.4099\n",
      "epoch: 133, beta=0, frac_anom=0.05\n",
      "Train Epoch: 134 [0/8040 (0%)]\tLoss: 19.995506\n",
      "Train Epoch: 134 [1200/8040 (15%)]\tLoss: 19.767773\n",
      "Train Epoch: 134 [2400/8040 (30%)]\tLoss: 18.479602\n",
      "Train Epoch: 134 [3600/8040 (45%)]\tLoss: 19.409932\n",
      "Train Epoch: 134 [4800/8040 (60%)]\tLoss: 18.237280\n",
      "Train Epoch: 134 [6000/8040 (75%)]\tLoss: 18.602661\n",
      "Train Epoch: 134 [7200/8040 (90%)]\tLoss: 20.403556\n",
      "====> Epoch: 134 Average loss: 19.4034\n",
      "epoch: 134, beta=0, frac_anom=0.05\n",
      "Train Epoch: 135 [0/8040 (0%)]\tLoss: 19.027513\n",
      "Train Epoch: 135 [1200/8040 (15%)]\tLoss: 19.043018\n",
      "Train Epoch: 135 [2400/8040 (30%)]\tLoss: 21.107157\n",
      "Train Epoch: 135 [3600/8040 (45%)]\tLoss: 19.778544\n",
      "Train Epoch: 135 [4800/8040 (60%)]\tLoss: 19.295142\n",
      "Train Epoch: 135 [6000/8040 (75%)]\tLoss: 19.125303\n",
      "Train Epoch: 135 [7200/8040 (90%)]\tLoss: 20.065999\n",
      "====> Epoch: 135 Average loss: 19.4323\n",
      "epoch: 135, beta=0, frac_anom=0.05\n",
      "Train Epoch: 136 [0/8040 (0%)]\tLoss: 19.431899\n",
      "Train Epoch: 136 [1200/8040 (15%)]\tLoss: 19.018591\n",
      "Train Epoch: 136 [2400/8040 (30%)]\tLoss: 19.589022\n",
      "Train Epoch: 136 [3600/8040 (45%)]\tLoss: 19.412679\n",
      "Train Epoch: 136 [4800/8040 (60%)]\tLoss: 19.291687\n",
      "Train Epoch: 136 [6000/8040 (75%)]\tLoss: 19.673566\n",
      "Train Epoch: 136 [7200/8040 (90%)]\tLoss: 19.930953\n",
      "====> Epoch: 136 Average loss: 19.4193\n",
      "epoch: 136, beta=0, frac_anom=0.05\n",
      "Train Epoch: 137 [0/8040 (0%)]\tLoss: 20.281982\n",
      "Train Epoch: 137 [1200/8040 (15%)]\tLoss: 19.137779\n",
      "Train Epoch: 137 [2400/8040 (30%)]\tLoss: 20.038483\n",
      "Train Epoch: 137 [3600/8040 (45%)]\tLoss: 20.220410\n",
      "Train Epoch: 137 [4800/8040 (60%)]\tLoss: 19.669159\n",
      "Train Epoch: 137 [6000/8040 (75%)]\tLoss: 20.240780\n",
      "Train Epoch: 137 [7200/8040 (90%)]\tLoss: 19.613568\n",
      "====> Epoch: 137 Average loss: 19.4121\n",
      "epoch: 137, beta=0, frac_anom=0.05\n",
      "Train Epoch: 138 [0/8040 (0%)]\tLoss: 20.155837\n",
      "Train Epoch: 138 [1200/8040 (15%)]\tLoss: 20.442493\n",
      "Train Epoch: 138 [2400/8040 (30%)]\tLoss: 20.242458\n",
      "Train Epoch: 138 [3600/8040 (45%)]\tLoss: 19.254903\n",
      "Train Epoch: 138 [4800/8040 (60%)]\tLoss: 19.703902\n",
      "Train Epoch: 138 [6000/8040 (75%)]\tLoss: 19.241652\n",
      "Train Epoch: 138 [7200/8040 (90%)]\tLoss: 19.959745\n",
      "====> Epoch: 138 Average loss: 19.4313\n",
      "epoch: 138, beta=0, frac_anom=0.05\n",
      "Train Epoch: 139 [0/8040 (0%)]\tLoss: 18.982540\n",
      "Train Epoch: 139 [1200/8040 (15%)]\tLoss: 19.304358\n",
      "Train Epoch: 139 [2400/8040 (30%)]\tLoss: 18.704254\n",
      "Train Epoch: 139 [3600/8040 (45%)]\tLoss: 19.523043\n",
      "Train Epoch: 139 [4800/8040 (60%)]\tLoss: 19.394969\n",
      "Train Epoch: 139 [6000/8040 (75%)]\tLoss: 20.001931\n",
      "Train Epoch: 139 [7200/8040 (90%)]\tLoss: 19.526270\n",
      "====> Epoch: 139 Average loss: 19.4476\n",
      "epoch: 139, beta=0, frac_anom=0.05\n",
      "Train Epoch: 140 [0/8040 (0%)]\tLoss: 20.063165\n",
      "Train Epoch: 140 [1200/8040 (15%)]\tLoss: 20.462354\n",
      "Train Epoch: 140 [2400/8040 (30%)]\tLoss: 19.599990\n",
      "Train Epoch: 140 [3600/8040 (45%)]\tLoss: 19.465841\n",
      "Train Epoch: 140 [4800/8040 (60%)]\tLoss: 20.974011\n",
      "Train Epoch: 140 [6000/8040 (75%)]\tLoss: 18.952197\n",
      "Train Epoch: 140 [7200/8040 (90%)]\tLoss: 20.499534\n",
      "====> Epoch: 140 Average loss: 19.4286\n",
      "epoch: 140, beta=0, frac_anom=0.05\n",
      "Train Epoch: 141 [0/8040 (0%)]\tLoss: 18.700468\n",
      "Train Epoch: 141 [1200/8040 (15%)]\tLoss: 19.271318\n",
      "Train Epoch: 141 [2400/8040 (30%)]\tLoss: 18.635319\n",
      "Train Epoch: 141 [3600/8040 (45%)]\tLoss: 19.501422\n",
      "Train Epoch: 141 [4800/8040 (60%)]\tLoss: 18.427100\n",
      "Train Epoch: 141 [6000/8040 (75%)]\tLoss: 19.121775\n",
      "Train Epoch: 141 [7200/8040 (90%)]\tLoss: 18.521436\n",
      "====> Epoch: 141 Average loss: 19.3470\n",
      "epoch: 141, beta=0, frac_anom=0.05\n",
      "Train Epoch: 142 [0/8040 (0%)]\tLoss: 19.775124\n",
      "Train Epoch: 142 [1200/8040 (15%)]\tLoss: 19.883883\n",
      "Train Epoch: 142 [2400/8040 (30%)]\tLoss: 18.947626\n",
      "Train Epoch: 142 [3600/8040 (45%)]\tLoss: 18.598855\n",
      "Train Epoch: 142 [4800/8040 (60%)]\tLoss: 19.300822\n",
      "Train Epoch: 142 [6000/8040 (75%)]\tLoss: 19.272886\n",
      "Train Epoch: 142 [7200/8040 (90%)]\tLoss: 19.451756\n",
      "====> Epoch: 142 Average loss: 19.3716\n",
      "epoch: 142, beta=0, frac_anom=0.05\n",
      "Train Epoch: 143 [0/8040 (0%)]\tLoss: 18.490202\n",
      "Train Epoch: 143 [1200/8040 (15%)]\tLoss: 19.945850\n",
      "Train Epoch: 143 [2400/8040 (30%)]\tLoss: 18.486631\n",
      "Train Epoch: 143 [3600/8040 (45%)]\tLoss: 19.459218\n",
      "Train Epoch: 143 [4800/8040 (60%)]\tLoss: 19.970050\n",
      "Train Epoch: 143 [6000/8040 (75%)]\tLoss: 20.165672\n",
      "Train Epoch: 143 [7200/8040 (90%)]\tLoss: 20.378560\n",
      "====> Epoch: 143 Average loss: 19.3829\n",
      "epoch: 143, beta=0, frac_anom=0.05\n",
      "Train Epoch: 144 [0/8040 (0%)]\tLoss: 18.814148\n",
      "Train Epoch: 144 [1200/8040 (15%)]\tLoss: 20.084578\n",
      "Train Epoch: 144 [2400/8040 (30%)]\tLoss: 19.329331\n",
      "Train Epoch: 144 [3600/8040 (45%)]\tLoss: 19.935319\n",
      "Train Epoch: 144 [4800/8040 (60%)]\tLoss: 18.686263\n",
      "Train Epoch: 144 [6000/8040 (75%)]\tLoss: 19.574044\n",
      "Train Epoch: 144 [7200/8040 (90%)]\tLoss: 20.046061\n",
      "====> Epoch: 144 Average loss: 19.3790\n",
      "epoch: 144, beta=0, frac_anom=0.05\n",
      "Train Epoch: 145 [0/8040 (0%)]\tLoss: 18.865308\n",
      "Train Epoch: 145 [1200/8040 (15%)]\tLoss: 18.339793\n",
      "Train Epoch: 145 [2400/8040 (30%)]\tLoss: 19.422119\n",
      "Train Epoch: 145 [3600/8040 (45%)]\tLoss: 18.568095\n",
      "Train Epoch: 145 [4800/8040 (60%)]\tLoss: 19.780214\n",
      "Train Epoch: 145 [6000/8040 (75%)]\tLoss: 18.660651\n",
      "Train Epoch: 145 [7200/8040 (90%)]\tLoss: 19.243805\n",
      "====> Epoch: 145 Average loss: 19.3810\n",
      "epoch: 145, beta=0, frac_anom=0.05\n",
      "Train Epoch: 146 [0/8040 (0%)]\tLoss: 17.993040\n",
      "Train Epoch: 146 [1200/8040 (15%)]\tLoss: 19.368109\n",
      "Train Epoch: 146 [2400/8040 (30%)]\tLoss: 19.291370\n",
      "Train Epoch: 146 [3600/8040 (45%)]\tLoss: 19.166329\n",
      "Train Epoch: 146 [4800/8040 (60%)]\tLoss: 19.296842\n",
      "Train Epoch: 146 [6000/8040 (75%)]\tLoss: 19.669289\n",
      "Train Epoch: 146 [7200/8040 (90%)]\tLoss: 19.823151\n",
      "====> Epoch: 146 Average loss: 19.4114\n",
      "epoch: 146, beta=0, frac_anom=0.05\n",
      "Train Epoch: 147 [0/8040 (0%)]\tLoss: 19.465808\n",
      "Train Epoch: 147 [1200/8040 (15%)]\tLoss: 19.372117\n",
      "Train Epoch: 147 [2400/8040 (30%)]\tLoss: 19.542731\n",
      "Train Epoch: 147 [3600/8040 (45%)]\tLoss: 19.270345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 147 [4800/8040 (60%)]\tLoss: 19.870872\n",
      "Train Epoch: 147 [6000/8040 (75%)]\tLoss: 20.300822\n",
      "Train Epoch: 147 [7200/8040 (90%)]\tLoss: 18.896529\n",
      "====> Epoch: 147 Average loss: 19.4266\n",
      "epoch: 147, beta=0, frac_anom=0.05\n",
      "Train Epoch: 148 [0/8040 (0%)]\tLoss: 18.875889\n",
      "Train Epoch: 148 [1200/8040 (15%)]\tLoss: 19.116484\n",
      "Train Epoch: 148 [2400/8040 (30%)]\tLoss: 20.100924\n",
      "Train Epoch: 148 [3600/8040 (45%)]\tLoss: 19.411572\n",
      "Train Epoch: 148 [4800/8040 (60%)]\tLoss: 20.361841\n",
      "Train Epoch: 148 [6000/8040 (75%)]\tLoss: 19.355682\n",
      "Train Epoch: 148 [7200/8040 (90%)]\tLoss: 19.200254\n",
      "====> Epoch: 148 Average loss: 19.3990\n",
      "epoch: 148, beta=0, frac_anom=0.05\n",
      "Train Epoch: 149 [0/8040 (0%)]\tLoss: 19.469979\n",
      "Train Epoch: 149 [1200/8040 (15%)]\tLoss: 19.574379\n",
      "Train Epoch: 149 [2400/8040 (30%)]\tLoss: 19.977096\n",
      "Train Epoch: 149 [3600/8040 (45%)]\tLoss: 19.847593\n",
      "Train Epoch: 149 [4800/8040 (60%)]\tLoss: 18.131999\n",
      "Train Epoch: 149 [6000/8040 (75%)]\tLoss: 19.385775\n",
      "Train Epoch: 149 [7200/8040 (90%)]\tLoss: 20.455151\n",
      "====> Epoch: 149 Average loss: 19.4222\n",
      "epoch: 149, beta=0, frac_anom=0.05\n",
      "Train Epoch: 150 [0/8040 (0%)]\tLoss: 18.967072\n",
      "Train Epoch: 150 [1200/8040 (15%)]\tLoss: 19.629468\n",
      "Train Epoch: 150 [2400/8040 (30%)]\tLoss: 18.285514\n",
      "Train Epoch: 150 [3600/8040 (45%)]\tLoss: 18.380835\n",
      "Train Epoch: 150 [4800/8040 (60%)]\tLoss: 18.500197\n",
      "Train Epoch: 150 [6000/8040 (75%)]\tLoss: 18.349703\n",
      "Train Epoch: 150 [7200/8040 (90%)]\tLoss: 19.114421\n",
      "====> Epoch: 150 Average loss: 19.3771\n",
      "epoch: 150, beta=0, frac_anom=0.05\n",
      "====> Test set loss: 13.2423\n",
      "Train Epoch: 1 [0/8040 (0%)]\tLoss: 147.018799\n",
      "Train Epoch: 1 [1200/8040 (15%)]\tLoss: 48.192806\n",
      "Train Epoch: 1 [2400/8040 (30%)]\tLoss: 39.741016\n",
      "Train Epoch: 1 [3600/8040 (45%)]\tLoss: 35.523413\n",
      "Train Epoch: 1 [4800/8040 (60%)]\tLoss: 32.328312\n",
      "Train Epoch: 1 [6000/8040 (75%)]\tLoss: 32.259719\n",
      "Train Epoch: 1 [7200/8040 (90%)]\tLoss: 31.678174\n",
      "====> Epoch: 1 Average loss: 42.0889\n",
      "epoch: 1, beta=0, frac_anom=0.1\n",
      "Train Epoch: 2 [0/8040 (0%)]\tLoss: 31.166288\n",
      "Train Epoch: 2 [1200/8040 (15%)]\tLoss: 34.609009\n",
      "Train Epoch: 2 [2400/8040 (30%)]\tLoss: 32.063969\n",
      "Train Epoch: 2 [3600/8040 (45%)]\tLoss: 28.716288\n",
      "Train Epoch: 2 [4800/8040 (60%)]\tLoss: 31.713137\n",
      "Train Epoch: 2 [6000/8040 (75%)]\tLoss: 28.645516\n",
      "Train Epoch: 2 [7200/8040 (90%)]\tLoss: 31.615615\n",
      "====> Epoch: 2 Average loss: 30.8853\n",
      "epoch: 2, beta=0, frac_anom=0.1\n",
      "Train Epoch: 3 [0/8040 (0%)]\tLoss: 30.861450\n",
      "Train Epoch: 3 [1200/8040 (15%)]\tLoss: 29.838090\n",
      "Train Epoch: 3 [2400/8040 (30%)]\tLoss: 28.660728\n",
      "Train Epoch: 3 [3600/8040 (45%)]\tLoss: 28.786818\n",
      "Train Epoch: 3 [4800/8040 (60%)]\tLoss: 28.078666\n",
      "Train Epoch: 3 [6000/8040 (75%)]\tLoss: 27.851430\n",
      "Train Epoch: 3 [7200/8040 (90%)]\tLoss: 25.842043\n",
      "====> Epoch: 3 Average loss: 28.6965\n",
      "epoch: 3, beta=0, frac_anom=0.1\n",
      "Train Epoch: 4 [0/8040 (0%)]\tLoss: 27.612382\n",
      "Train Epoch: 4 [1200/8040 (15%)]\tLoss: 25.472076\n",
      "Train Epoch: 4 [2400/8040 (30%)]\tLoss: 26.252352\n",
      "Train Epoch: 4 [3600/8040 (45%)]\tLoss: 26.218837\n",
      "Train Epoch: 4 [4800/8040 (60%)]\tLoss: 27.062612\n",
      "Train Epoch: 4 [6000/8040 (75%)]\tLoss: 27.650545\n",
      "Train Epoch: 4 [7200/8040 (90%)]\tLoss: 24.616718\n",
      "====> Epoch: 4 Average loss: 27.1366\n",
      "epoch: 4, beta=0, frac_anom=0.1\n",
      "Train Epoch: 5 [0/8040 (0%)]\tLoss: 27.176624\n",
      "Train Epoch: 5 [1200/8040 (15%)]\tLoss: 24.764032\n",
      "Train Epoch: 5 [2400/8040 (30%)]\tLoss: 24.965715\n",
      "Train Epoch: 5 [3600/8040 (45%)]\tLoss: 24.906370\n",
      "Train Epoch: 5 [4800/8040 (60%)]\tLoss: 27.620882\n",
      "Train Epoch: 5 [6000/8040 (75%)]\tLoss: 26.530137\n",
      "Train Epoch: 5 [7200/8040 (90%)]\tLoss: 26.436454\n",
      "====> Epoch: 5 Average loss: 26.1953\n",
      "epoch: 5, beta=0, frac_anom=0.1\n",
      "Train Epoch: 6 [0/8040 (0%)]\tLoss: 26.699345\n",
      "Train Epoch: 6 [1200/8040 (15%)]\tLoss: 24.163959\n",
      "Train Epoch: 6 [2400/8040 (30%)]\tLoss: 26.288383\n",
      "Train Epoch: 6 [3600/8040 (45%)]\tLoss: 24.572288\n",
      "Train Epoch: 6 [4800/8040 (60%)]\tLoss: 23.301540\n",
      "Train Epoch: 6 [6000/8040 (75%)]\tLoss: 24.966756\n",
      "Train Epoch: 6 [7200/8040 (90%)]\tLoss: 24.444071\n",
      "====> Epoch: 6 Average loss: 25.5587\n",
      "epoch: 6, beta=0, frac_anom=0.1\n",
      "Train Epoch: 7 [0/8040 (0%)]\tLoss: 25.108295\n",
      "Train Epoch: 7 [1200/8040 (15%)]\tLoss: 24.805233\n",
      "Train Epoch: 7 [2400/8040 (30%)]\tLoss: 25.753125\n",
      "Train Epoch: 7 [3600/8040 (45%)]\tLoss: 26.504366\n",
      "Train Epoch: 7 [4800/8040 (60%)]\tLoss: 25.194993\n",
      "Train Epoch: 7 [6000/8040 (75%)]\tLoss: 25.831075\n",
      "Train Epoch: 7 [7200/8040 (90%)]\tLoss: 25.832697\n",
      "====> Epoch: 7 Average loss: 25.0489\n",
      "epoch: 7, beta=0, frac_anom=0.1\n",
      "Train Epoch: 8 [0/8040 (0%)]\tLoss: 24.140401\n",
      "Train Epoch: 8 [1200/8040 (15%)]\tLoss: 24.957910\n",
      "Train Epoch: 8 [2400/8040 (30%)]\tLoss: 26.251245\n",
      "Train Epoch: 8 [3600/8040 (45%)]\tLoss: 25.197577\n",
      "Train Epoch: 8 [4800/8040 (60%)]\tLoss: 23.907650\n",
      "Train Epoch: 8 [6000/8040 (75%)]\tLoss: 24.043225\n",
      "Train Epoch: 8 [7200/8040 (90%)]\tLoss: 24.695349\n",
      "====> Epoch: 8 Average loss: 24.6272\n",
      "epoch: 8, beta=0, frac_anom=0.1\n",
      "Train Epoch: 9 [0/8040 (0%)]\tLoss: 24.351725\n",
      "Train Epoch: 9 [1200/8040 (15%)]\tLoss: 26.334548\n",
      "Train Epoch: 9 [2400/8040 (30%)]\tLoss: 25.976113\n",
      "Train Epoch: 9 [3600/8040 (45%)]\tLoss: 23.377209\n",
      "Train Epoch: 9 [4800/8040 (60%)]\tLoss: 23.481388\n",
      "Train Epoch: 9 [6000/8040 (75%)]\tLoss: 23.910185\n",
      "Train Epoch: 9 [7200/8040 (90%)]\tLoss: 24.476744\n",
      "====> Epoch: 9 Average loss: 24.2208\n",
      "epoch: 9, beta=0, frac_anom=0.1\n",
      "Train Epoch: 10 [0/8040 (0%)]\tLoss: 24.311499\n",
      "Train Epoch: 10 [1200/8040 (15%)]\tLoss: 24.277342\n",
      "Train Epoch: 10 [2400/8040 (30%)]\tLoss: 24.324015\n",
      "Train Epoch: 10 [3600/8040 (45%)]\tLoss: 23.587508\n",
      "Train Epoch: 10 [4800/8040 (60%)]\tLoss: 23.995721\n",
      "Train Epoch: 10 [6000/8040 (75%)]\tLoss: 24.057084\n",
      "Train Epoch: 10 [7200/8040 (90%)]\tLoss: 23.522705\n",
      "====> Epoch: 10 Average loss: 23.7981\n",
      "epoch: 10, beta=0, frac_anom=0.1\n",
      "Train Epoch: 11 [0/8040 (0%)]\tLoss: 23.146440\n",
      "Train Epoch: 11 [1200/8040 (15%)]\tLoss: 23.910063\n",
      "Train Epoch: 11 [2400/8040 (30%)]\tLoss: 22.146301\n",
      "Train Epoch: 11 [3600/8040 (45%)]\tLoss: 24.596895\n",
      "Train Epoch: 11 [4800/8040 (60%)]\tLoss: 22.379926\n",
      "Train Epoch: 11 [6000/8040 (75%)]\tLoss: 24.404036\n",
      "Train Epoch: 11 [7200/8040 (90%)]\tLoss: 24.988853\n",
      "====> Epoch: 11 Average loss: 23.5663\n",
      "epoch: 11, beta=0, frac_anom=0.1\n",
      "Train Epoch: 12 [0/8040 (0%)]\tLoss: 24.555316\n",
      "Train Epoch: 12 [1200/8040 (15%)]\tLoss: 22.785376\n",
      "Train Epoch: 12 [2400/8040 (30%)]\tLoss: 22.004618\n",
      "Train Epoch: 12 [3600/8040 (45%)]\tLoss: 23.108931\n",
      "Train Epoch: 12 [4800/8040 (60%)]\tLoss: 22.645872\n",
      "Train Epoch: 12 [6000/8040 (75%)]\tLoss: 22.430098\n",
      "Train Epoch: 12 [7200/8040 (90%)]\tLoss: 22.696895\n",
      "====> Epoch: 12 Average loss: 23.2483\n",
      "epoch: 12, beta=0, frac_anom=0.1\n",
      "Train Epoch: 13 [0/8040 (0%)]\tLoss: 22.820900\n",
      "Train Epoch: 13 [1200/8040 (15%)]\tLoss: 23.003925\n",
      "Train Epoch: 13 [2400/8040 (30%)]\tLoss: 23.427395\n",
      "Train Epoch: 13 [3600/8040 (45%)]\tLoss: 22.904679\n",
      "Train Epoch: 13 [4800/8040 (60%)]\tLoss: 22.922611\n",
      "Train Epoch: 13 [6000/8040 (75%)]\tLoss: 22.526522\n",
      "Train Epoch: 13 [7200/8040 (90%)]\tLoss: 22.608038\n",
      "====> Epoch: 13 Average loss: 23.0493\n",
      "epoch: 13, beta=0, frac_anom=0.1\n",
      "Train Epoch: 14 [0/8040 (0%)]\tLoss: 24.033976\n",
      "Train Epoch: 14 [1200/8040 (15%)]\tLoss: 23.374896\n",
      "Train Epoch: 14 [2400/8040 (30%)]\tLoss: 24.479814\n",
      "Train Epoch: 14 [3600/8040 (45%)]\tLoss: 23.708191\n",
      "Train Epoch: 14 [4800/8040 (60%)]\tLoss: 21.577871\n",
      "Train Epoch: 14 [6000/8040 (75%)]\tLoss: 22.326434\n",
      "Train Epoch: 14 [7200/8040 (90%)]\tLoss: 21.506331\n",
      "====> Epoch: 14 Average loss: 22.8574\n",
      "epoch: 14, beta=0, frac_anom=0.1\n",
      "Train Epoch: 15 [0/8040 (0%)]\tLoss: 22.088688\n",
      "Train Epoch: 15 [1200/8040 (15%)]\tLoss: 22.304423\n",
      "Train Epoch: 15 [2400/8040 (30%)]\tLoss: 23.591191\n",
      "Train Epoch: 15 [3600/8040 (45%)]\tLoss: 21.005888\n",
      "Train Epoch: 15 [4800/8040 (60%)]\tLoss: 24.277277\n",
      "Train Epoch: 15 [6000/8040 (75%)]\tLoss: 22.537524\n",
      "Train Epoch: 15 [7200/8040 (90%)]\tLoss: 22.735386\n",
      "====> Epoch: 15 Average loss: 22.6937\n",
      "epoch: 15, beta=0, frac_anom=0.1\n",
      "Train Epoch: 16 [0/8040 (0%)]\tLoss: 22.875309\n",
      "Train Epoch: 16 [1200/8040 (15%)]\tLoss: 22.517299\n",
      "Train Epoch: 16 [2400/8040 (30%)]\tLoss: 22.091772\n",
      "Train Epoch: 16 [3600/8040 (45%)]\tLoss: 23.881777\n",
      "Train Epoch: 16 [4800/8040 (60%)]\tLoss: 21.700321\n",
      "Train Epoch: 16 [6000/8040 (75%)]\tLoss: 22.592769\n",
      "Train Epoch: 16 [7200/8040 (90%)]\tLoss: 22.366797\n",
      "====> Epoch: 16 Average loss: 22.5060\n",
      "epoch: 16, beta=0, frac_anom=0.1\n",
      "Train Epoch: 17 [0/8040 (0%)]\tLoss: 22.152230\n",
      "Train Epoch: 17 [1200/8040 (15%)]\tLoss: 23.152991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 [2400/8040 (30%)]\tLoss: 21.052230\n",
      "Train Epoch: 17 [3600/8040 (45%)]\tLoss: 22.454521\n",
      "Train Epoch: 17 [4800/8040 (60%)]\tLoss: 21.797717\n",
      "Train Epoch: 17 [6000/8040 (75%)]\tLoss: 21.844657\n",
      "Train Epoch: 17 [7200/8040 (90%)]\tLoss: 21.048511\n",
      "====> Epoch: 17 Average loss: 22.4016\n",
      "epoch: 17, beta=0, frac_anom=0.1\n",
      "Train Epoch: 18 [0/8040 (0%)]\tLoss: 23.121873\n",
      "Train Epoch: 18 [1200/8040 (15%)]\tLoss: 20.771857\n",
      "Train Epoch: 18 [2400/8040 (30%)]\tLoss: 23.350431\n",
      "Train Epoch: 18 [3600/8040 (45%)]\tLoss: 21.828764\n",
      "Train Epoch: 18 [4800/8040 (60%)]\tLoss: 22.397378\n",
      "Train Epoch: 18 [6000/8040 (75%)]\tLoss: 21.951876\n",
      "Train Epoch: 18 [7200/8040 (90%)]\tLoss: 22.171560\n",
      "====> Epoch: 18 Average loss: 22.2661\n",
      "epoch: 18, beta=0, frac_anom=0.1\n",
      "Train Epoch: 19 [0/8040 (0%)]\tLoss: 21.105550\n",
      "Train Epoch: 19 [1200/8040 (15%)]\tLoss: 21.354993\n",
      "Train Epoch: 19 [2400/8040 (30%)]\tLoss: 22.007511\n",
      "Train Epoch: 19 [3600/8040 (45%)]\tLoss: 21.692997\n",
      "Train Epoch: 19 [4800/8040 (60%)]\tLoss: 22.709344\n",
      "Train Epoch: 19 [6000/8040 (75%)]\tLoss: 22.263371\n",
      "Train Epoch: 19 [7200/8040 (90%)]\tLoss: 23.446193\n",
      "====> Epoch: 19 Average loss: 22.1881\n",
      "epoch: 19, beta=0, frac_anom=0.1\n",
      "Train Epoch: 20 [0/8040 (0%)]\tLoss: 22.566768\n",
      "Train Epoch: 20 [1200/8040 (15%)]\tLoss: 21.600671\n",
      "Train Epoch: 20 [2400/8040 (30%)]\tLoss: 22.322803\n",
      "Train Epoch: 20 [3600/8040 (45%)]\tLoss: 23.437124\n",
      "Train Epoch: 20 [4800/8040 (60%)]\tLoss: 19.588145\n",
      "Train Epoch: 20 [6000/8040 (75%)]\tLoss: 22.650104\n",
      "Train Epoch: 20 [7200/8040 (90%)]\tLoss: 21.522412\n",
      "====> Epoch: 20 Average loss: 22.1811\n",
      "epoch: 20, beta=0, frac_anom=0.1\n",
      "Train Epoch: 21 [0/8040 (0%)]\tLoss: 22.559013\n",
      "Train Epoch: 21 [1200/8040 (15%)]\tLoss: 22.264168\n",
      "Train Epoch: 21 [2400/8040 (30%)]\tLoss: 22.808565\n",
      "Train Epoch: 21 [3600/8040 (45%)]\tLoss: 22.143835\n",
      "Train Epoch: 21 [4800/8040 (60%)]\tLoss: 21.375726\n",
      "Train Epoch: 21 [6000/8040 (75%)]\tLoss: 21.670636\n",
      "Train Epoch: 21 [7200/8040 (90%)]\tLoss: 21.637476\n",
      "====> Epoch: 21 Average loss: 22.0614\n",
      "epoch: 21, beta=0, frac_anom=0.1\n",
      "Train Epoch: 22 [0/8040 (0%)]\tLoss: 21.615483\n",
      "Train Epoch: 22 [1200/8040 (15%)]\tLoss: 20.858181\n",
      "Train Epoch: 22 [2400/8040 (30%)]\tLoss: 22.648859\n",
      "Train Epoch: 22 [3600/8040 (45%)]\tLoss: 23.114270\n",
      "Train Epoch: 22 [4800/8040 (60%)]\tLoss: 21.694135\n",
      "Train Epoch: 22 [6000/8040 (75%)]\tLoss: 20.987067\n",
      "Train Epoch: 22 [7200/8040 (90%)]\tLoss: 22.587744\n",
      "====> Epoch: 22 Average loss: 21.9176\n",
      "epoch: 22, beta=0, frac_anom=0.1\n",
      "Train Epoch: 23 [0/8040 (0%)]\tLoss: 21.748556\n",
      "Train Epoch: 23 [1200/8040 (15%)]\tLoss: 25.057933\n",
      "Train Epoch: 23 [2400/8040 (30%)]\tLoss: 21.306368\n",
      "Train Epoch: 23 [3600/8040 (45%)]\tLoss: 22.290206\n",
      "Train Epoch: 23 [4800/8040 (60%)]\tLoss: 22.823610\n",
      "Train Epoch: 23 [6000/8040 (75%)]\tLoss: 20.903495\n",
      "Train Epoch: 23 [7200/8040 (90%)]\tLoss: 22.551666\n",
      "====> Epoch: 23 Average loss: 21.8526\n",
      "epoch: 23, beta=0, frac_anom=0.1\n",
      "Train Epoch: 24 [0/8040 (0%)]\tLoss: 22.397013\n",
      "Train Epoch: 24 [1200/8040 (15%)]\tLoss: 20.609357\n",
      "Train Epoch: 24 [2400/8040 (30%)]\tLoss: 22.232294\n",
      "Train Epoch: 24 [3600/8040 (45%)]\tLoss: 20.658525\n",
      "Train Epoch: 24 [4800/8040 (60%)]\tLoss: 22.628491\n",
      "Train Epoch: 24 [6000/8040 (75%)]\tLoss: 21.355387\n",
      "Train Epoch: 24 [7200/8040 (90%)]\tLoss: 22.185457\n",
      "====> Epoch: 24 Average loss: 21.8429\n",
      "epoch: 24, beta=0, frac_anom=0.1\n",
      "Train Epoch: 25 [0/8040 (0%)]\tLoss: 22.603603\n",
      "Train Epoch: 25 [1200/8040 (15%)]\tLoss: 21.618974\n",
      "Train Epoch: 25 [2400/8040 (30%)]\tLoss: 20.146543\n",
      "Train Epoch: 25 [3600/8040 (45%)]\tLoss: 21.658541\n",
      "Train Epoch: 25 [4800/8040 (60%)]\tLoss: 23.165863\n",
      "Train Epoch: 25 [6000/8040 (75%)]\tLoss: 21.391351\n",
      "Train Epoch: 25 [7200/8040 (90%)]\tLoss: 21.512520\n",
      "====> Epoch: 25 Average loss: 21.8123\n",
      "epoch: 25, beta=0, frac_anom=0.1\n",
      "Train Epoch: 26 [0/8040 (0%)]\tLoss: 22.380927\n",
      "Train Epoch: 26 [1200/8040 (15%)]\tLoss: 21.011100\n",
      "Train Epoch: 26 [2400/8040 (30%)]\tLoss: 21.807161\n",
      "Train Epoch: 26 [3600/8040 (45%)]\tLoss: 22.491644\n",
      "Train Epoch: 26 [4800/8040 (60%)]\tLoss: 22.785449\n",
      "Train Epoch: 26 [6000/8040 (75%)]\tLoss: 22.145772\n",
      "Train Epoch: 26 [7200/8040 (90%)]\tLoss: 22.475126\n",
      "====> Epoch: 26 Average loss: 21.7469\n",
      "epoch: 26, beta=0, frac_anom=0.1\n",
      "Train Epoch: 27 [0/8040 (0%)]\tLoss: 21.782686\n",
      "Train Epoch: 27 [1200/8040 (15%)]\tLoss: 21.960156\n",
      "Train Epoch: 27 [2400/8040 (30%)]\tLoss: 21.579163\n",
      "Train Epoch: 27 [3600/8040 (45%)]\tLoss: 22.070416\n",
      "Train Epoch: 27 [4800/8040 (60%)]\tLoss: 21.742387\n",
      "Train Epoch: 27 [6000/8040 (75%)]\tLoss: 22.829862\n",
      "Train Epoch: 27 [7200/8040 (90%)]\tLoss: 22.136235\n",
      "====> Epoch: 27 Average loss: 21.6780\n",
      "epoch: 27, beta=0, frac_anom=0.1\n",
      "Train Epoch: 28 [0/8040 (0%)]\tLoss: 20.933179\n",
      "Train Epoch: 28 [1200/8040 (15%)]\tLoss: 21.571014\n",
      "Train Epoch: 28 [2400/8040 (30%)]\tLoss: 21.316703\n",
      "Train Epoch: 28 [3600/8040 (45%)]\tLoss: 20.682418\n",
      "Train Epoch: 28 [4800/8040 (60%)]\tLoss: 23.330070\n",
      "Train Epoch: 28 [6000/8040 (75%)]\tLoss: 21.179020\n",
      "Train Epoch: 28 [7200/8040 (90%)]\tLoss: 20.358264\n",
      "====> Epoch: 28 Average loss: 21.6075\n",
      "epoch: 28, beta=0, frac_anom=0.1\n",
      "Train Epoch: 29 [0/8040 (0%)]\tLoss: 21.281441\n",
      "Train Epoch: 29 [1200/8040 (15%)]\tLoss: 20.593406\n",
      "Train Epoch: 29 [2400/8040 (30%)]\tLoss: 21.329478\n",
      "Train Epoch: 29 [3600/8040 (45%)]\tLoss: 23.058602\n",
      "Train Epoch: 29 [4800/8040 (60%)]\tLoss: 20.853017\n",
      "Train Epoch: 29 [6000/8040 (75%)]\tLoss: 22.567432\n",
      "Train Epoch: 29 [7200/8040 (90%)]\tLoss: 22.250948\n",
      "====> Epoch: 29 Average loss: 21.5901\n",
      "epoch: 29, beta=0, frac_anom=0.1\n",
      "Train Epoch: 30 [0/8040 (0%)]\tLoss: 23.051546\n",
      "Train Epoch: 30 [1200/8040 (15%)]\tLoss: 21.270992\n",
      "Train Epoch: 30 [2400/8040 (30%)]\tLoss: 22.663507\n",
      "Train Epoch: 30 [3600/8040 (45%)]\tLoss: 21.104744\n",
      "Train Epoch: 30 [4800/8040 (60%)]\tLoss: 21.774780\n",
      "Train Epoch: 30 [6000/8040 (75%)]\tLoss: 21.919104\n",
      "Train Epoch: 30 [7200/8040 (90%)]\tLoss: 23.331317\n",
      "====> Epoch: 30 Average loss: 21.5306\n",
      "epoch: 30, beta=0, frac_anom=0.1\n",
      "Train Epoch: 31 [0/8040 (0%)]\tLoss: 19.560232\n",
      "Train Epoch: 31 [1200/8040 (15%)]\tLoss: 21.351479\n",
      "Train Epoch: 31 [2400/8040 (30%)]\tLoss: 20.488436\n",
      "Train Epoch: 31 [3600/8040 (45%)]\tLoss: 22.141199\n",
      "Train Epoch: 31 [4800/8040 (60%)]\tLoss: 20.305162\n",
      "Train Epoch: 31 [6000/8040 (75%)]\tLoss: 21.523910\n",
      "Train Epoch: 31 [7200/8040 (90%)]\tLoss: 21.708537\n",
      "====> Epoch: 31 Average loss: 21.4931\n",
      "epoch: 31, beta=0, frac_anom=0.1\n",
      "Train Epoch: 32 [0/8040 (0%)]\tLoss: 22.262158\n",
      "Train Epoch: 32 [1200/8040 (15%)]\tLoss: 22.247579\n",
      "Train Epoch: 32 [2400/8040 (30%)]\tLoss: 22.042188\n",
      "Train Epoch: 32 [3600/8040 (45%)]\tLoss: 21.322799\n",
      "Train Epoch: 32 [4800/8040 (60%)]\tLoss: 20.338043\n",
      "Train Epoch: 32 [6000/8040 (75%)]\tLoss: 18.937494\n",
      "Train Epoch: 32 [7200/8040 (90%)]\tLoss: 22.979492\n",
      "====> Epoch: 32 Average loss: 21.4857\n",
      "epoch: 32, beta=0, frac_anom=0.1\n",
      "Train Epoch: 33 [0/8040 (0%)]\tLoss: 20.421155\n",
      "Train Epoch: 33 [1200/8040 (15%)]\tLoss: 22.122886\n",
      "Train Epoch: 33 [2400/8040 (30%)]\tLoss: 21.205039\n",
      "Train Epoch: 33 [3600/8040 (45%)]\tLoss: 20.553548\n",
      "Train Epoch: 33 [4800/8040 (60%)]\tLoss: 21.671806\n",
      "Train Epoch: 33 [6000/8040 (75%)]\tLoss: 22.049125\n",
      "Train Epoch: 33 [7200/8040 (90%)]\tLoss: 19.900264\n",
      "====> Epoch: 33 Average loss: 21.4452\n",
      "epoch: 33, beta=0, frac_anom=0.1\n",
      "Train Epoch: 34 [0/8040 (0%)]\tLoss: 19.447970\n",
      "Train Epoch: 34 [1200/8040 (15%)]\tLoss: 20.261576\n",
      "Train Epoch: 34 [2400/8040 (30%)]\tLoss: 20.979968\n",
      "Train Epoch: 34 [3600/8040 (45%)]\tLoss: 21.663434\n",
      "Train Epoch: 34 [4800/8040 (60%)]\tLoss: 21.046092\n",
      "Train Epoch: 34 [6000/8040 (75%)]\tLoss: 19.550826\n",
      "Train Epoch: 34 [7200/8040 (90%)]\tLoss: 21.537457\n",
      "====> Epoch: 34 Average loss: 21.3689\n",
      "epoch: 34, beta=0, frac_anom=0.1\n",
      "Train Epoch: 35 [0/8040 (0%)]\tLoss: 21.199174\n",
      "Train Epoch: 35 [1200/8040 (15%)]\tLoss: 20.975641\n",
      "Train Epoch: 35 [2400/8040 (30%)]\tLoss: 20.913127\n",
      "Train Epoch: 35 [3600/8040 (45%)]\tLoss: 19.897091\n",
      "Train Epoch: 35 [4800/8040 (60%)]\tLoss: 21.991899\n",
      "Train Epoch: 35 [6000/8040 (75%)]\tLoss: 20.549723\n",
      "Train Epoch: 35 [7200/8040 (90%)]\tLoss: 21.572323\n",
      "====> Epoch: 35 Average loss: 21.3444\n",
      "epoch: 35, beta=0, frac_anom=0.1\n",
      "Train Epoch: 36 [0/8040 (0%)]\tLoss: 21.309332\n",
      "Train Epoch: 36 [1200/8040 (15%)]\tLoss: 20.618056\n",
      "Train Epoch: 36 [2400/8040 (30%)]\tLoss: 21.913660\n",
      "Train Epoch: 36 [3600/8040 (45%)]\tLoss: 19.748226\n",
      "Train Epoch: 36 [4800/8040 (60%)]\tLoss: 20.426709\n",
      "Train Epoch: 36 [6000/8040 (75%)]\tLoss: 20.966113\n",
      "Train Epoch: 36 [7200/8040 (90%)]\tLoss: 21.528149\n",
      "====> Epoch: 36 Average loss: 21.3699\n",
      "epoch: 36, beta=0, frac_anom=0.1\n",
      "Train Epoch: 37 [0/8040 (0%)]\tLoss: 19.971834\n",
      "Train Epoch: 37 [1200/8040 (15%)]\tLoss: 21.567651\n",
      "Train Epoch: 37 [2400/8040 (30%)]\tLoss: 22.277759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 37 [3600/8040 (45%)]\tLoss: 20.586357\n",
      "Train Epoch: 37 [4800/8040 (60%)]\tLoss: 20.713971\n",
      "Train Epoch: 37 [6000/8040 (75%)]\tLoss: 23.094971\n",
      "Train Epoch: 37 [7200/8040 (90%)]\tLoss: 21.321568\n",
      "====> Epoch: 37 Average loss: 21.3130\n",
      "epoch: 37, beta=0, frac_anom=0.1\n",
      "Train Epoch: 38 [0/8040 (0%)]\tLoss: 22.575291\n",
      "Train Epoch: 38 [1200/8040 (15%)]\tLoss: 21.113399\n",
      "Train Epoch: 38 [2400/8040 (30%)]\tLoss: 21.022821\n",
      "Train Epoch: 38 [3600/8040 (45%)]\tLoss: 21.510244\n",
      "Train Epoch: 38 [4800/8040 (60%)]\tLoss: 21.393978\n",
      "Train Epoch: 38 [6000/8040 (75%)]\tLoss: 21.288590\n",
      "Train Epoch: 38 [7200/8040 (90%)]\tLoss: 20.510722\n",
      "====> Epoch: 38 Average loss: 21.2824\n",
      "epoch: 38, beta=0, frac_anom=0.1\n",
      "Train Epoch: 39 [0/8040 (0%)]\tLoss: 20.894515\n",
      "Train Epoch: 39 [1200/8040 (15%)]\tLoss: 21.383093\n",
      "Train Epoch: 39 [2400/8040 (30%)]\tLoss: 20.145970\n",
      "Train Epoch: 39 [3600/8040 (45%)]\tLoss: 20.129663\n",
      "Train Epoch: 39 [4800/8040 (60%)]\tLoss: 21.470288\n",
      "Train Epoch: 39 [6000/8040 (75%)]\tLoss: 22.320203\n",
      "Train Epoch: 39 [7200/8040 (90%)]\tLoss: 20.715686\n",
      "====> Epoch: 39 Average loss: 21.2047\n",
      "epoch: 39, beta=0, frac_anom=0.1\n",
      "Train Epoch: 40 [0/8040 (0%)]\tLoss: 22.241288\n",
      "Train Epoch: 40 [1200/8040 (15%)]\tLoss: 21.608293\n",
      "Train Epoch: 40 [2400/8040 (30%)]\tLoss: 21.640181\n",
      "Train Epoch: 40 [3600/8040 (45%)]\tLoss: 20.055343\n",
      "Train Epoch: 40 [4800/8040 (60%)]\tLoss: 20.207393\n",
      "Train Epoch: 40 [6000/8040 (75%)]\tLoss: 20.550203\n",
      "Train Epoch: 40 [7200/8040 (90%)]\tLoss: 21.114899\n",
      "====> Epoch: 40 Average loss: 21.1863\n",
      "epoch: 40, beta=0, frac_anom=0.1\n",
      "Train Epoch: 41 [0/8040 (0%)]\tLoss: 21.196836\n",
      "Train Epoch: 41 [1200/8040 (15%)]\tLoss: 21.533895\n",
      "Train Epoch: 41 [2400/8040 (30%)]\tLoss: 20.915800\n",
      "Train Epoch: 41 [3600/8040 (45%)]\tLoss: 21.005632\n",
      "Train Epoch: 41 [4800/8040 (60%)]\tLoss: 20.205216\n",
      "Train Epoch: 41 [6000/8040 (75%)]\tLoss: 21.223527\n",
      "Train Epoch: 41 [7200/8040 (90%)]\tLoss: 19.972013\n",
      "====> Epoch: 41 Average loss: 21.1732\n",
      "epoch: 41, beta=0, frac_anom=0.1\n",
      "Train Epoch: 42 [0/8040 (0%)]\tLoss: 20.482869\n",
      "Train Epoch: 42 [1200/8040 (15%)]\tLoss: 21.004338\n",
      "Train Epoch: 42 [2400/8040 (30%)]\tLoss: 21.353170\n",
      "Train Epoch: 42 [3600/8040 (45%)]\tLoss: 21.558667\n",
      "Train Epoch: 42 [4800/8040 (60%)]\tLoss: 19.958403\n",
      "Train Epoch: 42 [6000/8040 (75%)]\tLoss: 21.537337\n",
      "Train Epoch: 42 [7200/8040 (90%)]\tLoss: 21.915420\n",
      "====> Epoch: 42 Average loss: 21.1696\n",
      "epoch: 42, beta=0, frac_anom=0.1\n",
      "Train Epoch: 43 [0/8040 (0%)]\tLoss: 20.817680\n",
      "Train Epoch: 43 [1200/8040 (15%)]\tLoss: 19.469503\n",
      "Train Epoch: 43 [2400/8040 (30%)]\tLoss: 21.482688\n",
      "Train Epoch: 43 [3600/8040 (45%)]\tLoss: 21.179700\n",
      "Train Epoch: 43 [4800/8040 (60%)]\tLoss: 20.533108\n",
      "Train Epoch: 43 [6000/8040 (75%)]\tLoss: 21.399764\n",
      "Train Epoch: 43 [7200/8040 (90%)]\tLoss: 22.058620\n",
      "====> Epoch: 43 Average loss: 21.0907\n",
      "epoch: 43, beta=0, frac_anom=0.1\n",
      "Train Epoch: 44 [0/8040 (0%)]\tLoss: 21.941650\n",
      "Train Epoch: 44 [1200/8040 (15%)]\tLoss: 20.302588\n",
      "Train Epoch: 44 [2400/8040 (30%)]\tLoss: 22.329574\n",
      "Train Epoch: 44 [3600/8040 (45%)]\tLoss: 21.582292\n",
      "Train Epoch: 44 [4800/8040 (60%)]\tLoss: 20.960738\n",
      "Train Epoch: 44 [6000/8040 (75%)]\tLoss: 20.559277\n",
      "Train Epoch: 44 [7200/8040 (90%)]\tLoss: 20.326839\n",
      "====> Epoch: 44 Average loss: 21.0776\n",
      "epoch: 44, beta=0, frac_anom=0.1\n",
      "Train Epoch: 45 [0/8040 (0%)]\tLoss: 21.708252\n",
      "Train Epoch: 45 [1200/8040 (15%)]\tLoss: 20.421407\n",
      "Train Epoch: 45 [2400/8040 (30%)]\tLoss: 20.379138\n",
      "Train Epoch: 45 [3600/8040 (45%)]\tLoss: 21.190788\n",
      "Train Epoch: 45 [4800/8040 (60%)]\tLoss: 20.777810\n",
      "Train Epoch: 45 [6000/8040 (75%)]\tLoss: 22.201538\n",
      "Train Epoch: 45 [7200/8040 (90%)]\tLoss: 21.062600\n",
      "====> Epoch: 45 Average loss: 21.0594\n",
      "epoch: 45, beta=0, frac_anom=0.1\n",
      "Train Epoch: 46 [0/8040 (0%)]\tLoss: 20.808789\n",
      "Train Epoch: 46 [1200/8040 (15%)]\tLoss: 20.422524\n",
      "Train Epoch: 46 [2400/8040 (30%)]\tLoss: 19.889779\n",
      "Train Epoch: 46 [3600/8040 (45%)]\tLoss: 19.880204\n",
      "Train Epoch: 46 [4800/8040 (60%)]\tLoss: 21.433561\n",
      "Train Epoch: 46 [6000/8040 (75%)]\tLoss: 20.655302\n",
      "Train Epoch: 46 [7200/8040 (90%)]\tLoss: 21.512036\n",
      "====> Epoch: 46 Average loss: 21.0418\n",
      "epoch: 46, beta=0, frac_anom=0.1\n",
      "Train Epoch: 47 [0/8040 (0%)]\tLoss: 20.703823\n",
      "Train Epoch: 47 [1200/8040 (15%)]\tLoss: 20.133429\n",
      "Train Epoch: 47 [2400/8040 (30%)]\tLoss: 22.923155\n",
      "Train Epoch: 47 [3600/8040 (45%)]\tLoss: 20.848307\n",
      "Train Epoch: 47 [4800/8040 (60%)]\tLoss: 21.680387\n",
      "Train Epoch: 47 [6000/8040 (75%)]\tLoss: 22.076829\n",
      "Train Epoch: 47 [7200/8040 (90%)]\tLoss: 21.743976\n",
      "====> Epoch: 47 Average loss: 21.0175\n",
      "epoch: 47, beta=0, frac_anom=0.1\n",
      "Train Epoch: 48 [0/8040 (0%)]\tLoss: 20.201831\n",
      "Train Epoch: 48 [1200/8040 (15%)]\tLoss: 20.642206\n",
      "Train Epoch: 48 [2400/8040 (30%)]\tLoss: 20.782922\n",
      "Train Epoch: 48 [3600/8040 (45%)]\tLoss: 21.184475\n",
      "Train Epoch: 48 [4800/8040 (60%)]\tLoss: 21.650478\n",
      "Train Epoch: 48 [6000/8040 (75%)]\tLoss: 21.580391\n",
      "Train Epoch: 48 [7200/8040 (90%)]\tLoss: 19.989811\n",
      "====> Epoch: 48 Average loss: 21.0166\n",
      "epoch: 48, beta=0, frac_anom=0.1\n",
      "Train Epoch: 49 [0/8040 (0%)]\tLoss: 21.801619\n",
      "Train Epoch: 49 [1200/8040 (15%)]\tLoss: 20.921220\n",
      "Train Epoch: 49 [2400/8040 (30%)]\tLoss: 21.499552\n",
      "Train Epoch: 49 [3600/8040 (45%)]\tLoss: 21.051595\n",
      "Train Epoch: 49 [4800/8040 (60%)]\tLoss: 21.549703\n",
      "Train Epoch: 49 [6000/8040 (75%)]\tLoss: 20.678672\n",
      "Train Epoch: 49 [7200/8040 (90%)]\tLoss: 20.750525\n",
      "====> Epoch: 49 Average loss: 20.9972\n",
      "epoch: 49, beta=0, frac_anom=0.1\n",
      "Train Epoch: 50 [0/8040 (0%)]\tLoss: 20.651290\n",
      "Train Epoch: 50 [1200/8040 (15%)]\tLoss: 20.855176\n",
      "Train Epoch: 50 [2400/8040 (30%)]\tLoss: 21.470093\n",
      "Train Epoch: 50 [3600/8040 (45%)]\tLoss: 20.294189\n",
      "Train Epoch: 50 [4800/8040 (60%)]\tLoss: 20.348181\n",
      "Train Epoch: 50 [6000/8040 (75%)]\tLoss: 22.079565\n",
      "Train Epoch: 50 [7200/8040 (90%)]\tLoss: 21.142220\n",
      "====> Epoch: 50 Average loss: 20.9481\n",
      "epoch: 50, beta=0, frac_anom=0.1\n",
      "Train Epoch: 51 [0/8040 (0%)]\tLoss: 21.377413\n",
      "Train Epoch: 51 [1200/8040 (15%)]\tLoss: 21.951267\n",
      "Train Epoch: 51 [2400/8040 (30%)]\tLoss: 21.048680\n",
      "Train Epoch: 51 [3600/8040 (45%)]\tLoss: 21.679484\n",
      "Train Epoch: 51 [4800/8040 (60%)]\tLoss: 19.705615\n",
      "Train Epoch: 51 [6000/8040 (75%)]\tLoss: 21.626054\n",
      "Train Epoch: 51 [7200/8040 (90%)]\tLoss: 19.779948\n",
      "====> Epoch: 51 Average loss: 20.9232\n",
      "epoch: 51, beta=0, frac_anom=0.1\n",
      "Train Epoch: 52 [0/8040 (0%)]\tLoss: 21.987268\n",
      "Train Epoch: 52 [1200/8040 (15%)]\tLoss: 21.857581\n",
      "Train Epoch: 52 [2400/8040 (30%)]\tLoss: 20.975889\n",
      "Train Epoch: 52 [3600/8040 (45%)]\tLoss: 19.735154\n",
      "Train Epoch: 52 [4800/8040 (60%)]\tLoss: 21.857829\n",
      "Train Epoch: 52 [6000/8040 (75%)]\tLoss: 21.495394\n",
      "Train Epoch: 52 [7200/8040 (90%)]\tLoss: 20.444847\n",
      "====> Epoch: 52 Average loss: 20.9089\n",
      "epoch: 52, beta=0, frac_anom=0.1\n",
      "Train Epoch: 53 [0/8040 (0%)]\tLoss: 20.709139\n",
      "Train Epoch: 53 [1200/8040 (15%)]\tLoss: 21.774363\n",
      "Train Epoch: 53 [2400/8040 (30%)]\tLoss: 21.890833\n",
      "Train Epoch: 53 [3600/8040 (45%)]\tLoss: 21.451910\n",
      "Train Epoch: 53 [4800/8040 (60%)]\tLoss: 21.496981\n",
      "Train Epoch: 53 [6000/8040 (75%)]\tLoss: 21.460317\n",
      "Train Epoch: 53 [7200/8040 (90%)]\tLoss: 21.498250\n",
      "====> Epoch: 53 Average loss: 20.8601\n",
      "epoch: 53, beta=0, frac_anom=0.1\n",
      "Train Epoch: 54 [0/8040 (0%)]\tLoss: 20.272302\n",
      "Train Epoch: 54 [1200/8040 (15%)]\tLoss: 21.099628\n",
      "Train Epoch: 54 [2400/8040 (30%)]\tLoss: 20.472109\n",
      "Train Epoch: 54 [3600/8040 (45%)]\tLoss: 21.010807\n",
      "Train Epoch: 54 [4800/8040 (60%)]\tLoss: 20.043292\n",
      "Train Epoch: 54 [6000/8040 (75%)]\tLoss: 20.656004\n",
      "Train Epoch: 54 [7200/8040 (90%)]\tLoss: 19.512988\n",
      "====> Epoch: 54 Average loss: 20.8440\n",
      "epoch: 54, beta=0, frac_anom=0.1\n",
      "Train Epoch: 55 [0/8040 (0%)]\tLoss: 20.828164\n",
      "Train Epoch: 55 [1200/8040 (15%)]\tLoss: 21.431254\n",
      "Train Epoch: 55 [2400/8040 (30%)]\tLoss: 20.827091\n",
      "Train Epoch: 55 [3600/8040 (45%)]\tLoss: 20.604649\n",
      "Train Epoch: 55 [4800/8040 (60%)]\tLoss: 20.831521\n",
      "Train Epoch: 55 [6000/8040 (75%)]\tLoss: 19.989115\n",
      "Train Epoch: 55 [7200/8040 (90%)]\tLoss: 21.194493\n",
      "====> Epoch: 55 Average loss: 20.8631\n",
      "epoch: 55, beta=0, frac_anom=0.1\n",
      "Train Epoch: 56 [0/8040 (0%)]\tLoss: 20.033763\n",
      "Train Epoch: 56 [1200/8040 (15%)]\tLoss: 19.746940\n",
      "Train Epoch: 56 [2400/8040 (30%)]\tLoss: 19.918709\n",
      "Train Epoch: 56 [3600/8040 (45%)]\tLoss: 23.313137\n",
      "Train Epoch: 56 [4800/8040 (60%)]\tLoss: 19.985522\n",
      "Train Epoch: 56 [6000/8040 (75%)]\tLoss: 21.517289\n",
      "Train Epoch: 56 [7200/8040 (90%)]\tLoss: 20.850891\n",
      "====> Epoch: 56 Average loss: 20.8160\n",
      "epoch: 56, beta=0, frac_anom=0.1\n",
      "Train Epoch: 57 [0/8040 (0%)]\tLoss: 18.977071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 57 [1200/8040 (15%)]\tLoss: 21.179787\n",
      "Train Epoch: 57 [2400/8040 (30%)]\tLoss: 21.784113\n",
      "Train Epoch: 57 [3600/8040 (45%)]\tLoss: 20.023865\n",
      "Train Epoch: 57 [4800/8040 (60%)]\tLoss: 21.329093\n",
      "Train Epoch: 57 [6000/8040 (75%)]\tLoss: 21.595490\n",
      "Train Epoch: 57 [7200/8040 (90%)]\tLoss: 19.793713\n",
      "====> Epoch: 57 Average loss: 20.7689\n",
      "epoch: 57, beta=0, frac_anom=0.1\n",
      "Train Epoch: 58 [0/8040 (0%)]\tLoss: 20.608946\n",
      "Train Epoch: 58 [1200/8040 (15%)]\tLoss: 20.882597\n",
      "Train Epoch: 58 [2400/8040 (30%)]\tLoss: 20.869804\n",
      "Train Epoch: 58 [3600/8040 (45%)]\tLoss: 21.059739\n",
      "Train Epoch: 58 [4800/8040 (60%)]\tLoss: 21.900741\n",
      "Train Epoch: 58 [6000/8040 (75%)]\tLoss: 21.186853\n",
      "Train Epoch: 58 [7200/8040 (90%)]\tLoss: 20.963611\n",
      "====> Epoch: 58 Average loss: 20.7622\n",
      "epoch: 58, beta=0, frac_anom=0.1\n",
      "Train Epoch: 59 [0/8040 (0%)]\tLoss: 21.085549\n",
      "Train Epoch: 59 [1200/8040 (15%)]\tLoss: 19.793953\n",
      "Train Epoch: 59 [2400/8040 (30%)]\tLoss: 20.092501\n",
      "Train Epoch: 59 [3600/8040 (45%)]\tLoss: 21.977625\n",
      "Train Epoch: 59 [4800/8040 (60%)]\tLoss: 20.738639\n",
      "Train Epoch: 59 [6000/8040 (75%)]\tLoss: 20.604012\n",
      "Train Epoch: 59 [7200/8040 (90%)]\tLoss: 19.686106\n",
      "====> Epoch: 59 Average loss: 20.7717\n",
      "epoch: 59, beta=0, frac_anom=0.1\n",
      "Train Epoch: 60 [0/8040 (0%)]\tLoss: 20.269466\n",
      "Train Epoch: 60 [1200/8040 (15%)]\tLoss: 20.309668\n",
      "Train Epoch: 60 [2400/8040 (30%)]\tLoss: 20.323566\n",
      "Train Epoch: 60 [3600/8040 (45%)]\tLoss: 21.477173\n",
      "Train Epoch: 60 [4800/8040 (60%)]\tLoss: 20.509253\n",
      "Train Epoch: 60 [6000/8040 (75%)]\tLoss: 19.743648\n",
      "Train Epoch: 60 [7200/8040 (90%)]\tLoss: 20.933065\n",
      "====> Epoch: 60 Average loss: 20.7423\n",
      "epoch: 60, beta=0, frac_anom=0.1\n",
      "Train Epoch: 61 [0/8040 (0%)]\tLoss: 20.508061\n",
      "Train Epoch: 61 [1200/8040 (15%)]\tLoss: 20.628465\n",
      "Train Epoch: 61 [2400/8040 (30%)]\tLoss: 21.982646\n",
      "Train Epoch: 61 [3600/8040 (45%)]\tLoss: 21.271769\n",
      "Train Epoch: 61 [4800/8040 (60%)]\tLoss: 20.153491\n",
      "Train Epoch: 61 [6000/8040 (75%)]\tLoss: 20.166827\n",
      "Train Epoch: 61 [7200/8040 (90%)]\tLoss: 20.794328\n",
      "====> Epoch: 61 Average loss: 20.6963\n",
      "epoch: 61, beta=0, frac_anom=0.1\n",
      "Train Epoch: 62 [0/8040 (0%)]\tLoss: 20.406348\n",
      "Train Epoch: 62 [1200/8040 (15%)]\tLoss: 21.086031\n",
      "Train Epoch: 62 [2400/8040 (30%)]\tLoss: 21.538218\n",
      "Train Epoch: 62 [3600/8040 (45%)]\tLoss: 20.679736\n",
      "Train Epoch: 62 [4800/8040 (60%)]\tLoss: 19.894051\n",
      "Train Epoch: 62 [6000/8040 (75%)]\tLoss: 20.769682\n",
      "Train Epoch: 62 [7200/8040 (90%)]\tLoss: 22.128585\n",
      "====> Epoch: 62 Average loss: 20.6423\n",
      "epoch: 62, beta=0, frac_anom=0.1\n",
      "Train Epoch: 63 [0/8040 (0%)]\tLoss: 20.895998\n",
      "Train Epoch: 63 [1200/8040 (15%)]\tLoss: 20.159733\n",
      "Train Epoch: 63 [2400/8040 (30%)]\tLoss: 20.009762\n",
      "Train Epoch: 63 [3600/8040 (45%)]\tLoss: 19.878369\n",
      "Train Epoch: 63 [4800/8040 (60%)]\tLoss: 20.263163\n",
      "Train Epoch: 63 [6000/8040 (75%)]\tLoss: 21.812166\n",
      "Train Epoch: 63 [7200/8040 (90%)]\tLoss: 19.946149\n",
      "====> Epoch: 63 Average loss: 20.6570\n",
      "epoch: 63, beta=0, frac_anom=0.1\n",
      "Train Epoch: 64 [0/8040 (0%)]\tLoss: 21.467662\n",
      "Train Epoch: 64 [1200/8040 (15%)]\tLoss: 19.922685\n",
      "Train Epoch: 64 [2400/8040 (30%)]\tLoss: 21.654814\n",
      "Train Epoch: 64 [3600/8040 (45%)]\tLoss: 19.190867\n",
      "Train Epoch: 64 [4800/8040 (60%)]\tLoss: 20.998293\n",
      "Train Epoch: 64 [6000/8040 (75%)]\tLoss: 20.217063\n",
      "Train Epoch: 64 [7200/8040 (90%)]\tLoss: 20.943504\n",
      "====> Epoch: 64 Average loss: 20.6638\n",
      "epoch: 64, beta=0, frac_anom=0.1\n",
      "Train Epoch: 65 [0/8040 (0%)]\tLoss: 20.788757\n",
      "Train Epoch: 65 [1200/8040 (15%)]\tLoss: 20.642611\n",
      "Train Epoch: 65 [2400/8040 (30%)]\tLoss: 20.468083\n",
      "Train Epoch: 65 [3600/8040 (45%)]\tLoss: 19.461983\n",
      "Train Epoch: 65 [4800/8040 (60%)]\tLoss: 21.452635\n",
      "Train Epoch: 65 [6000/8040 (75%)]\tLoss: 20.723525\n",
      "Train Epoch: 65 [7200/8040 (90%)]\tLoss: 20.882804\n",
      "====> Epoch: 65 Average loss: 20.6745\n",
      "epoch: 65, beta=0, frac_anom=0.1\n",
      "Train Epoch: 66 [0/8040 (0%)]\tLoss: 20.678682\n",
      "Train Epoch: 66 [1200/8040 (15%)]\tLoss: 19.646043\n",
      "Train Epoch: 66 [2400/8040 (30%)]\tLoss: 19.954008\n",
      "Train Epoch: 66 [3600/8040 (45%)]\tLoss: 20.497298\n",
      "Train Epoch: 66 [4800/8040 (60%)]\tLoss: 20.943644\n",
      "Train Epoch: 66 [6000/8040 (75%)]\tLoss: 20.332113\n",
      "Train Epoch: 66 [7200/8040 (90%)]\tLoss: 21.386051\n",
      "====> Epoch: 66 Average loss: 20.6120\n",
      "epoch: 66, beta=0, frac_anom=0.1\n",
      "Train Epoch: 67 [0/8040 (0%)]\tLoss: 21.646855\n",
      "Train Epoch: 67 [1200/8040 (15%)]\tLoss: 20.212016\n",
      "Train Epoch: 67 [2400/8040 (30%)]\tLoss: 20.193087\n",
      "Train Epoch: 67 [3600/8040 (45%)]\tLoss: 20.021169\n",
      "Train Epoch: 67 [4800/8040 (60%)]\tLoss: 19.363320\n",
      "Train Epoch: 67 [6000/8040 (75%)]\tLoss: 21.344122\n",
      "Train Epoch: 67 [7200/8040 (90%)]\tLoss: 21.631809\n",
      "====> Epoch: 67 Average loss: 20.6011\n",
      "epoch: 67, beta=0, frac_anom=0.1\n",
      "Train Epoch: 68 [0/8040 (0%)]\tLoss: 19.209926\n",
      "Train Epoch: 68 [1200/8040 (15%)]\tLoss: 20.151101\n",
      "Train Epoch: 68 [2400/8040 (30%)]\tLoss: 21.099654\n",
      "Train Epoch: 68 [3600/8040 (45%)]\tLoss: 20.984825\n",
      "Train Epoch: 68 [4800/8040 (60%)]\tLoss: 19.404456\n",
      "Train Epoch: 68 [6000/8040 (75%)]\tLoss: 21.670056\n",
      "Train Epoch: 68 [7200/8040 (90%)]\tLoss: 21.198779\n",
      "====> Epoch: 68 Average loss: 20.5782\n",
      "epoch: 68, beta=0, frac_anom=0.1\n",
      "Train Epoch: 69 [0/8040 (0%)]\tLoss: 19.995929\n",
      "Train Epoch: 69 [1200/8040 (15%)]\tLoss: 20.687758\n",
      "Train Epoch: 69 [2400/8040 (30%)]\tLoss: 20.289677\n",
      "Train Epoch: 69 [3600/8040 (45%)]\tLoss: 20.513110\n",
      "Train Epoch: 69 [4800/8040 (60%)]\tLoss: 21.367977\n",
      "Train Epoch: 69 [6000/8040 (75%)]\tLoss: 20.713047\n",
      "Train Epoch: 69 [7200/8040 (90%)]\tLoss: 21.042511\n",
      "====> Epoch: 69 Average loss: 20.5667\n",
      "epoch: 69, beta=0, frac_anom=0.1\n",
      "Train Epoch: 70 [0/8040 (0%)]\tLoss: 20.854567\n",
      "Train Epoch: 70 [1200/8040 (15%)]\tLoss: 20.265759\n",
      "Train Epoch: 70 [2400/8040 (30%)]\tLoss: 20.963253\n",
      "Train Epoch: 70 [3600/8040 (45%)]\tLoss: 20.696594\n",
      "Train Epoch: 70 [4800/8040 (60%)]\tLoss: 20.319014\n",
      "Train Epoch: 70 [6000/8040 (75%)]\tLoss: 19.937191\n",
      "Train Epoch: 70 [7200/8040 (90%)]\tLoss: 19.144132\n",
      "====> Epoch: 70 Average loss: 20.5523\n",
      "epoch: 70, beta=0, frac_anom=0.1\n",
      "Train Epoch: 71 [0/8040 (0%)]\tLoss: 20.768402\n",
      "Train Epoch: 71 [1200/8040 (15%)]\tLoss: 20.203674\n",
      "Train Epoch: 71 [2400/8040 (30%)]\tLoss: 20.311702\n",
      "Train Epoch: 71 [3600/8040 (45%)]\tLoss: 21.155113\n",
      "Train Epoch: 71 [4800/8040 (60%)]\tLoss: 21.021031\n",
      "Train Epoch: 71 [6000/8040 (75%)]\tLoss: 20.430162\n",
      "Train Epoch: 71 [7200/8040 (90%)]\tLoss: 21.086898\n",
      "====> Epoch: 71 Average loss: 20.5439\n",
      "epoch: 71, beta=0, frac_anom=0.1\n",
      "Train Epoch: 72 [0/8040 (0%)]\tLoss: 19.078343\n",
      "Train Epoch: 72 [1200/8040 (15%)]\tLoss: 20.284896\n",
      "Train Epoch: 72 [2400/8040 (30%)]\tLoss: 19.771924\n",
      "Train Epoch: 72 [3600/8040 (45%)]\tLoss: 21.111161\n",
      "Train Epoch: 72 [4800/8040 (60%)]\tLoss: 20.138342\n",
      "Train Epoch: 72 [6000/8040 (75%)]\tLoss: 19.875110\n",
      "Train Epoch: 72 [7200/8040 (90%)]\tLoss: 20.783964\n",
      "====> Epoch: 72 Average loss: 20.5183\n",
      "epoch: 72, beta=0, frac_anom=0.1\n",
      "Train Epoch: 73 [0/8040 (0%)]\tLoss: 19.996016\n",
      "Train Epoch: 73 [1200/8040 (15%)]\tLoss: 21.324410\n",
      "Train Epoch: 73 [2400/8040 (30%)]\tLoss: 21.024862\n",
      "Train Epoch: 73 [3600/8040 (45%)]\tLoss: 20.079877\n",
      "Train Epoch: 73 [4800/8040 (60%)]\tLoss: 20.332117\n",
      "Train Epoch: 73 [6000/8040 (75%)]\tLoss: 20.106594\n",
      "Train Epoch: 73 [7200/8040 (90%)]\tLoss: 19.852413\n",
      "====> Epoch: 73 Average loss: 20.5421\n",
      "epoch: 73, beta=0, frac_anom=0.1\n",
      "Train Epoch: 74 [0/8040 (0%)]\tLoss: 20.257458\n",
      "Train Epoch: 74 [1200/8040 (15%)]\tLoss: 20.905750\n",
      "Train Epoch: 74 [2400/8040 (30%)]\tLoss: 20.426506\n",
      "Train Epoch: 74 [3600/8040 (45%)]\tLoss: 20.629256\n",
      "Train Epoch: 74 [4800/8040 (60%)]\tLoss: 20.136816\n",
      "Train Epoch: 74 [6000/8040 (75%)]\tLoss: 20.536206\n",
      "Train Epoch: 74 [7200/8040 (90%)]\tLoss: 20.564103\n",
      "====> Epoch: 74 Average loss: 20.5207\n",
      "epoch: 74, beta=0, frac_anom=0.1\n",
      "Train Epoch: 75 [0/8040 (0%)]\tLoss: 20.879757\n",
      "Train Epoch: 75 [1200/8040 (15%)]\tLoss: 20.500726\n",
      "Train Epoch: 75 [2400/8040 (30%)]\tLoss: 20.700087\n",
      "Train Epoch: 75 [3600/8040 (45%)]\tLoss: 19.719450\n",
      "Train Epoch: 75 [4800/8040 (60%)]\tLoss: 19.901013\n",
      "Train Epoch: 75 [6000/8040 (75%)]\tLoss: 20.634041\n",
      "Train Epoch: 75 [7200/8040 (90%)]\tLoss: 21.600004\n",
      "====> Epoch: 75 Average loss: 20.4928\n",
      "epoch: 75, beta=0, frac_anom=0.1\n",
      "Train Epoch: 76 [0/8040 (0%)]\tLoss: 20.219956\n",
      "Train Epoch: 76 [1200/8040 (15%)]\tLoss: 19.387602\n",
      "Train Epoch: 76 [2400/8040 (30%)]\tLoss: 20.419912\n",
      "Train Epoch: 76 [3600/8040 (45%)]\tLoss: 20.430164\n",
      "Train Epoch: 76 [4800/8040 (60%)]\tLoss: 20.005514\n",
      "Train Epoch: 76 [6000/8040 (75%)]\tLoss: 19.547205\n",
      "Train Epoch: 76 [7200/8040 (90%)]\tLoss: 20.319340\n",
      "====> Epoch: 76 Average loss: 20.4786\n",
      "epoch: 76, beta=0, frac_anom=0.1\n",
      "Train Epoch: 77 [0/8040 (0%)]\tLoss: 19.146427\n",
      "Train Epoch: 77 [1200/8040 (15%)]\tLoss: 21.045353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 77 [2400/8040 (30%)]\tLoss: 20.557843\n",
      "Train Epoch: 77 [3600/8040 (45%)]\tLoss: 21.316935\n",
      "Train Epoch: 77 [4800/8040 (60%)]\tLoss: 20.738025\n",
      "Train Epoch: 77 [6000/8040 (75%)]\tLoss: 21.231419\n",
      "Train Epoch: 77 [7200/8040 (90%)]\tLoss: 21.089766\n",
      "====> Epoch: 77 Average loss: 20.4925\n",
      "epoch: 77, beta=0, frac_anom=0.1\n",
      "Train Epoch: 78 [0/8040 (0%)]\tLoss: 20.797559\n",
      "Train Epoch: 78 [1200/8040 (15%)]\tLoss: 20.423743\n",
      "Train Epoch: 78 [2400/8040 (30%)]\tLoss: 19.088428\n",
      "Train Epoch: 78 [3600/8040 (45%)]\tLoss: 19.742963\n",
      "Train Epoch: 78 [4800/8040 (60%)]\tLoss: 20.885042\n",
      "Train Epoch: 78 [6000/8040 (75%)]\tLoss: 20.666150\n",
      "Train Epoch: 78 [7200/8040 (90%)]\tLoss: 19.533006\n",
      "====> Epoch: 78 Average loss: 20.4665\n",
      "epoch: 78, beta=0, frac_anom=0.1\n",
      "Train Epoch: 79 [0/8040 (0%)]\tLoss: 20.766789\n",
      "Train Epoch: 79 [1200/8040 (15%)]\tLoss: 20.886914\n",
      "Train Epoch: 79 [2400/8040 (30%)]\tLoss: 20.007432\n",
      "Train Epoch: 79 [3600/8040 (45%)]\tLoss: 20.700175\n",
      "Train Epoch: 79 [4800/8040 (60%)]\tLoss: 19.960557\n",
      "Train Epoch: 79 [6000/8040 (75%)]\tLoss: 21.169525\n",
      "Train Epoch: 79 [7200/8040 (90%)]\tLoss: 19.837581\n",
      "====> Epoch: 79 Average loss: 20.4750\n",
      "epoch: 79, beta=0, frac_anom=0.1\n",
      "Train Epoch: 80 [0/8040 (0%)]\tLoss: 19.588672\n",
      "Train Epoch: 80 [1200/8040 (15%)]\tLoss: 20.356588\n",
      "Train Epoch: 80 [2400/8040 (30%)]\tLoss: 20.035042\n",
      "Train Epoch: 80 [3600/8040 (45%)]\tLoss: 21.027071\n",
      "Train Epoch: 80 [4800/8040 (60%)]\tLoss: 19.971617\n",
      "Train Epoch: 80 [6000/8040 (75%)]\tLoss: 20.713338\n",
      "Train Epoch: 80 [7200/8040 (90%)]\tLoss: 19.908541\n",
      "====> Epoch: 80 Average loss: 20.3785\n",
      "epoch: 80, beta=0, frac_anom=0.1\n",
      "Train Epoch: 81 [0/8040 (0%)]\tLoss: 19.790055\n",
      "Train Epoch: 81 [1200/8040 (15%)]\tLoss: 18.925380\n",
      "Train Epoch: 81 [2400/8040 (30%)]\tLoss: 21.409176\n",
      "Train Epoch: 81 [3600/8040 (45%)]\tLoss: 20.017794\n",
      "Train Epoch: 81 [4800/8040 (60%)]\tLoss: 19.651422\n",
      "Train Epoch: 81 [6000/8040 (75%)]\tLoss: 21.501892\n",
      "Train Epoch: 81 [7200/8040 (90%)]\tLoss: 19.471600\n",
      "====> Epoch: 81 Average loss: 20.4654\n",
      "epoch: 81, beta=0, frac_anom=0.1\n",
      "Train Epoch: 82 [0/8040 (0%)]\tLoss: 22.085911\n",
      "Train Epoch: 82 [1200/8040 (15%)]\tLoss: 19.957428\n",
      "Train Epoch: 82 [2400/8040 (30%)]\tLoss: 21.022691\n",
      "Train Epoch: 82 [3600/8040 (45%)]\tLoss: 21.099270\n",
      "Train Epoch: 82 [4800/8040 (60%)]\tLoss: 19.143162\n",
      "Train Epoch: 82 [6000/8040 (75%)]\tLoss: 20.618018\n",
      "Train Epoch: 82 [7200/8040 (90%)]\tLoss: 21.293947\n",
      "====> Epoch: 82 Average loss: 20.4076\n",
      "epoch: 82, beta=0, frac_anom=0.1\n",
      "Train Epoch: 83 [0/8040 (0%)]\tLoss: 20.599790\n",
      "Train Epoch: 83 [1200/8040 (15%)]\tLoss: 19.724915\n",
      "Train Epoch: 83 [2400/8040 (30%)]\tLoss: 19.800966\n",
      "Train Epoch: 83 [3600/8040 (45%)]\tLoss: 21.050189\n",
      "Train Epoch: 83 [4800/8040 (60%)]\tLoss: 21.236808\n",
      "Train Epoch: 83 [6000/8040 (75%)]\tLoss: 20.916473\n",
      "Train Epoch: 83 [7200/8040 (90%)]\tLoss: 20.963814\n",
      "====> Epoch: 83 Average loss: 20.3714\n",
      "epoch: 83, beta=0, frac_anom=0.1\n",
      "Train Epoch: 84 [0/8040 (0%)]\tLoss: 20.440649\n",
      "Train Epoch: 84 [1200/8040 (15%)]\tLoss: 21.009546\n",
      "Train Epoch: 84 [2400/8040 (30%)]\tLoss: 21.147363\n",
      "Train Epoch: 84 [3600/8040 (45%)]\tLoss: 21.010057\n",
      "Train Epoch: 84 [4800/8040 (60%)]\tLoss: 20.672860\n",
      "Train Epoch: 84 [6000/8040 (75%)]\tLoss: 19.671490\n",
      "Train Epoch: 84 [7200/8040 (90%)]\tLoss: 20.146667\n",
      "====> Epoch: 84 Average loss: 20.4110\n",
      "epoch: 84, beta=0, frac_anom=0.1\n",
      "Train Epoch: 85 [0/8040 (0%)]\tLoss: 19.413599\n",
      "Train Epoch: 85 [1200/8040 (15%)]\tLoss: 19.690853\n",
      "Train Epoch: 85 [2400/8040 (30%)]\tLoss: 20.372620\n",
      "Train Epoch: 85 [3600/8040 (45%)]\tLoss: 19.365926\n",
      "Train Epoch: 85 [4800/8040 (60%)]\tLoss: 20.030245\n",
      "Train Epoch: 85 [6000/8040 (75%)]\tLoss: 20.658539\n",
      "Train Epoch: 85 [7200/8040 (90%)]\tLoss: 21.356144\n",
      "====> Epoch: 85 Average loss: 20.3676\n",
      "epoch: 85, beta=0, frac_anom=0.1\n",
      "Train Epoch: 86 [0/8040 (0%)]\tLoss: 20.914199\n",
      "Train Epoch: 86 [1200/8040 (15%)]\tLoss: 20.534855\n",
      "Train Epoch: 86 [2400/8040 (30%)]\tLoss: 19.449807\n",
      "Train Epoch: 86 [3600/8040 (45%)]\tLoss: 21.210453\n",
      "Train Epoch: 86 [4800/8040 (60%)]\tLoss: 20.138934\n",
      "Train Epoch: 86 [6000/8040 (75%)]\tLoss: 20.653215\n",
      "Train Epoch: 86 [7200/8040 (90%)]\tLoss: 20.480642\n",
      "====> Epoch: 86 Average loss: 20.4402\n",
      "epoch: 86, beta=0, frac_anom=0.1\n",
      "Train Epoch: 87 [0/8040 (0%)]\tLoss: 20.228031\n",
      "Train Epoch: 87 [1200/8040 (15%)]\tLoss: 19.792627\n",
      "Train Epoch: 87 [2400/8040 (30%)]\tLoss: 21.494666\n",
      "Train Epoch: 87 [3600/8040 (45%)]\tLoss: 20.399742\n",
      "Train Epoch: 87 [4800/8040 (60%)]\tLoss: 19.581051\n",
      "Train Epoch: 87 [6000/8040 (75%)]\tLoss: 19.904360\n",
      "Train Epoch: 87 [7200/8040 (90%)]\tLoss: 20.347750\n",
      "====> Epoch: 87 Average loss: 20.3858\n",
      "epoch: 87, beta=0, frac_anom=0.1\n",
      "Train Epoch: 88 [0/8040 (0%)]\tLoss: 19.503215\n",
      "Train Epoch: 88 [1200/8040 (15%)]\tLoss: 19.343640\n",
      "Train Epoch: 88 [2400/8040 (30%)]\tLoss: 20.256549\n",
      "Train Epoch: 88 [3600/8040 (45%)]\tLoss: 20.825256\n",
      "Train Epoch: 88 [4800/8040 (60%)]\tLoss: 19.165088\n",
      "Train Epoch: 88 [6000/8040 (75%)]\tLoss: 19.867153\n",
      "Train Epoch: 88 [7200/8040 (90%)]\tLoss: 20.854407\n",
      "====> Epoch: 88 Average loss: 20.3688\n",
      "epoch: 88, beta=0, frac_anom=0.1\n",
      "Train Epoch: 89 [0/8040 (0%)]\tLoss: 19.371124\n",
      "Train Epoch: 89 [1200/8040 (15%)]\tLoss: 19.023492\n",
      "Train Epoch: 89 [2400/8040 (30%)]\tLoss: 21.307229\n",
      "Train Epoch: 89 [3600/8040 (45%)]\tLoss: 20.045890\n",
      "Train Epoch: 89 [4800/8040 (60%)]\tLoss: 20.624320\n",
      "Train Epoch: 89 [6000/8040 (75%)]\tLoss: 21.701888\n",
      "Train Epoch: 89 [7200/8040 (90%)]\tLoss: 18.472866\n",
      "====> Epoch: 89 Average loss: 20.3465\n",
      "epoch: 89, beta=0, frac_anom=0.1\n",
      "Train Epoch: 90 [0/8040 (0%)]\tLoss: 19.728003\n",
      "Train Epoch: 90 [1200/8040 (15%)]\tLoss: 19.719301\n",
      "Train Epoch: 90 [2400/8040 (30%)]\tLoss: 20.879643\n",
      "Train Epoch: 90 [3600/8040 (45%)]\tLoss: 20.745199\n",
      "Train Epoch: 90 [4800/8040 (60%)]\tLoss: 20.098971\n",
      "Train Epoch: 90 [6000/8040 (75%)]\tLoss: 20.188464\n",
      "Train Epoch: 90 [7200/8040 (90%)]\tLoss: 19.694006\n",
      "====> Epoch: 90 Average loss: 20.3295\n",
      "epoch: 90, beta=0, frac_anom=0.1\n",
      "Train Epoch: 91 [0/8040 (0%)]\tLoss: 20.530815\n",
      "Train Epoch: 91 [1200/8040 (15%)]\tLoss: 19.758205\n",
      "Train Epoch: 91 [2400/8040 (30%)]\tLoss: 20.353276\n",
      "Train Epoch: 91 [3600/8040 (45%)]\tLoss: 20.154535\n",
      "Train Epoch: 91 [4800/8040 (60%)]\tLoss: 20.502494\n",
      "Train Epoch: 91 [6000/8040 (75%)]\tLoss: 21.425881\n",
      "Train Epoch: 91 [7200/8040 (90%)]\tLoss: 19.854549\n",
      "====> Epoch: 91 Average loss: 20.3194\n",
      "epoch: 91, beta=0, frac_anom=0.1\n",
      "Train Epoch: 92 [0/8040 (0%)]\tLoss: 19.451078\n",
      "Train Epoch: 92 [1200/8040 (15%)]\tLoss: 19.868042\n",
      "Train Epoch: 92 [2400/8040 (30%)]\tLoss: 21.337622\n",
      "Train Epoch: 92 [3600/8040 (45%)]\tLoss: 19.593726\n",
      "Train Epoch: 92 [4800/8040 (60%)]\tLoss: 21.228261\n",
      "Train Epoch: 92 [6000/8040 (75%)]\tLoss: 19.314217\n",
      "Train Epoch: 92 [7200/8040 (90%)]\tLoss: 20.156567\n",
      "====> Epoch: 92 Average loss: 20.2482\n",
      "epoch: 92, beta=0, frac_anom=0.1\n",
      "Train Epoch: 93 [0/8040 (0%)]\tLoss: 20.173820\n",
      "Train Epoch: 93 [1200/8040 (15%)]\tLoss: 21.215332\n",
      "Train Epoch: 93 [2400/8040 (30%)]\tLoss: 20.419592\n",
      "Train Epoch: 93 [3600/8040 (45%)]\tLoss: 20.121000\n",
      "Train Epoch: 93 [4800/8040 (60%)]\tLoss: 20.216838\n",
      "Train Epoch: 93 [6000/8040 (75%)]\tLoss: 18.332178\n",
      "Train Epoch: 93 [7200/8040 (90%)]\tLoss: 20.078042\n",
      "====> Epoch: 93 Average loss: 20.2563\n",
      "epoch: 93, beta=0, frac_anom=0.1\n",
      "Train Epoch: 94 [0/8040 (0%)]\tLoss: 20.024215\n",
      "Train Epoch: 94 [1200/8040 (15%)]\tLoss: 19.489779\n",
      "Train Epoch: 94 [2400/8040 (30%)]\tLoss: 20.175802\n",
      "Train Epoch: 94 [3600/8040 (45%)]\tLoss: 19.699361\n",
      "Train Epoch: 94 [4800/8040 (60%)]\tLoss: 19.204224\n",
      "Train Epoch: 94 [6000/8040 (75%)]\tLoss: 18.607776\n",
      "Train Epoch: 94 [7200/8040 (90%)]\tLoss: 20.537913\n",
      "====> Epoch: 94 Average loss: 20.2801\n",
      "epoch: 94, beta=0, frac_anom=0.1\n",
      "Train Epoch: 95 [0/8040 (0%)]\tLoss: 20.439209\n",
      "Train Epoch: 95 [1200/8040 (15%)]\tLoss: 20.879675\n",
      "Train Epoch: 95 [2400/8040 (30%)]\tLoss: 20.045341\n",
      "Train Epoch: 95 [3600/8040 (45%)]\tLoss: 20.410156\n",
      "Train Epoch: 95 [4800/8040 (60%)]\tLoss: 20.296065\n",
      "Train Epoch: 95 [6000/8040 (75%)]\tLoss: 20.253882\n",
      "Train Epoch: 95 [7200/8040 (90%)]\tLoss: 20.641121\n",
      "====> Epoch: 95 Average loss: 20.3490\n",
      "epoch: 95, beta=0, frac_anom=0.1\n",
      "Train Epoch: 96 [0/8040 (0%)]\tLoss: 19.691638\n",
      "Train Epoch: 96 [1200/8040 (15%)]\tLoss: 20.114897\n",
      "Train Epoch: 96 [2400/8040 (30%)]\tLoss: 20.158024\n",
      "Train Epoch: 96 [3600/8040 (45%)]\tLoss: 20.834670\n",
      "Train Epoch: 96 [4800/8040 (60%)]\tLoss: 19.180265\n",
      "Train Epoch: 96 [6000/8040 (75%)]\tLoss: 19.716121\n",
      "Train Epoch: 96 [7200/8040 (90%)]\tLoss: 20.555591\n",
      "====> Epoch: 96 Average loss: 20.3146\n",
      "epoch: 96, beta=0, frac_anom=0.1\n",
      "Train Epoch: 97 [0/8040 (0%)]\tLoss: 19.723177\n",
      "Train Epoch: 97 [1200/8040 (15%)]\tLoss: 20.313837\n",
      "Train Epoch: 97 [2400/8040 (30%)]\tLoss: 20.617896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 97 [3600/8040 (45%)]\tLoss: 19.674585\n",
      "Train Epoch: 97 [4800/8040 (60%)]\tLoss: 19.204761\n",
      "Train Epoch: 97 [6000/8040 (75%)]\tLoss: 19.982062\n",
      "Train Epoch: 97 [7200/8040 (90%)]\tLoss: 19.581042\n",
      "====> Epoch: 97 Average loss: 20.2726\n",
      "epoch: 97, beta=0, frac_anom=0.1\n",
      "Train Epoch: 98 [0/8040 (0%)]\tLoss: 20.995028\n",
      "Train Epoch: 98 [1200/8040 (15%)]\tLoss: 20.742049\n",
      "Train Epoch: 98 [2400/8040 (30%)]\tLoss: 20.967503\n",
      "Train Epoch: 98 [3600/8040 (45%)]\tLoss: 20.777024\n",
      "Train Epoch: 98 [4800/8040 (60%)]\tLoss: 21.072274\n",
      "Train Epoch: 98 [6000/8040 (75%)]\tLoss: 20.938869\n",
      "Train Epoch: 98 [7200/8040 (90%)]\tLoss: 19.640316\n",
      "====> Epoch: 98 Average loss: 20.2644\n",
      "epoch: 98, beta=0, frac_anom=0.1\n",
      "Train Epoch: 99 [0/8040 (0%)]\tLoss: 21.027651\n",
      "Train Epoch: 99 [1200/8040 (15%)]\tLoss: 19.912809\n",
      "Train Epoch: 99 [2400/8040 (30%)]\tLoss: 19.882861\n",
      "Train Epoch: 99 [3600/8040 (45%)]\tLoss: 19.743121\n",
      "Train Epoch: 99 [4800/8040 (60%)]\tLoss: 20.588847\n",
      "Train Epoch: 99 [6000/8040 (75%)]\tLoss: 20.140000\n",
      "Train Epoch: 99 [7200/8040 (90%)]\tLoss: 19.926506\n",
      "====> Epoch: 99 Average loss: 20.2355\n",
      "epoch: 99, beta=0, frac_anom=0.1\n",
      "Train Epoch: 100 [0/8040 (0%)]\tLoss: 20.185136\n",
      "Train Epoch: 100 [1200/8040 (15%)]\tLoss: 20.008956\n",
      "Train Epoch: 100 [2400/8040 (30%)]\tLoss: 18.636462\n",
      "Train Epoch: 100 [3600/8040 (45%)]\tLoss: 20.400551\n",
      "Train Epoch: 100 [4800/8040 (60%)]\tLoss: 20.897091\n",
      "Train Epoch: 100 [6000/8040 (75%)]\tLoss: 19.627256\n",
      "Train Epoch: 100 [7200/8040 (90%)]\tLoss: 20.921041\n",
      "====> Epoch: 100 Average loss: 20.2389\n",
      "epoch: 100, beta=0, frac_anom=0.1\n",
      "Train Epoch: 101 [0/8040 (0%)]\tLoss: 21.511143\n",
      "Train Epoch: 101 [1200/8040 (15%)]\tLoss: 20.594088\n",
      "Train Epoch: 101 [2400/8040 (30%)]\tLoss: 19.913458\n",
      "Train Epoch: 101 [3600/8040 (45%)]\tLoss: 20.604924\n",
      "Train Epoch: 101 [4800/8040 (60%)]\tLoss: 20.317924\n",
      "Train Epoch: 101 [6000/8040 (75%)]\tLoss: 18.847786\n",
      "Train Epoch: 101 [7200/8040 (90%)]\tLoss: 19.726335\n",
      "====> Epoch: 101 Average loss: 20.2113\n",
      "epoch: 101, beta=0, frac_anom=0.1\n",
      "Train Epoch: 102 [0/8040 (0%)]\tLoss: 19.150818\n",
      "Train Epoch: 102 [1200/8040 (15%)]\tLoss: 20.567468\n",
      "Train Epoch: 102 [2400/8040 (30%)]\tLoss: 20.085518\n",
      "Train Epoch: 102 [3600/8040 (45%)]\tLoss: 21.130713\n",
      "Train Epoch: 102 [4800/8040 (60%)]\tLoss: 20.207194\n",
      "Train Epoch: 102 [6000/8040 (75%)]\tLoss: 19.702108\n",
      "Train Epoch: 102 [7200/8040 (90%)]\tLoss: 21.075985\n",
      "====> Epoch: 102 Average loss: 20.2472\n",
      "epoch: 102, beta=0, frac_anom=0.1\n",
      "Train Epoch: 103 [0/8040 (0%)]\tLoss: 20.947994\n",
      "Train Epoch: 103 [1200/8040 (15%)]\tLoss: 20.520583\n",
      "Train Epoch: 103 [2400/8040 (30%)]\tLoss: 20.348181\n",
      "Train Epoch: 103 [3600/8040 (45%)]\tLoss: 20.765047\n",
      "Train Epoch: 103 [4800/8040 (60%)]\tLoss: 20.421558\n",
      "Train Epoch: 103 [6000/8040 (75%)]\tLoss: 19.675828\n",
      "Train Epoch: 103 [7200/8040 (90%)]\tLoss: 20.454490\n",
      "====> Epoch: 103 Average loss: 20.1964\n",
      "epoch: 103, beta=0, frac_anom=0.1\n",
      "Train Epoch: 104 [0/8040 (0%)]\tLoss: 21.666933\n",
      "Train Epoch: 104 [1200/8040 (15%)]\tLoss: 19.314211\n",
      "Train Epoch: 104 [2400/8040 (30%)]\tLoss: 20.019843\n",
      "Train Epoch: 104 [3600/8040 (45%)]\tLoss: 20.423893\n",
      "Train Epoch: 104 [4800/8040 (60%)]\tLoss: 19.836208\n",
      "Train Epoch: 104 [6000/8040 (75%)]\tLoss: 21.117456\n",
      "Train Epoch: 104 [7200/8040 (90%)]\tLoss: 20.106392\n",
      "====> Epoch: 104 Average loss: 20.2187\n",
      "epoch: 104, beta=0, frac_anom=0.1\n",
      "Train Epoch: 105 [0/8040 (0%)]\tLoss: 20.450305\n",
      "Train Epoch: 105 [1200/8040 (15%)]\tLoss: 20.051031\n",
      "Train Epoch: 105 [2400/8040 (30%)]\tLoss: 20.914347\n",
      "Train Epoch: 105 [3600/8040 (45%)]\tLoss: 20.003202\n",
      "Train Epoch: 105 [4800/8040 (60%)]\tLoss: 19.283097\n",
      "Train Epoch: 105 [6000/8040 (75%)]\tLoss: 20.264473\n",
      "Train Epoch: 105 [7200/8040 (90%)]\tLoss: 20.757137\n",
      "====> Epoch: 105 Average loss: 20.1615\n",
      "epoch: 105, beta=0, frac_anom=0.1\n",
      "Train Epoch: 106 [0/8040 (0%)]\tLoss: 20.829820\n",
      "Train Epoch: 106 [1200/8040 (15%)]\tLoss: 20.472089\n",
      "Train Epoch: 106 [2400/8040 (30%)]\tLoss: 21.060935\n",
      "Train Epoch: 106 [3600/8040 (45%)]\tLoss: 19.752795\n",
      "Train Epoch: 106 [4800/8040 (60%)]\tLoss: 20.257279\n",
      "Train Epoch: 106 [6000/8040 (75%)]\tLoss: 20.722030\n",
      "Train Epoch: 106 [7200/8040 (90%)]\tLoss: 19.982735\n",
      "====> Epoch: 106 Average loss: 20.1854\n",
      "epoch: 106, beta=0, frac_anom=0.1\n",
      "Train Epoch: 107 [0/8040 (0%)]\tLoss: 20.821114\n",
      "Train Epoch: 107 [1200/8040 (15%)]\tLoss: 19.830192\n",
      "Train Epoch: 107 [2400/8040 (30%)]\tLoss: 20.111743\n",
      "Train Epoch: 107 [3600/8040 (45%)]\tLoss: 20.796212\n",
      "Train Epoch: 107 [4800/8040 (60%)]\tLoss: 19.322662\n",
      "Train Epoch: 107 [6000/8040 (75%)]\tLoss: 20.460327\n",
      "Train Epoch: 107 [7200/8040 (90%)]\tLoss: 20.912000\n",
      "====> Epoch: 107 Average loss: 20.1792\n",
      "epoch: 107, beta=0, frac_anom=0.1\n",
      "Train Epoch: 108 [0/8040 (0%)]\tLoss: 20.616459\n",
      "Train Epoch: 108 [1200/8040 (15%)]\tLoss: 19.736011\n",
      "Train Epoch: 108 [2400/8040 (30%)]\tLoss: 20.281783\n",
      "Train Epoch: 108 [3600/8040 (45%)]\tLoss: 20.447675\n",
      "Train Epoch: 108 [4800/8040 (60%)]\tLoss: 20.915792\n",
      "Train Epoch: 108 [6000/8040 (75%)]\tLoss: 19.845386\n",
      "Train Epoch: 108 [7200/8040 (90%)]\tLoss: 20.905882\n",
      "====> Epoch: 108 Average loss: 20.1471\n",
      "epoch: 108, beta=0, frac_anom=0.1\n",
      "Train Epoch: 109 [0/8040 (0%)]\tLoss: 21.228772\n",
      "Train Epoch: 109 [1200/8040 (15%)]\tLoss: 20.605538\n",
      "Train Epoch: 109 [2400/8040 (30%)]\tLoss: 20.966056\n",
      "Train Epoch: 109 [3600/8040 (45%)]\tLoss: 18.609188\n",
      "Train Epoch: 109 [4800/8040 (60%)]\tLoss: 20.117120\n",
      "Train Epoch: 109 [6000/8040 (75%)]\tLoss: 21.361515\n",
      "Train Epoch: 109 [7200/8040 (90%)]\tLoss: 19.877622\n",
      "====> Epoch: 109 Average loss: 20.1649\n",
      "epoch: 109, beta=0, frac_anom=0.1\n",
      "Train Epoch: 110 [0/8040 (0%)]\tLoss: 19.346468\n",
      "Train Epoch: 110 [1200/8040 (15%)]\tLoss: 20.952264\n",
      "Train Epoch: 110 [2400/8040 (30%)]\tLoss: 20.747618\n",
      "Train Epoch: 110 [3600/8040 (45%)]\tLoss: 20.829639\n",
      "Train Epoch: 110 [4800/8040 (60%)]\tLoss: 19.622205\n",
      "Train Epoch: 110 [6000/8040 (75%)]\tLoss: 20.670056\n",
      "Train Epoch: 110 [7200/8040 (90%)]\tLoss: 20.419637\n",
      "====> Epoch: 110 Average loss: 20.1918\n",
      "epoch: 110, beta=0, frac_anom=0.1\n",
      "Train Epoch: 111 [0/8040 (0%)]\tLoss: 21.555656\n",
      "Train Epoch: 111 [1200/8040 (15%)]\tLoss: 19.889937\n",
      "Train Epoch: 111 [2400/8040 (30%)]\tLoss: 21.305231\n",
      "Train Epoch: 111 [3600/8040 (45%)]\tLoss: 19.156618\n",
      "Train Epoch: 111 [4800/8040 (60%)]\tLoss: 20.212842\n",
      "Train Epoch: 111 [6000/8040 (75%)]\tLoss: 20.052696\n",
      "Train Epoch: 111 [7200/8040 (90%)]\tLoss: 18.934965\n",
      "====> Epoch: 111 Average loss: 20.1458\n",
      "epoch: 111, beta=0, frac_anom=0.1\n",
      "Train Epoch: 112 [0/8040 (0%)]\tLoss: 20.527148\n",
      "Train Epoch: 112 [1200/8040 (15%)]\tLoss: 20.559721\n",
      "Train Epoch: 112 [2400/8040 (30%)]\tLoss: 19.801261\n",
      "Train Epoch: 112 [3600/8040 (45%)]\tLoss: 19.357808\n",
      "Train Epoch: 112 [4800/8040 (60%)]\tLoss: 20.716425\n",
      "Train Epoch: 112 [6000/8040 (75%)]\tLoss: 20.178642\n",
      "Train Epoch: 112 [7200/8040 (90%)]\tLoss: 18.646493\n",
      "====> Epoch: 112 Average loss: 20.1178\n",
      "epoch: 112, beta=0, frac_anom=0.1\n",
      "Train Epoch: 113 [0/8040 (0%)]\tLoss: 20.035693\n",
      "Train Epoch: 113 [1200/8040 (15%)]\tLoss: 20.517283\n",
      "Train Epoch: 113 [2400/8040 (30%)]\tLoss: 21.400260\n",
      "Train Epoch: 113 [3600/8040 (45%)]\tLoss: 19.995490\n",
      "Train Epoch: 113 [4800/8040 (60%)]\tLoss: 19.654567\n",
      "Train Epoch: 113 [6000/8040 (75%)]\tLoss: 20.969265\n",
      "Train Epoch: 113 [7200/8040 (90%)]\tLoss: 20.534717\n",
      "====> Epoch: 113 Average loss: 20.1696\n",
      "epoch: 113, beta=0, frac_anom=0.1\n",
      "Train Epoch: 114 [0/8040 (0%)]\tLoss: 18.823413\n",
      "Train Epoch: 114 [1200/8040 (15%)]\tLoss: 20.714602\n",
      "Train Epoch: 114 [2400/8040 (30%)]\tLoss: 21.549888\n",
      "Train Epoch: 114 [3600/8040 (45%)]\tLoss: 20.471973\n",
      "Train Epoch: 114 [4800/8040 (60%)]\tLoss: 20.580216\n",
      "Train Epoch: 114 [6000/8040 (75%)]\tLoss: 20.165906\n",
      "Train Epoch: 114 [7200/8040 (90%)]\tLoss: 19.850370\n",
      "====> Epoch: 114 Average loss: 20.1247\n",
      "epoch: 114, beta=0, frac_anom=0.1\n",
      "Train Epoch: 115 [0/8040 (0%)]\tLoss: 20.124264\n",
      "Train Epoch: 115 [1200/8040 (15%)]\tLoss: 19.678467\n",
      "Train Epoch: 115 [2400/8040 (30%)]\tLoss: 19.140426\n",
      "Train Epoch: 115 [3600/8040 (45%)]\tLoss: 19.982194\n",
      "Train Epoch: 115 [4800/8040 (60%)]\tLoss: 19.636098\n",
      "Train Epoch: 115 [6000/8040 (75%)]\tLoss: 19.989921\n",
      "Train Epoch: 115 [7200/8040 (90%)]\tLoss: 19.288544\n",
      "====> Epoch: 115 Average loss: 20.0954\n",
      "epoch: 115, beta=0, frac_anom=0.1\n",
      "Train Epoch: 116 [0/8040 (0%)]\tLoss: 19.946027\n",
      "Train Epoch: 116 [1200/8040 (15%)]\tLoss: 20.158492\n",
      "Train Epoch: 116 [2400/8040 (30%)]\tLoss: 19.787742\n",
      "Train Epoch: 116 [3600/8040 (45%)]\tLoss: 19.720591\n",
      "Train Epoch: 116 [4800/8040 (60%)]\tLoss: 19.196110\n",
      "Train Epoch: 116 [6000/8040 (75%)]\tLoss: 20.382996\n",
      "Train Epoch: 116 [7200/8040 (90%)]\tLoss: 19.711593\n",
      "====> Epoch: 116 Average loss: 20.1468\n",
      "epoch: 116, beta=0, frac_anom=0.1\n",
      "Train Epoch: 117 [0/8040 (0%)]\tLoss: 19.546379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 117 [1200/8040 (15%)]\tLoss: 19.746643\n",
      "Train Epoch: 117 [2400/8040 (30%)]\tLoss: 19.236312\n",
      "Train Epoch: 117 [3600/8040 (45%)]\tLoss: 20.251070\n",
      "Train Epoch: 117 [4800/8040 (60%)]\tLoss: 19.846051\n",
      "Train Epoch: 117 [6000/8040 (75%)]\tLoss: 18.643953\n",
      "Train Epoch: 117 [7200/8040 (90%)]\tLoss: 18.953654\n",
      "====> Epoch: 117 Average loss: 20.0554\n",
      "epoch: 117, beta=0, frac_anom=0.1\n",
      "Train Epoch: 118 [0/8040 (0%)]\tLoss: 20.201475\n",
      "Train Epoch: 118 [1200/8040 (15%)]\tLoss: 19.784229\n",
      "Train Epoch: 118 [2400/8040 (30%)]\tLoss: 19.914537\n",
      "Train Epoch: 118 [3600/8040 (45%)]\tLoss: 19.780485\n",
      "Train Epoch: 118 [4800/8040 (60%)]\tLoss: 20.302669\n",
      "Train Epoch: 118 [6000/8040 (75%)]\tLoss: 21.345186\n",
      "Train Epoch: 118 [7200/8040 (90%)]\tLoss: 20.332926\n",
      "====> Epoch: 118 Average loss: 20.0776\n",
      "epoch: 118, beta=0, frac_anom=0.1\n",
      "Train Epoch: 119 [0/8040 (0%)]\tLoss: 20.735913\n",
      "Train Epoch: 119 [1200/8040 (15%)]\tLoss: 20.523311\n",
      "Train Epoch: 119 [2400/8040 (30%)]\tLoss: 20.299264\n",
      "Train Epoch: 119 [3600/8040 (45%)]\tLoss: 20.659440\n",
      "Train Epoch: 119 [4800/8040 (60%)]\tLoss: 20.058514\n",
      "Train Epoch: 119 [6000/8040 (75%)]\tLoss: 20.229114\n",
      "Train Epoch: 119 [7200/8040 (90%)]\tLoss: 19.263985\n",
      "====> Epoch: 119 Average loss: 20.0777\n",
      "epoch: 119, beta=0, frac_anom=0.1\n",
      "Train Epoch: 120 [0/8040 (0%)]\tLoss: 19.257141\n",
      "Train Epoch: 120 [1200/8040 (15%)]\tLoss: 19.865149\n",
      "Train Epoch: 120 [2400/8040 (30%)]\tLoss: 20.589671\n",
      "Train Epoch: 120 [3600/8040 (45%)]\tLoss: 19.614848\n",
      "Train Epoch: 120 [4800/8040 (60%)]\tLoss: 19.598175\n",
      "Train Epoch: 120 [6000/8040 (75%)]\tLoss: 20.614834\n",
      "Train Epoch: 120 [7200/8040 (90%)]\tLoss: 19.462581\n",
      "====> Epoch: 120 Average loss: 20.0901\n",
      "epoch: 120, beta=0, frac_anom=0.1\n",
      "Train Epoch: 121 [0/8040 (0%)]\tLoss: 18.366081\n",
      "Train Epoch: 121 [1200/8040 (15%)]\tLoss: 19.630953\n",
      "Train Epoch: 121 [2400/8040 (30%)]\tLoss: 19.218941\n",
      "Train Epoch: 121 [3600/8040 (45%)]\tLoss: 20.906099\n",
      "Train Epoch: 121 [4800/8040 (60%)]\tLoss: 20.600940\n",
      "Train Epoch: 121 [6000/8040 (75%)]\tLoss: 18.430778\n",
      "Train Epoch: 121 [7200/8040 (90%)]\tLoss: 20.357550\n",
      "====> Epoch: 121 Average loss: 20.0990\n",
      "epoch: 121, beta=0, frac_anom=0.1\n",
      "Train Epoch: 122 [0/8040 (0%)]\tLoss: 19.705322\n",
      "Train Epoch: 122 [1200/8040 (15%)]\tLoss: 19.205298\n",
      "Train Epoch: 122 [2400/8040 (30%)]\tLoss: 19.839616\n",
      "Train Epoch: 122 [3600/8040 (45%)]\tLoss: 19.881340\n",
      "Train Epoch: 122 [4800/8040 (60%)]\tLoss: 20.726379\n",
      "Train Epoch: 122 [6000/8040 (75%)]\tLoss: 20.311548\n",
      "Train Epoch: 122 [7200/8040 (90%)]\tLoss: 20.472888\n",
      "====> Epoch: 122 Average loss: 20.0541\n",
      "epoch: 122, beta=0, frac_anom=0.1\n",
      "Train Epoch: 123 [0/8040 (0%)]\tLoss: 19.747465\n",
      "Train Epoch: 123 [1200/8040 (15%)]\tLoss: 18.359676\n",
      "Train Epoch: 123 [2400/8040 (30%)]\tLoss: 18.550517\n",
      "Train Epoch: 123 [3600/8040 (45%)]\tLoss: 20.365013\n",
      "Train Epoch: 123 [4800/8040 (60%)]\tLoss: 19.564022\n",
      "Train Epoch: 123 [6000/8040 (75%)]\tLoss: 19.318158\n",
      "Train Epoch: 123 [7200/8040 (90%)]\tLoss: 20.593201\n",
      "====> Epoch: 123 Average loss: 20.1003\n",
      "epoch: 123, beta=0, frac_anom=0.1\n",
      "Train Epoch: 124 [0/8040 (0%)]\tLoss: 19.833594\n",
      "Train Epoch: 124 [1200/8040 (15%)]\tLoss: 21.351933\n",
      "Train Epoch: 124 [2400/8040 (30%)]\tLoss: 19.236877\n",
      "Train Epoch: 124 [3600/8040 (45%)]\tLoss: 19.388717\n",
      "Train Epoch: 124 [4800/8040 (60%)]\tLoss: 20.524015\n",
      "Train Epoch: 124 [6000/8040 (75%)]\tLoss: 19.896456\n",
      "Train Epoch: 124 [7200/8040 (90%)]\tLoss: 18.967330\n",
      "====> Epoch: 124 Average loss: 20.0776\n",
      "epoch: 124, beta=0, frac_anom=0.1\n",
      "Train Epoch: 125 [0/8040 (0%)]\tLoss: 20.592015\n",
      "Train Epoch: 125 [1200/8040 (15%)]\tLoss: 19.924520\n",
      "Train Epoch: 125 [2400/8040 (30%)]\tLoss: 20.114465\n",
      "Train Epoch: 125 [3600/8040 (45%)]\tLoss: 20.227861\n",
      "Train Epoch: 125 [4800/8040 (60%)]\tLoss: 20.532298\n",
      "Train Epoch: 125 [6000/8040 (75%)]\tLoss: 20.114364\n",
      "Train Epoch: 125 [7200/8040 (90%)]\tLoss: 19.828194\n",
      "====> Epoch: 125 Average loss: 20.0500\n",
      "epoch: 125, beta=0, frac_anom=0.1\n",
      "Train Epoch: 126 [0/8040 (0%)]\tLoss: 19.975488\n",
      "Train Epoch: 126 [1200/8040 (15%)]\tLoss: 19.024007\n",
      "Train Epoch: 126 [2400/8040 (30%)]\tLoss: 19.632837\n",
      "Train Epoch: 126 [3600/8040 (45%)]\tLoss: 20.886479\n",
      "Train Epoch: 126 [4800/8040 (60%)]\tLoss: 19.986873\n",
      "Train Epoch: 126 [6000/8040 (75%)]\tLoss: 19.179865\n",
      "Train Epoch: 126 [7200/8040 (90%)]\tLoss: 18.888383\n",
      "====> Epoch: 126 Average loss: 20.0642\n",
      "epoch: 126, beta=0, frac_anom=0.1\n",
      "Train Epoch: 127 [0/8040 (0%)]\tLoss: 20.100201\n",
      "Train Epoch: 127 [1200/8040 (15%)]\tLoss: 20.077006\n",
      "Train Epoch: 127 [2400/8040 (30%)]\tLoss: 20.352006\n",
      "Train Epoch: 127 [3600/8040 (45%)]\tLoss: 20.109615\n",
      "Train Epoch: 127 [4800/8040 (60%)]\tLoss: 19.660748\n",
      "Train Epoch: 127 [6000/8040 (75%)]\tLoss: 20.507300\n",
      "Train Epoch: 127 [7200/8040 (90%)]\tLoss: 20.848857\n",
      "====> Epoch: 127 Average loss: 20.0394\n",
      "epoch: 127, beta=0, frac_anom=0.1\n",
      "Train Epoch: 128 [0/8040 (0%)]\tLoss: 21.272262\n",
      "Train Epoch: 128 [1200/8040 (15%)]\tLoss: 20.576766\n",
      "Train Epoch: 128 [2400/8040 (30%)]\tLoss: 18.910177\n",
      "Train Epoch: 128 [3600/8040 (45%)]\tLoss: 19.283832\n",
      "Train Epoch: 128 [4800/8040 (60%)]\tLoss: 19.318429\n",
      "Train Epoch: 128 [6000/8040 (75%)]\tLoss: 20.367328\n",
      "Train Epoch: 128 [7200/8040 (90%)]\tLoss: 20.367147\n",
      "====> Epoch: 128 Average loss: 20.0035\n",
      "epoch: 128, beta=0, frac_anom=0.1\n",
      "Train Epoch: 129 [0/8040 (0%)]\tLoss: 18.934047\n",
      "Train Epoch: 129 [1200/8040 (15%)]\tLoss: 19.594824\n",
      "Train Epoch: 129 [2400/8040 (30%)]\tLoss: 19.736326\n",
      "Train Epoch: 129 [3600/8040 (45%)]\tLoss: 20.328497\n",
      "Train Epoch: 129 [4800/8040 (60%)]\tLoss: 20.164836\n",
      "Train Epoch: 129 [6000/8040 (75%)]\tLoss: 19.717466\n",
      "Train Epoch: 129 [7200/8040 (90%)]\tLoss: 20.270058\n",
      "====> Epoch: 129 Average loss: 20.0153\n",
      "epoch: 129, beta=0, frac_anom=0.1\n",
      "Train Epoch: 130 [0/8040 (0%)]\tLoss: 19.288271\n",
      "Train Epoch: 130 [1200/8040 (15%)]\tLoss: 19.740194\n",
      "Train Epoch: 130 [2400/8040 (30%)]\tLoss: 20.002582\n",
      "Train Epoch: 130 [3600/8040 (45%)]\tLoss: 20.485669\n",
      "Train Epoch: 130 [4800/8040 (60%)]\tLoss: 20.030367\n",
      "Train Epoch: 130 [6000/8040 (75%)]\tLoss: 19.620614\n",
      "Train Epoch: 130 [7200/8040 (90%)]\tLoss: 20.071163\n",
      "====> Epoch: 130 Average loss: 20.0511\n",
      "epoch: 130, beta=0, frac_anom=0.1\n",
      "Train Epoch: 131 [0/8040 (0%)]\tLoss: 19.101009\n",
      "Train Epoch: 131 [1200/8040 (15%)]\tLoss: 20.065979\n",
      "Train Epoch: 131 [2400/8040 (30%)]\tLoss: 19.605501\n",
      "Train Epoch: 131 [3600/8040 (45%)]\tLoss: 20.981250\n",
      "Train Epoch: 131 [4800/8040 (60%)]\tLoss: 20.183736\n",
      "Train Epoch: 131 [6000/8040 (75%)]\tLoss: 18.887050\n",
      "Train Epoch: 131 [7200/8040 (90%)]\tLoss: 19.958628\n",
      "====> Epoch: 131 Average loss: 20.0518\n",
      "epoch: 131, beta=0, frac_anom=0.1\n",
      "Train Epoch: 132 [0/8040 (0%)]\tLoss: 20.164278\n",
      "Train Epoch: 132 [1200/8040 (15%)]\tLoss: 20.187988\n",
      "Train Epoch: 132 [2400/8040 (30%)]\tLoss: 19.385624\n",
      "Train Epoch: 132 [3600/8040 (45%)]\tLoss: 20.876752\n",
      "Train Epoch: 132 [4800/8040 (60%)]\tLoss: 20.029716\n",
      "Train Epoch: 132 [6000/8040 (75%)]\tLoss: 19.937260\n",
      "Train Epoch: 132 [7200/8040 (90%)]\tLoss: 20.570028\n",
      "====> Epoch: 132 Average loss: 20.0223\n",
      "epoch: 132, beta=0, frac_anom=0.1\n",
      "Train Epoch: 133 [0/8040 (0%)]\tLoss: 19.489555\n",
      "Train Epoch: 133 [1200/8040 (15%)]\tLoss: 19.024790\n",
      "Train Epoch: 133 [2400/8040 (30%)]\tLoss: 20.240464\n",
      "Train Epoch: 133 [3600/8040 (45%)]\tLoss: 19.993050\n",
      "Train Epoch: 133 [4800/8040 (60%)]\tLoss: 19.820150\n",
      "Train Epoch: 133 [6000/8040 (75%)]\tLoss: 20.273069\n",
      "Train Epoch: 133 [7200/8040 (90%)]\tLoss: 19.947402\n",
      "====> Epoch: 133 Average loss: 19.9932\n",
      "epoch: 133, beta=0, frac_anom=0.1\n",
      "Train Epoch: 134 [0/8040 (0%)]\tLoss: 20.956435\n",
      "Train Epoch: 134 [1200/8040 (15%)]\tLoss: 20.352702\n",
      "Train Epoch: 134 [2400/8040 (30%)]\tLoss: 19.089897\n",
      "Train Epoch: 134 [3600/8040 (45%)]\tLoss: 19.885655\n",
      "Train Epoch: 134 [4800/8040 (60%)]\tLoss: 19.423224\n",
      "Train Epoch: 134 [6000/8040 (75%)]\tLoss: 18.736387\n",
      "Train Epoch: 134 [7200/8040 (90%)]\tLoss: 20.715572\n",
      "====> Epoch: 134 Average loss: 19.9846\n",
      "epoch: 134, beta=0, frac_anom=0.1\n",
      "Train Epoch: 135 [0/8040 (0%)]\tLoss: 19.418783\n",
      "Train Epoch: 135 [1200/8040 (15%)]\tLoss: 19.941504\n",
      "Train Epoch: 135 [2400/8040 (30%)]\tLoss: 21.114290\n",
      "Train Epoch: 135 [3600/8040 (45%)]\tLoss: 20.911123\n",
      "Train Epoch: 135 [4800/8040 (60%)]\tLoss: 20.137065\n",
      "Train Epoch: 135 [6000/8040 (75%)]\tLoss: 19.278625\n",
      "Train Epoch: 135 [7200/8040 (90%)]\tLoss: 20.477378\n",
      "====> Epoch: 135 Average loss: 19.9740\n",
      "epoch: 135, beta=0, frac_anom=0.1\n",
      "Train Epoch: 136 [0/8040 (0%)]\tLoss: 20.672632\n",
      "Train Epoch: 136 [1200/8040 (15%)]\tLoss: 19.699534\n",
      "Train Epoch: 136 [2400/8040 (30%)]\tLoss: 20.024833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 136 [3600/8040 (45%)]\tLoss: 19.911434\n",
      "Train Epoch: 136 [4800/8040 (60%)]\tLoss: 20.234945\n",
      "Train Epoch: 136 [6000/8040 (75%)]\tLoss: 20.271513\n",
      "Train Epoch: 136 [7200/8040 (90%)]\tLoss: 20.399453\n",
      "====> Epoch: 136 Average loss: 20.0411\n",
      "epoch: 136, beta=0, frac_anom=0.1\n",
      "Train Epoch: 137 [0/8040 (0%)]\tLoss: 20.800993\n",
      "Train Epoch: 137 [1200/8040 (15%)]\tLoss: 19.149377\n",
      "Train Epoch: 137 [2400/8040 (30%)]\tLoss: 19.927704\n",
      "Train Epoch: 137 [3600/8040 (45%)]\tLoss: 20.668412\n",
      "Train Epoch: 137 [4800/8040 (60%)]\tLoss: 20.760327\n",
      "Train Epoch: 137 [6000/8040 (75%)]\tLoss: 20.579789\n",
      "Train Epoch: 137 [7200/8040 (90%)]\tLoss: 19.812899\n",
      "====> Epoch: 137 Average loss: 20.0156\n",
      "epoch: 137, beta=0, frac_anom=0.1\n",
      "Train Epoch: 138 [0/8040 (0%)]\tLoss: 20.411414\n",
      "Train Epoch: 138 [1200/8040 (15%)]\tLoss: 20.759914\n",
      "Train Epoch: 138 [2400/8040 (30%)]\tLoss: 20.806685\n",
      "Train Epoch: 138 [3600/8040 (45%)]\tLoss: 19.659678\n",
      "Train Epoch: 138 [4800/8040 (60%)]\tLoss: 20.116644\n",
      "Train Epoch: 138 [6000/8040 (75%)]\tLoss: 20.154388\n",
      "Train Epoch: 138 [7200/8040 (90%)]\tLoss: 20.536542\n",
      "====> Epoch: 138 Average loss: 20.0488\n",
      "epoch: 138, beta=0, frac_anom=0.1\n",
      "Train Epoch: 139 [0/8040 (0%)]\tLoss: 20.002698\n",
      "Train Epoch: 139 [1200/8040 (15%)]\tLoss: 19.627222\n",
      "Train Epoch: 139 [2400/8040 (30%)]\tLoss: 19.159831\n",
      "Train Epoch: 139 [3600/8040 (45%)]\tLoss: 20.381128\n",
      "Train Epoch: 139 [4800/8040 (60%)]\tLoss: 20.403591\n",
      "Train Epoch: 139 [6000/8040 (75%)]\tLoss: 20.519012\n",
      "Train Epoch: 139 [7200/8040 (90%)]\tLoss: 19.975557\n",
      "====> Epoch: 139 Average loss: 20.0023\n",
      "epoch: 139, beta=0, frac_anom=0.1\n",
      "Train Epoch: 140 [0/8040 (0%)]\tLoss: 20.892702\n",
      "Train Epoch: 140 [1200/8040 (15%)]\tLoss: 21.439473\n",
      "Train Epoch: 140 [2400/8040 (30%)]\tLoss: 19.957987\n",
      "Train Epoch: 140 [3600/8040 (45%)]\tLoss: 19.953035\n",
      "Train Epoch: 140 [4800/8040 (60%)]\tLoss: 21.604818\n",
      "Train Epoch: 140 [6000/8040 (75%)]\tLoss: 19.757259\n",
      "Train Epoch: 140 [7200/8040 (90%)]\tLoss: 20.165743\n",
      "====> Epoch: 140 Average loss: 19.9696\n",
      "epoch: 140, beta=0, frac_anom=0.1\n",
      "Train Epoch: 141 [0/8040 (0%)]\tLoss: 19.370516\n",
      "Train Epoch: 141 [1200/8040 (15%)]\tLoss: 19.794820\n",
      "Train Epoch: 141 [2400/8040 (30%)]\tLoss: 19.245310\n",
      "Train Epoch: 141 [3600/8040 (45%)]\tLoss: 20.340306\n",
      "Train Epoch: 141 [4800/8040 (60%)]\tLoss: 19.258645\n",
      "Train Epoch: 141 [6000/8040 (75%)]\tLoss: 19.695357\n",
      "Train Epoch: 141 [7200/8040 (90%)]\tLoss: 18.549268\n",
      "====> Epoch: 141 Average loss: 19.9550\n",
      "epoch: 141, beta=0, frac_anom=0.1\n",
      "Train Epoch: 142 [0/8040 (0%)]\tLoss: 20.362954\n",
      "Train Epoch: 142 [1200/8040 (15%)]\tLoss: 20.665710\n",
      "Train Epoch: 142 [2400/8040 (30%)]\tLoss: 19.744289\n",
      "Train Epoch: 142 [3600/8040 (45%)]\tLoss: 19.328752\n",
      "Train Epoch: 142 [4800/8040 (60%)]\tLoss: 20.047750\n",
      "Train Epoch: 142 [6000/8040 (75%)]\tLoss: 19.772715\n",
      "Train Epoch: 142 [7200/8040 (90%)]\tLoss: 20.082876\n",
      "====> Epoch: 142 Average loss: 19.9831\n",
      "epoch: 142, beta=0, frac_anom=0.1\n",
      "Train Epoch: 143 [0/8040 (0%)]\tLoss: 18.967615\n",
      "Train Epoch: 143 [1200/8040 (15%)]\tLoss: 20.032528\n",
      "Train Epoch: 143 [2400/8040 (30%)]\tLoss: 19.290035\n",
      "Train Epoch: 143 [3600/8040 (45%)]\tLoss: 19.641899\n",
      "Train Epoch: 143 [4800/8040 (60%)]\tLoss: 20.483683\n",
      "Train Epoch: 143 [6000/8040 (75%)]\tLoss: 20.483173\n",
      "Train Epoch: 143 [7200/8040 (90%)]\tLoss: 20.558573\n",
      "====> Epoch: 143 Average loss: 19.9665\n",
      "epoch: 143, beta=0, frac_anom=0.1\n",
      "Train Epoch: 144 [0/8040 (0%)]\tLoss: 19.585858\n",
      "Train Epoch: 144 [1200/8040 (15%)]\tLoss: 21.070691\n",
      "Train Epoch: 144 [2400/8040 (30%)]\tLoss: 20.468209\n",
      "Train Epoch: 144 [3600/8040 (45%)]\tLoss: 20.483101\n",
      "Train Epoch: 144 [4800/8040 (60%)]\tLoss: 19.400230\n",
      "Train Epoch: 144 [6000/8040 (75%)]\tLoss: 20.021033\n",
      "Train Epoch: 144 [7200/8040 (90%)]\tLoss: 20.506596\n",
      "====> Epoch: 144 Average loss: 19.9601\n",
      "epoch: 144, beta=0, frac_anom=0.1\n",
      "Train Epoch: 145 [0/8040 (0%)]\tLoss: 19.325354\n",
      "Train Epoch: 145 [1200/8040 (15%)]\tLoss: 19.299939\n",
      "Train Epoch: 145 [2400/8040 (30%)]\tLoss: 20.508529\n",
      "Train Epoch: 145 [3600/8040 (45%)]\tLoss: 19.319820\n",
      "Train Epoch: 145 [4800/8040 (60%)]\tLoss: 20.975468\n",
      "Train Epoch: 145 [6000/8040 (75%)]\tLoss: 19.661509\n",
      "Train Epoch: 145 [7200/8040 (90%)]\tLoss: 19.758415\n",
      "====> Epoch: 145 Average loss: 19.9619\n",
      "epoch: 145, beta=0, frac_anom=0.1\n",
      "Train Epoch: 146 [0/8040 (0%)]\tLoss: 19.339376\n",
      "Train Epoch: 146 [1200/8040 (15%)]\tLoss: 20.220341\n",
      "Train Epoch: 146 [2400/8040 (30%)]\tLoss: 19.172640\n",
      "Train Epoch: 146 [3600/8040 (45%)]\tLoss: 20.293111\n",
      "Train Epoch: 146 [4800/8040 (60%)]\tLoss: 20.234212\n",
      "Train Epoch: 146 [6000/8040 (75%)]\tLoss: 19.838403\n",
      "Train Epoch: 146 [7200/8040 (90%)]\tLoss: 20.009229\n",
      "====> Epoch: 146 Average loss: 19.9714\n",
      "epoch: 146, beta=0, frac_anom=0.1\n",
      "Train Epoch: 147 [0/8040 (0%)]\tLoss: 20.361637\n",
      "Train Epoch: 147 [1200/8040 (15%)]\tLoss: 20.333162\n",
      "Train Epoch: 147 [2400/8040 (30%)]\tLoss: 20.303896\n",
      "Train Epoch: 147 [3600/8040 (45%)]\tLoss: 20.286902\n",
      "Train Epoch: 147 [4800/8040 (60%)]\tLoss: 20.323629\n",
      "Train Epoch: 147 [6000/8040 (75%)]\tLoss: 20.365092\n",
      "Train Epoch: 147 [7200/8040 (90%)]\tLoss: 20.090334\n",
      "====> Epoch: 147 Average loss: 19.9787\n",
      "epoch: 147, beta=0, frac_anom=0.1\n",
      "Train Epoch: 148 [0/8040 (0%)]\tLoss: 19.166418\n",
      "Train Epoch: 148 [1200/8040 (15%)]\tLoss: 19.387376\n",
      "Train Epoch: 148 [2400/8040 (30%)]\tLoss: 20.191443\n",
      "Train Epoch: 148 [3600/8040 (45%)]\tLoss: 19.719954\n",
      "Train Epoch: 148 [4800/8040 (60%)]\tLoss: 21.101506\n",
      "Train Epoch: 148 [6000/8040 (75%)]\tLoss: 19.559932\n",
      "Train Epoch: 148 [7200/8040 (90%)]\tLoss: 19.527468\n",
      "====> Epoch: 148 Average loss: 19.9674\n",
      "epoch: 148, beta=0, frac_anom=0.1\n",
      "Train Epoch: 149 [0/8040 (0%)]\tLoss: 19.892169\n",
      "Train Epoch: 149 [1200/8040 (15%)]\tLoss: 20.306986\n",
      "Train Epoch: 149 [2400/8040 (30%)]\tLoss: 20.587333\n",
      "Train Epoch: 149 [3600/8040 (45%)]\tLoss: 20.355656\n",
      "Train Epoch: 149 [4800/8040 (60%)]\tLoss: 18.674227\n",
      "Train Epoch: 149 [6000/8040 (75%)]\tLoss: 19.845286\n",
      "Train Epoch: 149 [7200/8040 (90%)]\tLoss: 21.479954\n",
      "====> Epoch: 149 Average loss: 20.0037\n",
      "epoch: 149, beta=0, frac_anom=0.1\n",
      "Train Epoch: 150 [0/8040 (0%)]\tLoss: 19.570762\n",
      "Train Epoch: 150 [1200/8040 (15%)]\tLoss: 19.981976\n",
      "Train Epoch: 150 [2400/8040 (30%)]\tLoss: 18.556836\n",
      "Train Epoch: 150 [3600/8040 (45%)]\tLoss: 19.634603\n",
      "Train Epoch: 150 [4800/8040 (60%)]\tLoss: 19.886623\n",
      "Train Epoch: 150 [6000/8040 (75%)]\tLoss: 18.841585\n",
      "Train Epoch: 150 [7200/8040 (90%)]\tLoss: 19.828105\n",
      "====> Epoch: 150 Average loss: 19.9276\n",
      "epoch: 150, beta=0, frac_anom=0.1\n",
      "====> Test set loss: 13.8032\n",
      "Train Epoch: 1 [0/8040 (0%)]\tLoss: 100.193937\n",
      "Train Epoch: 1 [1200/8040 (15%)]\tLoss: 95.725651\n",
      "Train Epoch: 1 [2400/8040 (30%)]\tLoss: 93.822925\n",
      "Train Epoch: 1 [3600/8040 (45%)]\tLoss: 93.937280\n",
      "Train Epoch: 1 [4800/8040 (60%)]\tLoss: 93.608748\n",
      "Train Epoch: 1 [6000/8040 (75%)]\tLoss: 93.828442\n",
      "Train Epoch: 1 [7200/8040 (90%)]\tLoss: 93.320410\n",
      "====> Epoch: 1 Average loss: 94.4076\n",
      "epoch: 1, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 2 [0/8040 (0%)]\tLoss: 93.246493\n",
      "Train Epoch: 2 [1200/8040 (15%)]\tLoss: 93.306250\n",
      "Train Epoch: 2 [2400/8040 (30%)]\tLoss: 93.223950\n",
      "Train Epoch: 2 [3600/8040 (45%)]\tLoss: 92.366585\n",
      "Train Epoch: 2 [4800/8040 (60%)]\tLoss: 93.074154\n",
      "Train Epoch: 2 [6000/8040 (75%)]\tLoss: 92.557381\n",
      "Train Epoch: 2 [7200/8040 (90%)]\tLoss: 92.919588\n",
      "====> Epoch: 2 Average loss: 92.9228\n",
      "epoch: 2, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 3 [0/8040 (0%)]\tLoss: 93.105387\n",
      "Train Epoch: 3 [1200/8040 (15%)]\tLoss: 92.738704\n",
      "Train Epoch: 3 [2400/8040 (30%)]\tLoss: 92.329647\n",
      "Train Epoch: 3 [3600/8040 (45%)]\tLoss: 92.836230\n",
      "Train Epoch: 3 [4800/8040 (60%)]\tLoss: 92.803174\n",
      "Train Epoch: 3 [6000/8040 (75%)]\tLoss: 92.584326\n",
      "Train Epoch: 3 [7200/8040 (90%)]\tLoss: 92.701546\n",
      "====> Epoch: 3 Average loss: 92.6498\n",
      "epoch: 3, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 4 [0/8040 (0%)]\tLoss: 92.664339\n",
      "Train Epoch: 4 [1200/8040 (15%)]\tLoss: 92.448136\n",
      "Train Epoch: 4 [2400/8040 (30%)]\tLoss: 92.504313\n",
      "Train Epoch: 4 [3600/8040 (45%)]\tLoss: 92.266382\n",
      "Train Epoch: 4 [4800/8040 (60%)]\tLoss: 92.428328\n",
      "Train Epoch: 4 [6000/8040 (75%)]\tLoss: 92.935856\n",
      "Train Epoch: 4 [7200/8040 (90%)]\tLoss: 92.157902\n",
      "====> Epoch: 4 Average loss: 92.6013\n",
      "epoch: 4, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 5 [0/8040 (0%)]\tLoss: 92.542318\n",
      "Train Epoch: 5 [1200/8040 (15%)]\tLoss: 92.647038\n",
      "Train Epoch: 5 [2400/8040 (30%)]\tLoss: 92.663420\n",
      "Train Epoch: 5 [3600/8040 (45%)]\tLoss: 92.979508\n",
      "Train Epoch: 5 [4800/8040 (60%)]\tLoss: 92.685742\n",
      "Train Epoch: 5 [6000/8040 (75%)]\tLoss: 92.853914\n",
      "Train Epoch: 5 [7200/8040 (90%)]\tLoss: 92.328328\n",
      "====> Epoch: 5 Average loss: 92.5082\n",
      "epoch: 5, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 6 [0/8040 (0%)]\tLoss: 92.403727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [1200/8040 (15%)]\tLoss: 92.301400\n",
      "Train Epoch: 6 [2400/8040 (30%)]\tLoss: 92.503662\n",
      "Train Epoch: 6 [3600/8040 (45%)]\tLoss: 92.293376\n",
      "Train Epoch: 6 [4800/8040 (60%)]\tLoss: 92.003516\n",
      "Train Epoch: 6 [6000/8040 (75%)]\tLoss: 92.306746\n",
      "Train Epoch: 6 [7200/8040 (90%)]\tLoss: 92.160588\n",
      "====> Epoch: 6 Average loss: 92.3592\n",
      "epoch: 6, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 7 [0/8040 (0%)]\tLoss: 92.702547\n",
      "Train Epoch: 7 [1200/8040 (15%)]\tLoss: 92.031665\n",
      "Train Epoch: 7 [2400/8040 (30%)]\tLoss: 92.890324\n",
      "Train Epoch: 7 [3600/8040 (45%)]\tLoss: 92.295964\n",
      "Train Epoch: 7 [4800/8040 (60%)]\tLoss: 92.338053\n",
      "Train Epoch: 7 [6000/8040 (75%)]\tLoss: 92.525716\n",
      "Train Epoch: 7 [7200/8040 (90%)]\tLoss: 92.265251\n",
      "====> Epoch: 7 Average loss: 92.2371\n",
      "epoch: 7, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 8 [0/8040 (0%)]\tLoss: 91.543319\n",
      "Train Epoch: 8 [1200/8040 (15%)]\tLoss: 92.005770\n",
      "Train Epoch: 8 [2400/8040 (30%)]\tLoss: 92.366829\n",
      "Train Epoch: 8 [3600/8040 (45%)]\tLoss: 92.474447\n",
      "Train Epoch: 8 [4800/8040 (60%)]\tLoss: 92.031991\n",
      "Train Epoch: 8 [6000/8040 (75%)]\tLoss: 92.449829\n",
      "Train Epoch: 8 [7200/8040 (90%)]\tLoss: 91.645231\n",
      "====> Epoch: 8 Average loss: 92.1315\n",
      "epoch: 8, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 9 [0/8040 (0%)]\tLoss: 91.537207\n",
      "Train Epoch: 9 [1200/8040 (15%)]\tLoss: 92.292993\n",
      "Train Epoch: 9 [2400/8040 (30%)]\tLoss: 92.091374\n",
      "Train Epoch: 9 [3600/8040 (45%)]\tLoss: 92.101229\n",
      "Train Epoch: 9 [4800/8040 (60%)]\tLoss: 91.453670\n",
      "Train Epoch: 9 [6000/8040 (75%)]\tLoss: 92.177327\n",
      "Train Epoch: 9 [7200/8040 (90%)]\tLoss: 92.015723\n",
      "====> Epoch: 9 Average loss: 92.0561\n",
      "epoch: 9, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 10 [0/8040 (0%)]\tLoss: 92.154045\n",
      "Train Epoch: 10 [1200/8040 (15%)]\tLoss: 92.169059\n",
      "Train Epoch: 10 [2400/8040 (30%)]\tLoss: 91.991089\n",
      "Train Epoch: 10 [3600/8040 (45%)]\tLoss: 91.928833\n",
      "Train Epoch: 10 [4800/8040 (60%)]\tLoss: 92.168652\n",
      "Train Epoch: 10 [6000/8040 (75%)]\tLoss: 91.795923\n",
      "Train Epoch: 10 [7200/8040 (90%)]\tLoss: 92.012557\n",
      "====> Epoch: 10 Average loss: 92.0377\n",
      "epoch: 10, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 11 [0/8040 (0%)]\tLoss: 92.105981\n",
      "Train Epoch: 11 [1200/8040 (15%)]\tLoss: 92.140877\n",
      "Train Epoch: 11 [2400/8040 (30%)]\tLoss: 91.686808\n",
      "Train Epoch: 11 [3600/8040 (45%)]\tLoss: 92.173462\n",
      "Train Epoch: 11 [4800/8040 (60%)]\tLoss: 91.837703\n",
      "Train Epoch: 11 [6000/8040 (75%)]\tLoss: 91.906128\n",
      "Train Epoch: 11 [7200/8040 (90%)]\tLoss: 92.256323\n",
      "====> Epoch: 11 Average loss: 92.0075\n",
      "epoch: 11, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 12 [0/8040 (0%)]\tLoss: 91.789591\n",
      "Train Epoch: 12 [1200/8040 (15%)]\tLoss: 91.920199\n",
      "Train Epoch: 12 [2400/8040 (30%)]\tLoss: 92.008081\n",
      "Train Epoch: 12 [3600/8040 (45%)]\tLoss: 91.745508\n",
      "Train Epoch: 12 [4800/8040 (60%)]\tLoss: 92.072168\n",
      "Train Epoch: 12 [6000/8040 (75%)]\tLoss: 91.730282\n",
      "Train Epoch: 12 [7200/8040 (90%)]\tLoss: 91.963175\n",
      "====> Epoch: 12 Average loss: 91.9316\n",
      "epoch: 12, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 13 [0/8040 (0%)]\tLoss: 92.044393\n",
      "Train Epoch: 13 [1200/8040 (15%)]\tLoss: 91.736637\n",
      "Train Epoch: 13 [2400/8040 (30%)]\tLoss: 92.129053\n",
      "Train Epoch: 13 [3600/8040 (45%)]\tLoss: 92.005493\n",
      "Train Epoch: 13 [4800/8040 (60%)]\tLoss: 91.763428\n",
      "Train Epoch: 13 [6000/8040 (75%)]\tLoss: 91.923218\n",
      "Train Epoch: 13 [7200/8040 (90%)]\tLoss: 91.951603\n",
      "====> Epoch: 13 Average loss: 91.9277\n",
      "epoch: 13, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 14 [0/8040 (0%)]\tLoss: 91.992969\n",
      "Train Epoch: 14 [1200/8040 (15%)]\tLoss: 91.663582\n",
      "Train Epoch: 14 [2400/8040 (30%)]\tLoss: 92.225594\n",
      "Train Epoch: 14 [3600/8040 (45%)]\tLoss: 91.902116\n",
      "Train Epoch: 14 [4800/8040 (60%)]\tLoss: 91.795744\n",
      "Train Epoch: 14 [6000/8040 (75%)]\tLoss: 91.577653\n",
      "Train Epoch: 14 [7200/8040 (90%)]\tLoss: 91.366325\n",
      "====> Epoch: 14 Average loss: 91.8809\n",
      "epoch: 14, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 15 [0/8040 (0%)]\tLoss: 91.785229\n",
      "Train Epoch: 15 [1200/8040 (15%)]\tLoss: 91.881592\n",
      "Train Epoch: 15 [2400/8040 (30%)]\tLoss: 92.302889\n",
      "Train Epoch: 15 [3600/8040 (45%)]\tLoss: 91.545719\n",
      "Train Epoch: 15 [4800/8040 (60%)]\tLoss: 92.370565\n",
      "Train Epoch: 15 [6000/8040 (75%)]\tLoss: 91.932259\n",
      "Train Epoch: 15 [7200/8040 (90%)]\tLoss: 91.882259\n",
      "====> Epoch: 15 Average loss: 91.8950\n",
      "epoch: 15, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 16 [0/8040 (0%)]\tLoss: 91.591667\n",
      "Train Epoch: 16 [1200/8040 (15%)]\tLoss: 91.956250\n",
      "Train Epoch: 16 [2400/8040 (30%)]\tLoss: 91.909855\n",
      "Train Epoch: 16 [3600/8040 (45%)]\tLoss: 92.054460\n",
      "Train Epoch: 16 [4800/8040 (60%)]\tLoss: 91.580265\n",
      "Train Epoch: 16 [6000/8040 (75%)]\tLoss: 92.084635\n",
      "Train Epoch: 16 [7200/8040 (90%)]\tLoss: 91.689925\n",
      "====> Epoch: 16 Average loss: 91.8354\n",
      "epoch: 16, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 17 [0/8040 (0%)]\tLoss: 91.767594\n",
      "Train Epoch: 17 [1200/8040 (15%)]\tLoss: 91.768693\n",
      "Train Epoch: 17 [2400/8040 (30%)]\tLoss: 91.655111\n",
      "Train Epoch: 17 [3600/8040 (45%)]\tLoss: 91.884546\n",
      "Train Epoch: 17 [4800/8040 (60%)]\tLoss: 91.540747\n",
      "Train Epoch: 17 [6000/8040 (75%)]\tLoss: 91.614453\n",
      "Train Epoch: 17 [7200/8040 (90%)]\tLoss: 91.884725\n",
      "====> Epoch: 17 Average loss: 91.8394\n",
      "epoch: 17, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 18 [0/8040 (0%)]\tLoss: 91.701090\n",
      "Train Epoch: 18 [1200/8040 (15%)]\tLoss: 91.552425\n",
      "Train Epoch: 18 [2400/8040 (30%)]\tLoss: 91.546134\n",
      "Train Epoch: 18 [3600/8040 (45%)]\tLoss: 91.487118\n",
      "Train Epoch: 18 [4800/8040 (60%)]\tLoss: 91.640894\n",
      "Train Epoch: 18 [6000/8040 (75%)]\tLoss: 91.507812\n",
      "Train Epoch: 18 [7200/8040 (90%)]\tLoss: 91.906844\n",
      "====> Epoch: 18 Average loss: 91.8306\n",
      "epoch: 18, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 19 [0/8040 (0%)]\tLoss: 91.552759\n",
      "Train Epoch: 19 [1200/8040 (15%)]\tLoss: 91.509041\n",
      "Train Epoch: 19 [2400/8040 (30%)]\tLoss: 91.707324\n",
      "Train Epoch: 19 [3600/8040 (45%)]\tLoss: 91.588436\n",
      "Train Epoch: 19 [4800/8040 (60%)]\tLoss: 91.831730\n",
      "Train Epoch: 19 [6000/8040 (75%)]\tLoss: 91.781388\n",
      "Train Epoch: 19 [7200/8040 (90%)]\tLoss: 92.078752\n",
      "====> Epoch: 19 Average loss: 91.8305\n",
      "epoch: 19, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 20 [0/8040 (0%)]\tLoss: 91.932308\n",
      "Train Epoch: 20 [1200/8040 (15%)]\tLoss: 91.653809\n",
      "Train Epoch: 20 [2400/8040 (30%)]\tLoss: 91.767887\n",
      "Train Epoch: 20 [3600/8040 (45%)]\tLoss: 91.915983\n",
      "Train Epoch: 20 [4800/8040 (60%)]\tLoss: 91.546224\n",
      "Train Epoch: 20 [6000/8040 (75%)]\tLoss: 91.785832\n",
      "Train Epoch: 20 [7200/8040 (90%)]\tLoss: 92.301074\n",
      "====> Epoch: 20 Average loss: 91.8228\n",
      "epoch: 20, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 21 [0/8040 (0%)]\tLoss: 91.863542\n",
      "Train Epoch: 21 [1200/8040 (15%)]\tLoss: 91.803914\n",
      "Train Epoch: 21 [2400/8040 (30%)]\tLoss: 91.839502\n",
      "Train Epoch: 21 [3600/8040 (45%)]\tLoss: 91.816260\n",
      "Train Epoch: 21 [4800/8040 (60%)]\tLoss: 91.698405\n",
      "Train Epoch: 21 [6000/8040 (75%)]\tLoss: 91.765983\n",
      "Train Epoch: 21 [7200/8040 (90%)]\tLoss: 91.745020\n",
      "====> Epoch: 21 Average loss: 91.7889\n",
      "epoch: 21, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 22 [0/8040 (0%)]\tLoss: 92.062427\n",
      "Train Epoch: 22 [1200/8040 (15%)]\tLoss: 91.722770\n",
      "Train Epoch: 22 [2400/8040 (30%)]\tLoss: 91.832853\n",
      "Train Epoch: 22 [3600/8040 (45%)]\tLoss: 91.920687\n",
      "Train Epoch: 22 [4800/8040 (60%)]\tLoss: 91.693685\n",
      "Train Epoch: 22 [6000/8040 (75%)]\tLoss: 91.598682\n",
      "Train Epoch: 22 [7200/8040 (90%)]\tLoss: 91.915112\n",
      "====> Epoch: 22 Average loss: 91.7834\n",
      "epoch: 22, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 23 [0/8040 (0%)]\tLoss: 91.765658\n",
      "Train Epoch: 23 [1200/8040 (15%)]\tLoss: 92.584188\n",
      "Train Epoch: 23 [2400/8040 (30%)]\tLoss: 91.994775\n",
      "Train Epoch: 23 [3600/8040 (45%)]\tLoss: 91.531592\n",
      "Train Epoch: 23 [4800/8040 (60%)]\tLoss: 92.128027\n",
      "Train Epoch: 23 [6000/8040 (75%)]\tLoss: 91.435807\n",
      "Train Epoch: 23 [7200/8040 (90%)]\tLoss: 91.674202\n",
      "====> Epoch: 23 Average loss: 91.7730\n",
      "epoch: 23, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 24 [0/8040 (0%)]\tLoss: 91.736108\n",
      "Train Epoch: 24 [1200/8040 (15%)]\tLoss: 91.266243\n",
      "Train Epoch: 24 [2400/8040 (30%)]\tLoss: 91.911206\n",
      "Train Epoch: 24 [3600/8040 (45%)]\tLoss: 91.475765\n",
      "Train Epoch: 24 [4800/8040 (60%)]\tLoss: 91.684025\n",
      "Train Epoch: 24 [6000/8040 (75%)]\tLoss: 91.901383\n",
      "Train Epoch: 24 [7200/8040 (90%)]\tLoss: 91.978499\n",
      "====> Epoch: 24 Average loss: 91.7565\n",
      "epoch: 24, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 25 [0/8040 (0%)]\tLoss: 91.961328\n",
      "Train Epoch: 25 [1200/8040 (15%)]\tLoss: 92.005575\n",
      "Train Epoch: 25 [2400/8040 (30%)]\tLoss: 91.203052\n",
      "Train Epoch: 25 [3600/8040 (45%)]\tLoss: 92.177344\n",
      "Train Epoch: 25 [4800/8040 (60%)]\tLoss: 91.762378\n",
      "Train Epoch: 25 [6000/8040 (75%)]\tLoss: 92.052067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 25 [7200/8040 (90%)]\tLoss: 91.753890\n",
      "====> Epoch: 25 Average loss: 91.7487\n",
      "epoch: 25, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 26 [0/8040 (0%)]\tLoss: 91.576953\n",
      "Train Epoch: 26 [1200/8040 (15%)]\tLoss: 91.583577\n",
      "Train Epoch: 26 [2400/8040 (30%)]\tLoss: 91.734473\n",
      "Train Epoch: 26 [3600/8040 (45%)]\tLoss: 92.189624\n",
      "Train Epoch: 26 [4800/8040 (60%)]\tLoss: 91.889819\n",
      "Train Epoch: 26 [6000/8040 (75%)]\tLoss: 91.939421\n",
      "Train Epoch: 26 [7200/8040 (90%)]\tLoss: 91.963021\n",
      "====> Epoch: 26 Average loss: 91.7204\n",
      "epoch: 26, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 27 [0/8040 (0%)]\tLoss: 91.781006\n",
      "Train Epoch: 27 [1200/8040 (15%)]\tLoss: 91.777718\n",
      "Train Epoch: 27 [2400/8040 (30%)]\tLoss: 91.539941\n",
      "Train Epoch: 27 [3600/8040 (45%)]\tLoss: 92.009937\n",
      "Train Epoch: 27 [4800/8040 (60%)]\tLoss: 91.723088\n",
      "Train Epoch: 27 [6000/8040 (75%)]\tLoss: 91.945418\n",
      "Train Epoch: 27 [7200/8040 (90%)]\tLoss: 91.908325\n",
      "====> Epoch: 27 Average loss: 91.7325\n",
      "epoch: 27, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 28 [0/8040 (0%)]\tLoss: 91.924536\n",
      "Train Epoch: 28 [1200/8040 (15%)]\tLoss: 91.878573\n",
      "Train Epoch: 28 [2400/8040 (30%)]\tLoss: 92.107324\n",
      "Train Epoch: 28 [3600/8040 (45%)]\tLoss: 91.599829\n",
      "Train Epoch: 28 [4800/8040 (60%)]\tLoss: 92.117749\n",
      "Train Epoch: 28 [6000/8040 (75%)]\tLoss: 91.593368\n",
      "Train Epoch: 28 [7200/8040 (90%)]\tLoss: 91.432340\n",
      "====> Epoch: 28 Average loss: 91.7507\n",
      "epoch: 28, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 29 [0/8040 (0%)]\tLoss: 91.480021\n",
      "Train Epoch: 29 [1200/8040 (15%)]\tLoss: 91.217407\n",
      "Train Epoch: 29 [2400/8040 (30%)]\tLoss: 92.000033\n",
      "Train Epoch: 29 [3600/8040 (45%)]\tLoss: 91.725903\n",
      "Train Epoch: 29 [4800/8040 (60%)]\tLoss: 91.485327\n",
      "Train Epoch: 29 [6000/8040 (75%)]\tLoss: 91.768945\n",
      "Train Epoch: 29 [7200/8040 (90%)]\tLoss: 91.371818\n",
      "====> Epoch: 29 Average loss: 91.6858\n",
      "epoch: 29, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 30 [0/8040 (0%)]\tLoss: 91.993579\n",
      "Train Epoch: 30 [1200/8040 (15%)]\tLoss: 91.528442\n",
      "Train Epoch: 30 [2400/8040 (30%)]\tLoss: 92.169084\n",
      "Train Epoch: 30 [3600/8040 (45%)]\tLoss: 91.887964\n",
      "Train Epoch: 30 [4800/8040 (60%)]\tLoss: 91.757528\n",
      "Train Epoch: 30 [6000/8040 (75%)]\tLoss: 91.955680\n",
      "Train Epoch: 30 [7200/8040 (90%)]\tLoss: 91.900366\n",
      "====> Epoch: 30 Average loss: 91.7532\n",
      "epoch: 30, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 31 [0/8040 (0%)]\tLoss: 91.196305\n",
      "Train Epoch: 31 [1200/8040 (15%)]\tLoss: 91.309953\n",
      "Train Epoch: 31 [2400/8040 (30%)]\tLoss: 91.848047\n",
      "Train Epoch: 31 [3600/8040 (45%)]\tLoss: 92.150659\n",
      "Train Epoch: 31 [4800/8040 (60%)]\tLoss: 91.252189\n",
      "Train Epoch: 31 [6000/8040 (75%)]\tLoss: 91.456291\n",
      "Train Epoch: 31 [7200/8040 (90%)]\tLoss: 91.752979\n",
      "====> Epoch: 31 Average loss: 91.7132\n",
      "epoch: 31, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 32 [0/8040 (0%)]\tLoss: 91.480396\n",
      "Train Epoch: 32 [1200/8040 (15%)]\tLoss: 91.762394\n",
      "Train Epoch: 32 [2400/8040 (30%)]\tLoss: 91.989974\n",
      "Train Epoch: 32 [3600/8040 (45%)]\tLoss: 91.724780\n",
      "Train Epoch: 32 [4800/8040 (60%)]\tLoss: 91.763810\n",
      "Train Epoch: 32 [6000/8040 (75%)]\tLoss: 91.124902\n",
      "Train Epoch: 32 [7200/8040 (90%)]\tLoss: 92.144653\n",
      "====> Epoch: 32 Average loss: 91.6779\n",
      "epoch: 32, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 33 [0/8040 (0%)]\tLoss: 91.498918\n",
      "Train Epoch: 33 [1200/8040 (15%)]\tLoss: 91.954679\n",
      "Train Epoch: 33 [2400/8040 (30%)]\tLoss: 91.902018\n",
      "Train Epoch: 33 [3600/8040 (45%)]\tLoss: 91.845931\n",
      "Train Epoch: 33 [4800/8040 (60%)]\tLoss: 91.657422\n",
      "Train Epoch: 33 [6000/8040 (75%)]\tLoss: 91.914827\n",
      "Train Epoch: 33 [7200/8040 (90%)]\tLoss: 91.665210\n",
      "====> Epoch: 33 Average loss: 91.6912\n",
      "epoch: 33, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 34 [0/8040 (0%)]\tLoss: 91.159961\n",
      "Train Epoch: 34 [1200/8040 (15%)]\tLoss: 91.696257\n",
      "Train Epoch: 34 [2400/8040 (30%)]\tLoss: 91.642798\n",
      "Train Epoch: 34 [3600/8040 (45%)]\tLoss: 91.385498\n",
      "Train Epoch: 34 [4800/8040 (60%)]\tLoss: 91.354679\n",
      "Train Epoch: 34 [6000/8040 (75%)]\tLoss: 91.082625\n",
      "Train Epoch: 34 [7200/8040 (90%)]\tLoss: 91.851538\n",
      "====> Epoch: 34 Average loss: 91.6802\n",
      "epoch: 34, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 35 [0/8040 (0%)]\tLoss: 91.926888\n",
      "Train Epoch: 35 [1200/8040 (15%)]\tLoss: 91.662004\n",
      "Train Epoch: 35 [2400/8040 (30%)]\tLoss: 91.709701\n",
      "Train Epoch: 35 [3600/8040 (45%)]\tLoss: 91.220182\n",
      "Train Epoch: 35 [4800/8040 (60%)]\tLoss: 91.850138\n",
      "Train Epoch: 35 [6000/8040 (75%)]\tLoss: 91.609692\n",
      "Train Epoch: 35 [7200/8040 (90%)]\tLoss: 91.860490\n",
      "====> Epoch: 35 Average loss: 91.7046\n",
      "epoch: 35, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 36 [0/8040 (0%)]\tLoss: 91.784261\n",
      "Train Epoch: 36 [1200/8040 (15%)]\tLoss: 91.613582\n",
      "Train Epoch: 36 [2400/8040 (30%)]\tLoss: 91.793188\n",
      "Train Epoch: 36 [3600/8040 (45%)]\tLoss: 91.468506\n",
      "Train Epoch: 36 [4800/8040 (60%)]\tLoss: 91.688615\n",
      "Train Epoch: 36 [6000/8040 (75%)]\tLoss: 91.632812\n",
      "Train Epoch: 36 [7200/8040 (90%)]\tLoss: 91.796069\n",
      "====> Epoch: 36 Average loss: 91.6965\n",
      "epoch: 36, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 37 [0/8040 (0%)]\tLoss: 91.496053\n",
      "Train Epoch: 37 [1200/8040 (15%)]\tLoss: 91.614176\n",
      "Train Epoch: 37 [2400/8040 (30%)]\tLoss: 91.992814\n",
      "Train Epoch: 37 [3600/8040 (45%)]\tLoss: 91.055176\n",
      "Train Epoch: 37 [4800/8040 (60%)]\tLoss: 91.623698\n",
      "Train Epoch: 37 [6000/8040 (75%)]\tLoss: 91.736491\n",
      "Train Epoch: 37 [7200/8040 (90%)]\tLoss: 91.364950\n",
      "====> Epoch: 37 Average loss: 91.6822\n",
      "epoch: 37, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 38 [0/8040 (0%)]\tLoss: 91.787134\n",
      "Train Epoch: 38 [1200/8040 (15%)]\tLoss: 91.492269\n",
      "Train Epoch: 38 [2400/8040 (30%)]\tLoss: 91.860547\n",
      "Train Epoch: 38 [3600/8040 (45%)]\tLoss: 91.932967\n",
      "Train Epoch: 38 [4800/8040 (60%)]\tLoss: 91.584595\n",
      "Train Epoch: 38 [6000/8040 (75%)]\tLoss: 91.856104\n",
      "Train Epoch: 38 [7200/8040 (90%)]\tLoss: 91.245955\n",
      "====> Epoch: 38 Average loss: 91.6808\n",
      "epoch: 38, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 39 [0/8040 (0%)]\tLoss: 91.639893\n",
      "Train Epoch: 39 [1200/8040 (15%)]\tLoss: 91.589681\n",
      "Train Epoch: 39 [2400/8040 (30%)]\tLoss: 91.280819\n",
      "Train Epoch: 39 [3600/8040 (45%)]\tLoss: 91.304451\n",
      "Train Epoch: 39 [4800/8040 (60%)]\tLoss: 91.104061\n",
      "Train Epoch: 39 [6000/8040 (75%)]\tLoss: 91.619132\n",
      "Train Epoch: 39 [7200/8040 (90%)]\tLoss: 91.554867\n",
      "====> Epoch: 39 Average loss: 91.6627\n",
      "epoch: 39, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 40 [0/8040 (0%)]\tLoss: 91.995923\n",
      "Train Epoch: 40 [1200/8040 (15%)]\tLoss: 91.938631\n",
      "Train Epoch: 40 [2400/8040 (30%)]\tLoss: 91.608870\n",
      "Train Epoch: 40 [3600/8040 (45%)]\tLoss: 91.613883\n",
      "Train Epoch: 40 [4800/8040 (60%)]\tLoss: 91.614209\n",
      "Train Epoch: 40 [6000/8040 (75%)]\tLoss: 91.857495\n",
      "Train Epoch: 40 [7200/8040 (90%)]\tLoss: 91.667912\n",
      "====> Epoch: 40 Average loss: 91.6778\n",
      "epoch: 40, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 41 [0/8040 (0%)]\tLoss: 91.721639\n",
      "Train Epoch: 41 [1200/8040 (15%)]\tLoss: 91.793465\n",
      "Train Epoch: 41 [2400/8040 (30%)]\tLoss: 91.584465\n",
      "Train Epoch: 41 [3600/8040 (45%)]\tLoss: 91.535547\n",
      "Train Epoch: 41 [4800/8040 (60%)]\tLoss: 91.243213\n",
      "Train Epoch: 41 [6000/8040 (75%)]\tLoss: 91.970426\n",
      "Train Epoch: 41 [7200/8040 (90%)]\tLoss: 91.403402\n",
      "====> Epoch: 41 Average loss: 91.6707\n",
      "epoch: 41, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 42 [0/8040 (0%)]\tLoss: 91.593351\n",
      "Train Epoch: 42 [1200/8040 (15%)]\tLoss: 91.582015\n",
      "Train Epoch: 42 [2400/8040 (30%)]\tLoss: 91.738354\n",
      "Train Epoch: 42 [3600/8040 (45%)]\tLoss: 91.549626\n",
      "Train Epoch: 42 [4800/8040 (60%)]\tLoss: 91.446663\n",
      "Train Epoch: 42 [6000/8040 (75%)]\tLoss: 91.392489\n",
      "Train Epoch: 42 [7200/8040 (90%)]\tLoss: 92.025505\n",
      "====> Epoch: 42 Average loss: 91.6693\n",
      "epoch: 42, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 43 [0/8040 (0%)]\tLoss: 91.341064\n",
      "Train Epoch: 43 [1200/8040 (15%)]\tLoss: 91.341146\n",
      "Train Epoch: 43 [2400/8040 (30%)]\tLoss: 91.974821\n",
      "Train Epoch: 43 [3600/8040 (45%)]\tLoss: 91.720793\n",
      "Train Epoch: 43 [4800/8040 (60%)]\tLoss: 91.562126\n",
      "Train Epoch: 43 [6000/8040 (75%)]\tLoss: 91.501961\n",
      "Train Epoch: 43 [7200/8040 (90%)]\tLoss: 91.709489\n",
      "====> Epoch: 43 Average loss: 91.6757\n",
      "epoch: 43, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 44 [0/8040 (0%)]\tLoss: 91.507316\n",
      "Train Epoch: 44 [1200/8040 (15%)]\tLoss: 91.628882\n",
      "Train Epoch: 44 [2400/8040 (30%)]\tLoss: 91.599642\n",
      "Train Epoch: 44 [3600/8040 (45%)]\tLoss: 91.575317\n",
      "Train Epoch: 44 [4800/8040 (60%)]\tLoss: 91.667204\n",
      "Train Epoch: 44 [6000/8040 (75%)]\tLoss: 91.438289\n",
      "Train Epoch: 44 [7200/8040 (90%)]\tLoss: 91.496566\n",
      "====> Epoch: 44 Average loss: 91.6625\n",
      "epoch: 44, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 45 [0/8040 (0%)]\tLoss: 91.738371\n",
      "Train Epoch: 45 [1200/8040 (15%)]\tLoss: 91.389071\n",
      "Train Epoch: 45 [2400/8040 (30%)]\tLoss: 91.703996\n",
      "Train Epoch: 45 [3600/8040 (45%)]\tLoss: 91.894116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 45 [4800/8040 (60%)]\tLoss: 91.903670\n",
      "Train Epoch: 45 [6000/8040 (75%)]\tLoss: 91.499740\n",
      "Train Epoch: 45 [7200/8040 (90%)]\tLoss: 91.533968\n",
      "====> Epoch: 45 Average loss: 91.6457\n",
      "epoch: 45, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 46 [0/8040 (0%)]\tLoss: 91.463395\n",
      "Train Epoch: 46 [1200/8040 (15%)]\tLoss: 91.909204\n",
      "Train Epoch: 46 [2400/8040 (30%)]\tLoss: 91.563631\n",
      "Train Epoch: 46 [3600/8040 (45%)]\tLoss: 91.216976\n",
      "Train Epoch: 46 [4800/8040 (60%)]\tLoss: 91.874227\n",
      "Train Epoch: 46 [6000/8040 (75%)]\tLoss: 91.368514\n",
      "Train Epoch: 46 [7200/8040 (90%)]\tLoss: 91.682544\n",
      "====> Epoch: 46 Average loss: 91.6521\n",
      "epoch: 46, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 47 [0/8040 (0%)]\tLoss: 91.088159\n",
      "Train Epoch: 47 [1200/8040 (15%)]\tLoss: 91.342863\n",
      "Train Epoch: 47 [2400/8040 (30%)]\tLoss: 91.914339\n",
      "Train Epoch: 47 [3600/8040 (45%)]\tLoss: 91.797323\n",
      "Train Epoch: 47 [4800/8040 (60%)]\tLoss: 91.685807\n",
      "Train Epoch: 47 [6000/8040 (75%)]\tLoss: 91.737646\n",
      "Train Epoch: 47 [7200/8040 (90%)]\tLoss: 91.666813\n",
      "====> Epoch: 47 Average loss: 91.6360\n",
      "epoch: 47, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 48 [0/8040 (0%)]\tLoss: 91.433757\n",
      "Train Epoch: 48 [1200/8040 (15%)]\tLoss: 91.410042\n",
      "Train Epoch: 48 [2400/8040 (30%)]\tLoss: 91.191602\n",
      "Train Epoch: 48 [3600/8040 (45%)]\tLoss: 91.749422\n",
      "Train Epoch: 48 [4800/8040 (60%)]\tLoss: 91.579207\n",
      "Train Epoch: 48 [6000/8040 (75%)]\tLoss: 91.782666\n",
      "Train Epoch: 48 [7200/8040 (90%)]\tLoss: 91.243693\n",
      "====> Epoch: 48 Average loss: 91.6237\n",
      "epoch: 48, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 49 [0/8040 (0%)]\tLoss: 91.818221\n",
      "Train Epoch: 49 [1200/8040 (15%)]\tLoss: 91.677360\n",
      "Train Epoch: 49 [2400/8040 (30%)]\tLoss: 91.701164\n",
      "Train Epoch: 49 [3600/8040 (45%)]\tLoss: 91.359774\n",
      "Train Epoch: 49 [4800/8040 (60%)]\tLoss: 91.833211\n",
      "Train Epoch: 49 [6000/8040 (75%)]\tLoss: 91.816016\n",
      "Train Epoch: 49 [7200/8040 (90%)]\tLoss: 91.604362\n",
      "====> Epoch: 49 Average loss: 91.6280\n",
      "epoch: 49, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 50 [0/8040 (0%)]\tLoss: 91.380094\n",
      "Train Epoch: 50 [1200/8040 (15%)]\tLoss: 91.729215\n",
      "Train Epoch: 50 [2400/8040 (30%)]\tLoss: 91.975016\n",
      "Train Epoch: 50 [3600/8040 (45%)]\tLoss: 91.245475\n",
      "Train Epoch: 50 [4800/8040 (60%)]\tLoss: 91.560962\n",
      "Train Epoch: 50 [6000/8040 (75%)]\tLoss: 91.868799\n",
      "Train Epoch: 50 [7200/8040 (90%)]\tLoss: 91.988346\n",
      "====> Epoch: 50 Average loss: 91.6225\n",
      "epoch: 50, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 51 [0/8040 (0%)]\tLoss: 91.985067\n",
      "Train Epoch: 51 [1200/8040 (15%)]\tLoss: 91.680412\n",
      "Train Epoch: 51 [2400/8040 (30%)]\tLoss: 91.589640\n",
      "Train Epoch: 51 [3600/8040 (45%)]\tLoss: 91.841968\n",
      "Train Epoch: 51 [4800/8040 (60%)]\tLoss: 91.516642\n",
      "Train Epoch: 51 [6000/8040 (75%)]\tLoss: 91.736654\n",
      "Train Epoch: 51 [7200/8040 (90%)]\tLoss: 91.385864\n",
      "====> Epoch: 51 Average loss: 91.6061\n",
      "epoch: 51, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 52 [0/8040 (0%)]\tLoss: 91.730599\n",
      "Train Epoch: 52 [1200/8040 (15%)]\tLoss: 91.704924\n",
      "Train Epoch: 52 [2400/8040 (30%)]\tLoss: 91.281022\n",
      "Train Epoch: 52 [3600/8040 (45%)]\tLoss: 91.125334\n",
      "Train Epoch: 52 [4800/8040 (60%)]\tLoss: 91.490828\n",
      "Train Epoch: 52 [6000/8040 (75%)]\tLoss: 91.481055\n",
      "Train Epoch: 52 [7200/8040 (90%)]\tLoss: 91.644067\n",
      "====> Epoch: 52 Average loss: 91.6283\n",
      "epoch: 52, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 53 [0/8040 (0%)]\tLoss: 91.188664\n",
      "Train Epoch: 53 [1200/8040 (15%)]\tLoss: 92.063957\n",
      "Train Epoch: 53 [2400/8040 (30%)]\tLoss: 91.475700\n",
      "Train Epoch: 53 [3600/8040 (45%)]\tLoss: 91.786882\n",
      "Train Epoch: 53 [4800/8040 (60%)]\tLoss: 91.580835\n",
      "Train Epoch: 53 [6000/8040 (75%)]\tLoss: 91.609863\n",
      "Train Epoch: 53 [7200/8040 (90%)]\tLoss: 91.798446\n",
      "====> Epoch: 53 Average loss: 91.6140\n",
      "epoch: 53, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 54 [0/8040 (0%)]\tLoss: 91.381242\n",
      "Train Epoch: 54 [1200/8040 (15%)]\tLoss: 92.038737\n",
      "Train Epoch: 54 [2400/8040 (30%)]\tLoss: 91.666040\n",
      "Train Epoch: 54 [3600/8040 (45%)]\tLoss: 91.445068\n",
      "Train Epoch: 54 [4800/8040 (60%)]\tLoss: 91.207292\n",
      "Train Epoch: 54 [6000/8040 (75%)]\tLoss: 91.293563\n",
      "Train Epoch: 54 [7200/8040 (90%)]\tLoss: 91.277930\n",
      "====> Epoch: 54 Average loss: 91.6009\n",
      "epoch: 54, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 55 [0/8040 (0%)]\tLoss: 91.904696\n",
      "Train Epoch: 55 [1200/8040 (15%)]\tLoss: 91.553825\n",
      "Train Epoch: 55 [2400/8040 (30%)]\tLoss: 91.624959\n",
      "Train Epoch: 55 [3600/8040 (45%)]\tLoss: 91.678923\n",
      "Train Epoch: 55 [4800/8040 (60%)]\tLoss: 91.597681\n",
      "Train Epoch: 55 [6000/8040 (75%)]\tLoss: 91.488981\n",
      "Train Epoch: 55 [7200/8040 (90%)]\tLoss: 91.991024\n",
      "====> Epoch: 55 Average loss: 91.6089\n",
      "epoch: 55, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 56 [0/8040 (0%)]\tLoss: 91.425684\n",
      "Train Epoch: 56 [1200/8040 (15%)]\tLoss: 91.247933\n",
      "Train Epoch: 56 [2400/8040 (30%)]\tLoss: 91.427271\n",
      "Train Epoch: 56 [3600/8040 (45%)]\tLoss: 92.009399\n",
      "Train Epoch: 56 [4800/8040 (60%)]\tLoss: 91.339657\n",
      "Train Epoch: 56 [6000/8040 (75%)]\tLoss: 91.747518\n",
      "Train Epoch: 56 [7200/8040 (90%)]\tLoss: 91.846395\n",
      "====> Epoch: 56 Average loss: 91.5844\n",
      "epoch: 56, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 57 [0/8040 (0%)]\tLoss: 91.188615\n",
      "Train Epoch: 57 [1200/8040 (15%)]\tLoss: 91.668766\n",
      "Train Epoch: 57 [2400/8040 (30%)]\tLoss: 91.819393\n",
      "Train Epoch: 57 [3600/8040 (45%)]\tLoss: 91.541219\n",
      "Train Epoch: 57 [4800/8040 (60%)]\tLoss: 91.779346\n",
      "Train Epoch: 57 [6000/8040 (75%)]\tLoss: 91.800366\n",
      "Train Epoch: 57 [7200/8040 (90%)]\tLoss: 91.545190\n",
      "====> Epoch: 57 Average loss: 91.5938\n",
      "epoch: 57, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 58 [0/8040 (0%)]\tLoss: 91.571582\n",
      "Train Epoch: 58 [1200/8040 (15%)]\tLoss: 91.623820\n",
      "Train Epoch: 58 [2400/8040 (30%)]\tLoss: 91.561458\n",
      "Train Epoch: 58 [3600/8040 (45%)]\tLoss: 91.803621\n",
      "Train Epoch: 58 [4800/8040 (60%)]\tLoss: 91.855664\n",
      "Train Epoch: 58 [6000/8040 (75%)]\tLoss: 91.875057\n",
      "Train Epoch: 58 [7200/8040 (90%)]\tLoss: 91.896981\n",
      "====> Epoch: 58 Average loss: 91.6100\n",
      "epoch: 58, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 59 [0/8040 (0%)]\tLoss: 91.558879\n",
      "Train Epoch: 59 [1200/8040 (15%)]\tLoss: 91.174479\n",
      "Train Epoch: 59 [2400/8040 (30%)]\tLoss: 91.426261\n",
      "Train Epoch: 59 [3600/8040 (45%)]\tLoss: 91.671940\n",
      "Train Epoch: 59 [4800/8040 (60%)]\tLoss: 91.538713\n",
      "Train Epoch: 59 [6000/8040 (75%)]\tLoss: 91.455632\n",
      "Train Epoch: 59 [7200/8040 (90%)]\tLoss: 91.194914\n",
      "====> Epoch: 59 Average loss: 91.5990\n",
      "epoch: 59, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 60 [0/8040 (0%)]\tLoss: 91.834823\n",
      "Train Epoch: 60 [1200/8040 (15%)]\tLoss: 91.501034\n",
      "Train Epoch: 60 [2400/8040 (30%)]\tLoss: 91.528678\n",
      "Train Epoch: 60 [3600/8040 (45%)]\tLoss: 91.982764\n",
      "Train Epoch: 60 [4800/8040 (60%)]\tLoss: 91.660685\n",
      "Train Epoch: 60 [6000/8040 (75%)]\tLoss: 91.341187\n",
      "Train Epoch: 60 [7200/8040 (90%)]\tLoss: 91.684204\n",
      "====> Epoch: 60 Average loss: 91.6097\n",
      "epoch: 60, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 61 [0/8040 (0%)]\tLoss: 91.372933\n",
      "Train Epoch: 61 [1200/8040 (15%)]\tLoss: 91.987394\n",
      "Train Epoch: 61 [2400/8040 (30%)]\tLoss: 92.074178\n",
      "Train Epoch: 61 [3600/8040 (45%)]\tLoss: 91.507048\n",
      "Train Epoch: 61 [4800/8040 (60%)]\tLoss: 91.654248\n",
      "Train Epoch: 61 [6000/8040 (75%)]\tLoss: 91.658993\n",
      "Train Epoch: 61 [7200/8040 (90%)]\tLoss: 91.692098\n",
      "====> Epoch: 61 Average loss: 91.6230\n",
      "epoch: 61, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 62 [0/8040 (0%)]\tLoss: 91.172233\n",
      "Train Epoch: 62 [1200/8040 (15%)]\tLoss: 91.674577\n",
      "Train Epoch: 62 [2400/8040 (30%)]\tLoss: 91.466536\n",
      "Train Epoch: 62 [3600/8040 (45%)]\tLoss: 91.295589\n",
      "Train Epoch: 62 [4800/8040 (60%)]\tLoss: 91.381038\n",
      "Train Epoch: 62 [6000/8040 (75%)]\tLoss: 91.621965\n",
      "Train Epoch: 62 [7200/8040 (90%)]\tLoss: 91.651603\n",
      "====> Epoch: 62 Average loss: 91.5635\n",
      "epoch: 62, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 63 [0/8040 (0%)]\tLoss: 91.574300\n",
      "Train Epoch: 63 [1200/8040 (15%)]\tLoss: 91.496216\n",
      "Train Epoch: 63 [2400/8040 (30%)]\tLoss: 91.347721\n",
      "Train Epoch: 63 [3600/8040 (45%)]\tLoss: 91.598608\n",
      "Train Epoch: 63 [4800/8040 (60%)]\tLoss: 91.466650\n",
      "Train Epoch: 63 [6000/8040 (75%)]\tLoss: 92.030322\n",
      "Train Epoch: 63 [7200/8040 (90%)]\tLoss: 91.304777\n",
      "====> Epoch: 63 Average loss: 91.5756\n",
      "epoch: 63, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 64 [0/8040 (0%)]\tLoss: 91.825578\n",
      "Train Epoch: 64 [1200/8040 (15%)]\tLoss: 91.649284\n",
      "Train Epoch: 64 [2400/8040 (30%)]\tLoss: 91.682503\n",
      "Train Epoch: 64 [3600/8040 (45%)]\tLoss: 91.242065\n",
      "Train Epoch: 64 [4800/8040 (60%)]\tLoss: 91.645833\n",
      "Train Epoch: 64 [6000/8040 (75%)]\tLoss: 91.236589\n",
      "Train Epoch: 64 [7200/8040 (90%)]\tLoss: 91.295410\n",
      "====> Epoch: 64 Average loss: 91.5485\n",
      "epoch: 64, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 65 [0/8040 (0%)]\tLoss: 91.868978\n",
      "Train Epoch: 65 [1200/8040 (15%)]\tLoss: 91.635392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 65 [2400/8040 (30%)]\tLoss: 91.632487\n",
      "Train Epoch: 65 [3600/8040 (45%)]\tLoss: 91.524878\n",
      "Train Epoch: 65 [4800/8040 (60%)]\tLoss: 91.869653\n",
      "Train Epoch: 65 [6000/8040 (75%)]\tLoss: 92.040601\n",
      "Train Epoch: 65 [7200/8040 (90%)]\tLoss: 91.236792\n",
      "====> Epoch: 65 Average loss: 91.5831\n",
      "epoch: 65, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 66 [0/8040 (0%)]\tLoss: 91.634937\n",
      "Train Epoch: 66 [1200/8040 (15%)]\tLoss: 91.303117\n",
      "Train Epoch: 66 [2400/8040 (30%)]\tLoss: 91.404622\n",
      "Train Epoch: 66 [3600/8040 (45%)]\tLoss: 91.883838\n",
      "Train Epoch: 66 [4800/8040 (60%)]\tLoss: 91.358944\n",
      "Train Epoch: 66 [6000/8040 (75%)]\tLoss: 91.184318\n",
      "Train Epoch: 66 [7200/8040 (90%)]\tLoss: 91.699080\n",
      "====> Epoch: 66 Average loss: 91.5962\n",
      "epoch: 66, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 67 [0/8040 (0%)]\tLoss: 92.012256\n",
      "Train Epoch: 67 [1200/8040 (15%)]\tLoss: 91.542863\n",
      "Train Epoch: 67 [2400/8040 (30%)]\tLoss: 91.408716\n",
      "Train Epoch: 67 [3600/8040 (45%)]\tLoss: 91.127726\n",
      "Train Epoch: 67 [4800/8040 (60%)]\tLoss: 91.535612\n",
      "Train Epoch: 67 [6000/8040 (75%)]\tLoss: 91.897160\n",
      "Train Epoch: 67 [7200/8040 (90%)]\tLoss: 92.003442\n",
      "====> Epoch: 67 Average loss: 91.5837\n",
      "epoch: 67, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 68 [0/8040 (0%)]\tLoss: 91.132699\n",
      "Train Epoch: 68 [1200/8040 (15%)]\tLoss: 91.510986\n",
      "Train Epoch: 68 [2400/8040 (30%)]\tLoss: 91.942562\n",
      "Train Epoch: 68 [3600/8040 (45%)]\tLoss: 91.434115\n",
      "Train Epoch: 68 [4800/8040 (60%)]\tLoss: 91.460221\n",
      "Train Epoch: 68 [6000/8040 (75%)]\tLoss: 91.827995\n",
      "Train Epoch: 68 [7200/8040 (90%)]\tLoss: 91.693734\n",
      "====> Epoch: 68 Average loss: 91.5648\n",
      "epoch: 68, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 69 [0/8040 (0%)]\tLoss: 91.781991\n",
      "Train Epoch: 69 [1200/8040 (15%)]\tLoss: 91.252254\n",
      "Train Epoch: 69 [2400/8040 (30%)]\tLoss: 91.442065\n",
      "Train Epoch: 69 [3600/8040 (45%)]\tLoss: 91.753304\n",
      "Train Epoch: 69 [4800/8040 (60%)]\tLoss: 91.739445\n",
      "Train Epoch: 69 [6000/8040 (75%)]\tLoss: 91.923975\n",
      "Train Epoch: 69 [7200/8040 (90%)]\tLoss: 91.926172\n",
      "====> Epoch: 69 Average loss: 91.5978\n",
      "epoch: 69, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 70 [0/8040 (0%)]\tLoss: 91.961263\n",
      "Train Epoch: 70 [1200/8040 (15%)]\tLoss: 91.678507\n",
      "Train Epoch: 70 [2400/8040 (30%)]\tLoss: 91.824683\n",
      "Train Epoch: 70 [3600/8040 (45%)]\tLoss: 91.532878\n",
      "Train Epoch: 70 [4800/8040 (60%)]\tLoss: 91.308293\n",
      "Train Epoch: 70 [6000/8040 (75%)]\tLoss: 91.535140\n",
      "Train Epoch: 70 [7200/8040 (90%)]\tLoss: 91.229704\n",
      "====> Epoch: 70 Average loss: 91.5731\n",
      "epoch: 70, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 71 [0/8040 (0%)]\tLoss: 91.446484\n",
      "Train Epoch: 71 [1200/8040 (15%)]\tLoss: 91.621924\n",
      "Train Epoch: 71 [2400/8040 (30%)]\tLoss: 91.612915\n",
      "Train Epoch: 71 [3600/8040 (45%)]\tLoss: 91.676595\n",
      "Train Epoch: 71 [4800/8040 (60%)]\tLoss: 91.658936\n",
      "Train Epoch: 71 [6000/8040 (75%)]\tLoss: 91.522209\n",
      "Train Epoch: 71 [7200/8040 (90%)]\tLoss: 91.643229\n",
      "====> Epoch: 71 Average loss: 91.5866\n",
      "epoch: 71, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 72 [0/8040 (0%)]\tLoss: 91.330037\n",
      "Train Epoch: 72 [1200/8040 (15%)]\tLoss: 91.568758\n",
      "Train Epoch: 72 [2400/8040 (30%)]\tLoss: 91.355998\n",
      "Train Epoch: 72 [3600/8040 (45%)]\tLoss: 92.012321\n",
      "Train Epoch: 72 [4800/8040 (60%)]\tLoss: 91.685929\n",
      "Train Epoch: 72 [6000/8040 (75%)]\tLoss: 91.393359\n",
      "Train Epoch: 72 [7200/8040 (90%)]\tLoss: 91.340063\n",
      "====> Epoch: 72 Average loss: 91.5677\n",
      "epoch: 72, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 73 [0/8040 (0%)]\tLoss: 91.178271\n",
      "Train Epoch: 73 [1200/8040 (15%)]\tLoss: 91.885978\n",
      "Train Epoch: 73 [2400/8040 (30%)]\tLoss: 91.718026\n",
      "Train Epoch: 73 [3600/8040 (45%)]\tLoss: 91.777157\n",
      "Train Epoch: 73 [4800/8040 (60%)]\tLoss: 91.737443\n",
      "Train Epoch: 73 [6000/8040 (75%)]\tLoss: 91.583838\n",
      "Train Epoch: 73 [7200/8040 (90%)]\tLoss: 91.419816\n",
      "====> Epoch: 73 Average loss: 91.5524\n",
      "epoch: 73, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 74 [0/8040 (0%)]\tLoss: 91.334871\n",
      "Train Epoch: 74 [1200/8040 (15%)]\tLoss: 91.734912\n",
      "Train Epoch: 74 [2400/8040 (30%)]\tLoss: 91.536971\n",
      "Train Epoch: 74 [3600/8040 (45%)]\tLoss: 91.806112\n",
      "Train Epoch: 74 [4800/8040 (60%)]\tLoss: 91.426359\n",
      "Train Epoch: 74 [6000/8040 (75%)]\tLoss: 91.115633\n",
      "Train Epoch: 74 [7200/8040 (90%)]\tLoss: 91.464225\n",
      "====> Epoch: 74 Average loss: 91.5710\n",
      "epoch: 74, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 75 [0/8040 (0%)]\tLoss: 91.574748\n",
      "Train Epoch: 75 [1200/8040 (15%)]\tLoss: 91.575765\n",
      "Train Epoch: 75 [2400/8040 (30%)]\tLoss: 91.705094\n",
      "Train Epoch: 75 [3600/8040 (45%)]\tLoss: 91.485758\n",
      "Train Epoch: 75 [4800/8040 (60%)]\tLoss: 91.668221\n",
      "Train Epoch: 75 [6000/8040 (75%)]\tLoss: 91.701742\n",
      "Train Epoch: 75 [7200/8040 (90%)]\tLoss: 91.614876\n",
      "====> Epoch: 75 Average loss: 91.5630\n",
      "epoch: 75, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 76 [0/8040 (0%)]\tLoss: 91.370337\n",
      "Train Epoch: 76 [1200/8040 (15%)]\tLoss: 91.478426\n",
      "Train Epoch: 76 [2400/8040 (30%)]\tLoss: 91.792814\n",
      "Train Epoch: 76 [3600/8040 (45%)]\tLoss: 91.582414\n",
      "Train Epoch: 76 [4800/8040 (60%)]\tLoss: 91.602384\n",
      "Train Epoch: 76 [6000/8040 (75%)]\tLoss: 91.311727\n",
      "Train Epoch: 76 [7200/8040 (90%)]\tLoss: 91.596965\n",
      "====> Epoch: 76 Average loss: 91.5503\n",
      "epoch: 76, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 77 [0/8040 (0%)]\tLoss: 91.391675\n",
      "Train Epoch: 77 [1200/8040 (15%)]\tLoss: 91.578735\n",
      "Train Epoch: 77 [2400/8040 (30%)]\tLoss: 91.721745\n",
      "Train Epoch: 77 [3600/8040 (45%)]\tLoss: 91.453589\n",
      "Train Epoch: 77 [4800/8040 (60%)]\tLoss: 91.671289\n",
      "Train Epoch: 77 [6000/8040 (75%)]\tLoss: 91.667879\n",
      "Train Epoch: 77 [7200/8040 (90%)]\tLoss: 91.827694\n",
      "====> Epoch: 77 Average loss: 91.5655\n",
      "epoch: 77, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 78 [0/8040 (0%)]\tLoss: 91.415666\n",
      "Train Epoch: 78 [1200/8040 (15%)]\tLoss: 91.553841\n",
      "Train Epoch: 78 [2400/8040 (30%)]\tLoss: 91.383049\n",
      "Train Epoch: 78 [3600/8040 (45%)]\tLoss: 91.439795\n",
      "Train Epoch: 78 [4800/8040 (60%)]\tLoss: 91.549455\n",
      "Train Epoch: 78 [6000/8040 (75%)]\tLoss: 91.807690\n",
      "Train Epoch: 78 [7200/8040 (90%)]\tLoss: 91.459172\n",
      "====> Epoch: 78 Average loss: 91.5476\n",
      "epoch: 78, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 79 [0/8040 (0%)]\tLoss: 91.481323\n",
      "Train Epoch: 79 [1200/8040 (15%)]\tLoss: 91.235767\n",
      "Train Epoch: 79 [2400/8040 (30%)]\tLoss: 91.286963\n",
      "Train Epoch: 79 [3600/8040 (45%)]\tLoss: 91.621200\n",
      "Train Epoch: 79 [4800/8040 (60%)]\tLoss: 91.701066\n",
      "Train Epoch: 79 [6000/8040 (75%)]\tLoss: 91.364478\n",
      "Train Epoch: 79 [7200/8040 (90%)]\tLoss: 91.355477\n",
      "====> Epoch: 79 Average loss: 91.5752\n",
      "epoch: 79, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 80 [0/8040 (0%)]\tLoss: 91.252197\n",
      "Train Epoch: 80 [1200/8040 (15%)]\tLoss: 91.462712\n",
      "Train Epoch: 80 [2400/8040 (30%)]\tLoss: 91.623926\n",
      "Train Epoch: 80 [3600/8040 (45%)]\tLoss: 91.486060\n",
      "Train Epoch: 80 [4800/8040 (60%)]\tLoss: 91.191455\n",
      "Train Epoch: 80 [6000/8040 (75%)]\tLoss: 91.506901\n",
      "Train Epoch: 80 [7200/8040 (90%)]\tLoss: 91.333268\n",
      "====> Epoch: 80 Average loss: 91.5420\n",
      "epoch: 80, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 81 [0/8040 (0%)]\tLoss: 91.747445\n",
      "Train Epoch: 81 [1200/8040 (15%)]\tLoss: 91.121899\n",
      "Train Epoch: 81 [2400/8040 (30%)]\tLoss: 91.694149\n",
      "Train Epoch: 81 [3600/8040 (45%)]\tLoss: 91.476131\n",
      "Train Epoch: 81 [4800/8040 (60%)]\tLoss: 91.302515\n",
      "Train Epoch: 81 [6000/8040 (75%)]\tLoss: 91.575968\n",
      "Train Epoch: 81 [7200/8040 (90%)]\tLoss: 91.377409\n",
      "====> Epoch: 81 Average loss: 91.5547\n",
      "epoch: 81, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 82 [0/8040 (0%)]\tLoss: 91.583480\n",
      "Train Epoch: 82 [1200/8040 (15%)]\tLoss: 91.648210\n",
      "Train Epoch: 82 [2400/8040 (30%)]\tLoss: 92.001888\n",
      "Train Epoch: 82 [3600/8040 (45%)]\tLoss: 91.417830\n",
      "Train Epoch: 82 [4800/8040 (60%)]\tLoss: 91.340527\n",
      "Train Epoch: 82 [6000/8040 (75%)]\tLoss: 91.687793\n",
      "Train Epoch: 82 [7200/8040 (90%)]\tLoss: 91.785091\n",
      "====> Epoch: 82 Average loss: 91.5388\n",
      "epoch: 82, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 83 [0/8040 (0%)]\tLoss: 91.663875\n",
      "Train Epoch: 83 [1200/8040 (15%)]\tLoss: 91.335026\n",
      "Train Epoch: 83 [2400/8040 (30%)]\tLoss: 91.239396\n",
      "Train Epoch: 83 [3600/8040 (45%)]\tLoss: 91.747762\n",
      "Train Epoch: 83 [4800/8040 (60%)]\tLoss: 91.603524\n",
      "Train Epoch: 83 [6000/8040 (75%)]\tLoss: 91.280721\n",
      "Train Epoch: 83 [7200/8040 (90%)]\tLoss: 91.467017\n",
      "====> Epoch: 83 Average loss: 91.5556\n",
      "epoch: 83, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 84 [0/8040 (0%)]\tLoss: 91.821265\n",
      "Train Epoch: 84 [1200/8040 (15%)]\tLoss: 91.820654\n",
      "Train Epoch: 84 [2400/8040 (30%)]\tLoss: 91.714437\n",
      "Train Epoch: 84 [3600/8040 (45%)]\tLoss: 91.672021\n",
      "Train Epoch: 84 [4800/8040 (60%)]\tLoss: 91.781665\n",
      "Train Epoch: 84 [6000/8040 (75%)]\tLoss: 91.255314\n",
      "Train Epoch: 84 [7200/8040 (90%)]\tLoss: 91.552425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 84 Average loss: 91.5737\n",
      "epoch: 84, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 85 [0/8040 (0%)]\tLoss: 91.237826\n",
      "Train Epoch: 85 [1200/8040 (15%)]\tLoss: 91.320931\n",
      "Train Epoch: 85 [2400/8040 (30%)]\tLoss: 91.658211\n",
      "Train Epoch: 85 [3600/8040 (45%)]\tLoss: 91.483545\n",
      "Train Epoch: 85 [4800/8040 (60%)]\tLoss: 91.785978\n",
      "Train Epoch: 85 [6000/8040 (75%)]\tLoss: 91.468953\n",
      "Train Epoch: 85 [7200/8040 (90%)]\tLoss: 91.946688\n",
      "====> Epoch: 85 Average loss: 91.5583\n",
      "epoch: 85, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 86 [0/8040 (0%)]\tLoss: 91.805054\n",
      "Train Epoch: 86 [1200/8040 (15%)]\tLoss: 91.945703\n",
      "Train Epoch: 86 [2400/8040 (30%)]\tLoss: 91.354712\n",
      "Train Epoch: 86 [3600/8040 (45%)]\tLoss: 91.516903\n",
      "Train Epoch: 86 [4800/8040 (60%)]\tLoss: 91.559367\n",
      "Train Epoch: 86 [6000/8040 (75%)]\tLoss: 91.512500\n",
      "Train Epoch: 86 [7200/8040 (90%)]\tLoss: 91.748397\n",
      "====> Epoch: 86 Average loss: 91.5489\n",
      "epoch: 86, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 87 [0/8040 (0%)]\tLoss: 91.832137\n",
      "Train Epoch: 87 [1200/8040 (15%)]\tLoss: 91.374845\n",
      "Train Epoch: 87 [2400/8040 (30%)]\tLoss: 91.740373\n",
      "Train Epoch: 87 [3600/8040 (45%)]\tLoss: 91.669792\n",
      "Train Epoch: 87 [4800/8040 (60%)]\tLoss: 91.164559\n",
      "Train Epoch: 87 [6000/8040 (75%)]\tLoss: 91.574813\n",
      "Train Epoch: 87 [7200/8040 (90%)]\tLoss: 91.561214\n",
      "====> Epoch: 87 Average loss: 91.5522\n",
      "epoch: 87, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 88 [0/8040 (0%)]\tLoss: 91.445719\n",
      "Train Epoch: 88 [1200/8040 (15%)]\tLoss: 91.186320\n",
      "Train Epoch: 88 [2400/8040 (30%)]\tLoss: 91.384017\n",
      "Train Epoch: 88 [3600/8040 (45%)]\tLoss: 91.742757\n",
      "Train Epoch: 88 [4800/8040 (60%)]\tLoss: 91.176847\n",
      "Train Epoch: 88 [6000/8040 (75%)]\tLoss: 91.305176\n",
      "Train Epoch: 88 [7200/8040 (90%)]\tLoss: 91.703394\n",
      "====> Epoch: 88 Average loss: 91.5403\n",
      "epoch: 88, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 89 [0/8040 (0%)]\tLoss: 90.992163\n",
      "Train Epoch: 89 [1200/8040 (15%)]\tLoss: 91.145190\n",
      "Train Epoch: 89 [2400/8040 (30%)]\tLoss: 91.818709\n",
      "Train Epoch: 89 [3600/8040 (45%)]\tLoss: 91.325114\n",
      "Train Epoch: 89 [4800/8040 (60%)]\tLoss: 91.615446\n",
      "Train Epoch: 89 [6000/8040 (75%)]\tLoss: 91.389282\n",
      "Train Epoch: 89 [7200/8040 (90%)]\tLoss: 90.981087\n",
      "====> Epoch: 89 Average loss: 91.5234\n",
      "epoch: 89, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 90 [0/8040 (0%)]\tLoss: 91.281494\n",
      "Train Epoch: 90 [1200/8040 (15%)]\tLoss: 91.442920\n",
      "Train Epoch: 90 [2400/8040 (30%)]\tLoss: 91.446029\n",
      "Train Epoch: 90 [3600/8040 (45%)]\tLoss: 91.399341\n",
      "Train Epoch: 90 [4800/8040 (60%)]\tLoss: 91.187703\n",
      "Train Epoch: 90 [6000/8040 (75%)]\tLoss: 91.469368\n",
      "Train Epoch: 90 [7200/8040 (90%)]\tLoss: 91.521281\n",
      "====> Epoch: 90 Average loss: 91.5681\n",
      "epoch: 90, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 91 [0/8040 (0%)]\tLoss: 91.500236\n",
      "Train Epoch: 91 [1200/8040 (15%)]\tLoss: 91.280371\n",
      "Train Epoch: 91 [2400/8040 (30%)]\tLoss: 91.631942\n",
      "Train Epoch: 91 [3600/8040 (45%)]\tLoss: 91.255461\n",
      "Train Epoch: 91 [4800/8040 (60%)]\tLoss: 91.280851\n",
      "Train Epoch: 91 [6000/8040 (75%)]\tLoss: 92.103133\n",
      "Train Epoch: 91 [7200/8040 (90%)]\tLoss: 91.235758\n",
      "====> Epoch: 91 Average loss: 91.5416\n",
      "epoch: 91, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 92 [0/8040 (0%)]\tLoss: 91.479370\n",
      "Train Epoch: 92 [1200/8040 (15%)]\tLoss: 91.675968\n",
      "Train Epoch: 92 [2400/8040 (30%)]\tLoss: 91.744230\n",
      "Train Epoch: 92 [3600/8040 (45%)]\tLoss: 91.100968\n",
      "Train Epoch: 92 [4800/8040 (60%)]\tLoss: 92.067651\n",
      "Train Epoch: 92 [6000/8040 (75%)]\tLoss: 91.460376\n",
      "Train Epoch: 92 [7200/8040 (90%)]\tLoss: 91.470239\n",
      "====> Epoch: 92 Average loss: 91.5295\n",
      "epoch: 92, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 93 [0/8040 (0%)]\tLoss: 91.627873\n",
      "Train Epoch: 93 [1200/8040 (15%)]\tLoss: 91.635091\n",
      "Train Epoch: 93 [2400/8040 (30%)]\tLoss: 91.852531\n",
      "Train Epoch: 93 [3600/8040 (45%)]\tLoss: 91.363477\n",
      "Train Epoch: 93 [4800/8040 (60%)]\tLoss: 91.419320\n",
      "Train Epoch: 93 [6000/8040 (75%)]\tLoss: 91.091911\n",
      "Train Epoch: 93 [7200/8040 (90%)]\tLoss: 91.344645\n",
      "====> Epoch: 93 Average loss: 91.5344\n",
      "epoch: 93, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 94 [0/8040 (0%)]\tLoss: 91.594629\n",
      "Train Epoch: 94 [1200/8040 (15%)]\tLoss: 91.253825\n",
      "Train Epoch: 94 [2400/8040 (30%)]\tLoss: 91.842318\n",
      "Train Epoch: 94 [3600/8040 (45%)]\tLoss: 91.129574\n",
      "Train Epoch: 94 [4800/8040 (60%)]\tLoss: 91.291781\n",
      "Train Epoch: 94 [6000/8040 (75%)]\tLoss: 91.072949\n",
      "Train Epoch: 94 [7200/8040 (90%)]\tLoss: 91.469067\n",
      "====> Epoch: 94 Average loss: 91.5358\n",
      "epoch: 94, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 95 [0/8040 (0%)]\tLoss: 91.699170\n",
      "Train Epoch: 95 [1200/8040 (15%)]\tLoss: 91.715747\n",
      "Train Epoch: 95 [2400/8040 (30%)]\tLoss: 91.422941\n",
      "Train Epoch: 95 [3600/8040 (45%)]\tLoss: 91.409880\n",
      "Train Epoch: 95 [4800/8040 (60%)]\tLoss: 91.524438\n",
      "Train Epoch: 95 [6000/8040 (75%)]\tLoss: 91.269051\n",
      "Train Epoch: 95 [7200/8040 (90%)]\tLoss: 91.422754\n",
      "====> Epoch: 95 Average loss: 91.5499\n",
      "epoch: 95, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 96 [0/8040 (0%)]\tLoss: 91.505493\n",
      "Train Epoch: 96 [1200/8040 (15%)]\tLoss: 91.734408\n",
      "Train Epoch: 96 [2400/8040 (30%)]\tLoss: 91.621143\n",
      "Train Epoch: 96 [3600/8040 (45%)]\tLoss: 91.507536\n",
      "Train Epoch: 96 [4800/8040 (60%)]\tLoss: 91.191553\n",
      "Train Epoch: 96 [6000/8040 (75%)]\tLoss: 91.470532\n",
      "Train Epoch: 96 [7200/8040 (90%)]\tLoss: 91.513558\n",
      "====> Epoch: 96 Average loss: 91.5446\n",
      "epoch: 96, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 97 [0/8040 (0%)]\tLoss: 91.487410\n",
      "Train Epoch: 97 [1200/8040 (15%)]\tLoss: 91.668555\n",
      "Train Epoch: 97 [2400/8040 (30%)]\tLoss: 91.370980\n",
      "Train Epoch: 97 [3600/8040 (45%)]\tLoss: 91.395036\n",
      "Train Epoch: 97 [4800/8040 (60%)]\tLoss: 91.248844\n",
      "Train Epoch: 97 [6000/8040 (75%)]\tLoss: 91.314624\n",
      "Train Epoch: 97 [7200/8040 (90%)]\tLoss: 91.691317\n",
      "====> Epoch: 97 Average loss: 91.5441\n",
      "epoch: 97, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 98 [0/8040 (0%)]\tLoss: 91.799943\n",
      "Train Epoch: 98 [1200/8040 (15%)]\tLoss: 91.423966\n",
      "Train Epoch: 98 [2400/8040 (30%)]\tLoss: 91.537573\n",
      "Train Epoch: 98 [3600/8040 (45%)]\tLoss: 91.513452\n",
      "Train Epoch: 98 [4800/8040 (60%)]\tLoss: 91.642049\n",
      "Train Epoch: 98 [6000/8040 (75%)]\tLoss: 91.667537\n",
      "Train Epoch: 98 [7200/8040 (90%)]\tLoss: 91.407967\n",
      "====> Epoch: 98 Average loss: 91.5500\n",
      "epoch: 98, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 99 [0/8040 (0%)]\tLoss: 91.548381\n",
      "Train Epoch: 99 [1200/8040 (15%)]\tLoss: 91.657178\n",
      "Train Epoch: 99 [2400/8040 (30%)]\tLoss: 91.582349\n",
      "Train Epoch: 99 [3600/8040 (45%)]\tLoss: 91.176750\n",
      "Train Epoch: 99 [4800/8040 (60%)]\tLoss: 91.523690\n",
      "Train Epoch: 99 [6000/8040 (75%)]\tLoss: 91.514958\n",
      "Train Epoch: 99 [7200/8040 (90%)]\tLoss: 91.533415\n",
      "====> Epoch: 99 Average loss: 91.5538\n",
      "epoch: 99, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 100 [0/8040 (0%)]\tLoss: 91.422965\n",
      "Train Epoch: 100 [1200/8040 (15%)]\tLoss: 91.427075\n",
      "Train Epoch: 100 [2400/8040 (30%)]\tLoss: 91.066772\n",
      "Train Epoch: 100 [3600/8040 (45%)]\tLoss: 91.472738\n",
      "Train Epoch: 100 [4800/8040 (60%)]\tLoss: 91.623120\n",
      "Train Epoch: 100 [6000/8040 (75%)]\tLoss: 91.620687\n",
      "Train Epoch: 100 [7200/8040 (90%)]\tLoss: 92.011580\n",
      "====> Epoch: 100 Average loss: 91.5339\n",
      "epoch: 100, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 101 [0/8040 (0%)]\tLoss: 92.058748\n",
      "Train Epoch: 101 [1200/8040 (15%)]\tLoss: 91.620858\n",
      "Train Epoch: 101 [2400/8040 (30%)]\tLoss: 91.756535\n",
      "Train Epoch: 101 [3600/8040 (45%)]\tLoss: 91.562728\n",
      "Train Epoch: 101 [4800/8040 (60%)]\tLoss: 91.461027\n",
      "Train Epoch: 101 [6000/8040 (75%)]\tLoss: 91.498088\n",
      "Train Epoch: 101 [7200/8040 (90%)]\tLoss: 91.389266\n",
      "====> Epoch: 101 Average loss: 91.5220\n",
      "epoch: 101, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 102 [0/8040 (0%)]\tLoss: 91.242830\n",
      "Train Epoch: 102 [1200/8040 (15%)]\tLoss: 92.021476\n",
      "Train Epoch: 102 [2400/8040 (30%)]\tLoss: 91.546240\n",
      "Train Epoch: 102 [3600/8040 (45%)]\tLoss: 91.670508\n",
      "Train Epoch: 102 [4800/8040 (60%)]\tLoss: 91.329940\n",
      "Train Epoch: 102 [6000/8040 (75%)]\tLoss: 91.061515\n",
      "Train Epoch: 102 [7200/8040 (90%)]\tLoss: 91.451652\n",
      "====> Epoch: 102 Average loss: 91.5272\n",
      "epoch: 102, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 103 [0/8040 (0%)]\tLoss: 92.040780\n",
      "Train Epoch: 103 [1200/8040 (15%)]\tLoss: 91.615340\n",
      "Train Epoch: 103 [2400/8040 (30%)]\tLoss: 91.652035\n",
      "Train Epoch: 103 [3600/8040 (45%)]\tLoss: 91.473031\n",
      "Train Epoch: 103 [4800/8040 (60%)]\tLoss: 91.435539\n",
      "Train Epoch: 103 [6000/8040 (75%)]\tLoss: 91.347070\n",
      "Train Epoch: 103 [7200/8040 (90%)]\tLoss: 91.612972\n",
      "====> Epoch: 103 Average loss: 91.5302\n",
      "epoch: 103, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 104 [0/8040 (0%)]\tLoss: 91.712939\n",
      "Train Epoch: 104 [1200/8040 (15%)]\tLoss: 91.399935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 104 [2400/8040 (30%)]\tLoss: 91.504118\n",
      "Train Epoch: 104 [3600/8040 (45%)]\tLoss: 91.519930\n",
      "Train Epoch: 104 [4800/8040 (60%)]\tLoss: 91.505688\n",
      "Train Epoch: 104 [6000/8040 (75%)]\tLoss: 91.610604\n",
      "Train Epoch: 104 [7200/8040 (90%)]\tLoss: 91.571370\n",
      "====> Epoch: 104 Average loss: 91.5283\n",
      "epoch: 104, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 105 [0/8040 (0%)]\tLoss: 91.341593\n",
      "Train Epoch: 105 [1200/8040 (15%)]\tLoss: 91.477726\n",
      "Train Epoch: 105 [2400/8040 (30%)]\tLoss: 91.705526\n",
      "Train Epoch: 105 [3600/8040 (45%)]\tLoss: 91.229622\n",
      "Train Epoch: 105 [4800/8040 (60%)]\tLoss: 91.321973\n",
      "Train Epoch: 105 [6000/8040 (75%)]\tLoss: 91.573177\n",
      "Train Epoch: 105 [7200/8040 (90%)]\tLoss: 91.997127\n",
      "====> Epoch: 105 Average loss: 91.5420\n",
      "epoch: 105, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 106 [0/8040 (0%)]\tLoss: 91.906405\n",
      "Train Epoch: 106 [1200/8040 (15%)]\tLoss: 91.446029\n",
      "Train Epoch: 106 [2400/8040 (30%)]\tLoss: 91.661702\n",
      "Train Epoch: 106 [3600/8040 (45%)]\tLoss: 91.532324\n",
      "Train Epoch: 106 [4800/8040 (60%)]\tLoss: 91.593807\n",
      "Train Epoch: 106 [6000/8040 (75%)]\tLoss: 91.621615\n",
      "Train Epoch: 106 [7200/8040 (90%)]\tLoss: 91.644051\n",
      "====> Epoch: 106 Average loss: 91.5522\n",
      "epoch: 106, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 107 [0/8040 (0%)]\tLoss: 91.633350\n",
      "Train Epoch: 107 [1200/8040 (15%)]\tLoss: 91.522013\n",
      "Train Epoch: 107 [2400/8040 (30%)]\tLoss: 91.414046\n",
      "Train Epoch: 107 [3600/8040 (45%)]\tLoss: 91.605200\n",
      "Train Epoch: 107 [4800/8040 (60%)]\tLoss: 91.246899\n",
      "Train Epoch: 107 [6000/8040 (75%)]\tLoss: 92.063713\n",
      "Train Epoch: 107 [7200/8040 (90%)]\tLoss: 91.895850\n",
      "====> Epoch: 107 Average loss: 91.5428\n",
      "epoch: 107, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 108 [0/8040 (0%)]\tLoss: 91.548055\n",
      "Train Epoch: 108 [1200/8040 (15%)]\tLoss: 91.572021\n",
      "Train Epoch: 108 [2400/8040 (30%)]\tLoss: 91.518490\n",
      "Train Epoch: 108 [3600/8040 (45%)]\tLoss: 91.396916\n",
      "Train Epoch: 108 [4800/8040 (60%)]\tLoss: 91.646338\n",
      "Train Epoch: 108 [6000/8040 (75%)]\tLoss: 91.709880\n",
      "Train Epoch: 108 [7200/8040 (90%)]\tLoss: 91.691357\n",
      "====> Epoch: 108 Average loss: 91.5158\n",
      "epoch: 108, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 109 [0/8040 (0%)]\tLoss: 91.780786\n",
      "Train Epoch: 109 [1200/8040 (15%)]\tLoss: 91.720654\n",
      "Train Epoch: 109 [2400/8040 (30%)]\tLoss: 91.306323\n",
      "Train Epoch: 109 [3600/8040 (45%)]\tLoss: 91.424178\n",
      "Train Epoch: 109 [4800/8040 (60%)]\tLoss: 91.590755\n",
      "Train Epoch: 109 [6000/8040 (75%)]\tLoss: 91.551725\n",
      "Train Epoch: 109 [7200/8040 (90%)]\tLoss: 91.277734\n",
      "====> Epoch: 109 Average loss: 91.5447\n",
      "epoch: 109, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 110 [0/8040 (0%)]\tLoss: 91.276465\n",
      "Train Epoch: 110 [1200/8040 (15%)]\tLoss: 91.785270\n",
      "Train Epoch: 110 [2400/8040 (30%)]\tLoss: 91.504183\n",
      "Train Epoch: 110 [3600/8040 (45%)]\tLoss: 91.246932\n",
      "Train Epoch: 110 [4800/8040 (60%)]\tLoss: 91.324520\n",
      "Train Epoch: 110 [6000/8040 (75%)]\tLoss: 91.747616\n",
      "Train Epoch: 110 [7200/8040 (90%)]\tLoss: 91.440226\n",
      "====> Epoch: 110 Average loss: 91.5124\n",
      "epoch: 110, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 111 [0/8040 (0%)]\tLoss: 91.690243\n",
      "Train Epoch: 111 [1200/8040 (15%)]\tLoss: 91.306104\n",
      "Train Epoch: 111 [2400/8040 (30%)]\tLoss: 91.900545\n",
      "Train Epoch: 111 [3600/8040 (45%)]\tLoss: 91.479175\n",
      "Train Epoch: 111 [4800/8040 (60%)]\tLoss: 91.443123\n",
      "Train Epoch: 111 [6000/8040 (75%)]\tLoss: 91.390373\n",
      "Train Epoch: 111 [7200/8040 (90%)]\tLoss: 91.154460\n",
      "====> Epoch: 111 Average loss: 91.5114\n",
      "epoch: 111, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 112 [0/8040 (0%)]\tLoss: 91.850212\n",
      "Train Epoch: 112 [1200/8040 (15%)]\tLoss: 91.685457\n",
      "Train Epoch: 112 [2400/8040 (30%)]\tLoss: 91.787769\n",
      "Train Epoch: 112 [3600/8040 (45%)]\tLoss: 91.441968\n",
      "Train Epoch: 112 [4800/8040 (60%)]\tLoss: 91.538664\n",
      "Train Epoch: 112 [6000/8040 (75%)]\tLoss: 91.418913\n",
      "Train Epoch: 112 [7200/8040 (90%)]\tLoss: 91.110506\n",
      "====> Epoch: 112 Average loss: 91.5310\n",
      "epoch: 112, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 113 [0/8040 (0%)]\tLoss: 91.629215\n",
      "Train Epoch: 113 [1200/8040 (15%)]\tLoss: 91.523739\n",
      "Train Epoch: 113 [2400/8040 (30%)]\tLoss: 91.967660\n",
      "Train Epoch: 113 [3600/8040 (45%)]\tLoss: 91.261043\n",
      "Train Epoch: 113 [4800/8040 (60%)]\tLoss: 91.311410\n",
      "Train Epoch: 113 [6000/8040 (75%)]\tLoss: 91.826628\n",
      "Train Epoch: 113 [7200/8040 (90%)]\tLoss: 91.586483\n",
      "====> Epoch: 113 Average loss: 91.5235\n",
      "epoch: 113, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 114 [0/8040 (0%)]\tLoss: 91.296883\n",
      "Train Epoch: 114 [1200/8040 (15%)]\tLoss: 91.580672\n",
      "Train Epoch: 114 [2400/8040 (30%)]\tLoss: 91.730705\n",
      "Train Epoch: 114 [3600/8040 (45%)]\tLoss: 91.813135\n",
      "Train Epoch: 114 [4800/8040 (60%)]\tLoss: 91.636336\n",
      "Train Epoch: 114 [6000/8040 (75%)]\tLoss: 91.604915\n",
      "Train Epoch: 114 [7200/8040 (90%)]\tLoss: 91.258480\n",
      "====> Epoch: 114 Average loss: 91.5307\n",
      "epoch: 114, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 115 [0/8040 (0%)]\tLoss: 91.611922\n",
      "Train Epoch: 115 [1200/8040 (15%)]\tLoss: 91.384481\n",
      "Train Epoch: 115 [2400/8040 (30%)]\tLoss: 91.253695\n",
      "Train Epoch: 115 [3600/8040 (45%)]\tLoss: 91.613916\n",
      "Train Epoch: 115 [4800/8040 (60%)]\tLoss: 91.377433\n",
      "Train Epoch: 115 [6000/8040 (75%)]\tLoss: 91.765576\n",
      "Train Epoch: 115 [7200/8040 (90%)]\tLoss: 91.007324\n",
      "====> Epoch: 115 Average loss: 91.5195\n",
      "epoch: 115, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 116 [0/8040 (0%)]\tLoss: 91.615967\n",
      "Train Epoch: 116 [1200/8040 (15%)]\tLoss: 91.760441\n",
      "Train Epoch: 116 [2400/8040 (30%)]\tLoss: 91.593758\n",
      "Train Epoch: 116 [3600/8040 (45%)]\tLoss: 91.426367\n",
      "Train Epoch: 116 [4800/8040 (60%)]\tLoss: 91.311100\n",
      "Train Epoch: 116 [6000/8040 (75%)]\tLoss: 91.699121\n",
      "Train Epoch: 116 [7200/8040 (90%)]\tLoss: 91.135010\n",
      "====> Epoch: 116 Average loss: 91.5275\n",
      "epoch: 116, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 117 [0/8040 (0%)]\tLoss: 91.224341\n",
      "Train Epoch: 117 [1200/8040 (15%)]\tLoss: 91.473958\n",
      "Train Epoch: 117 [2400/8040 (30%)]\tLoss: 91.420695\n",
      "Train Epoch: 117 [3600/8040 (45%)]\tLoss: 91.696549\n",
      "Train Epoch: 117 [4800/8040 (60%)]\tLoss: 91.937891\n",
      "Train Epoch: 117 [6000/8040 (75%)]\tLoss: 91.150000\n",
      "Train Epoch: 117 [7200/8040 (90%)]\tLoss: 91.422754\n",
      "====> Epoch: 117 Average loss: 91.5346\n",
      "epoch: 117, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 118 [0/8040 (0%)]\tLoss: 91.417725\n",
      "Train Epoch: 118 [1200/8040 (15%)]\tLoss: 91.685767\n",
      "Train Epoch: 118 [2400/8040 (30%)]\tLoss: 91.797331\n",
      "Train Epoch: 118 [3600/8040 (45%)]\tLoss: 91.394946\n",
      "Train Epoch: 118 [4800/8040 (60%)]\tLoss: 91.593799\n",
      "Train Epoch: 118 [6000/8040 (75%)]\tLoss: 91.444312\n",
      "Train Epoch: 118 [7200/8040 (90%)]\tLoss: 91.606022\n",
      "====> Epoch: 118 Average loss: 91.5222\n",
      "epoch: 118, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 119 [0/8040 (0%)]\tLoss: 91.345256\n",
      "Train Epoch: 119 [1200/8040 (15%)]\tLoss: 91.764771\n",
      "Train Epoch: 119 [2400/8040 (30%)]\tLoss: 91.430143\n",
      "Train Epoch: 119 [3600/8040 (45%)]\tLoss: 91.313599\n",
      "Train Epoch: 119 [4800/8040 (60%)]\tLoss: 91.808724\n",
      "Train Epoch: 119 [6000/8040 (75%)]\tLoss: 91.541935\n",
      "Train Epoch: 119 [7200/8040 (90%)]\tLoss: 91.447681\n",
      "====> Epoch: 119 Average loss: 91.5324\n",
      "epoch: 119, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 120 [0/8040 (0%)]\tLoss: 91.294621\n",
      "Train Epoch: 120 [1200/8040 (15%)]\tLoss: 91.540910\n",
      "Train Epoch: 120 [2400/8040 (30%)]\tLoss: 91.407088\n",
      "Train Epoch: 120 [3600/8040 (45%)]\tLoss: 91.877856\n",
      "Train Epoch: 120 [4800/8040 (60%)]\tLoss: 91.500863\n",
      "Train Epoch: 120 [6000/8040 (75%)]\tLoss: 91.356193\n",
      "Train Epoch: 120 [7200/8040 (90%)]\tLoss: 91.386174\n",
      "====> Epoch: 120 Average loss: 91.5207\n",
      "epoch: 120, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 121 [0/8040 (0%)]\tLoss: 91.112280\n",
      "Train Epoch: 121 [1200/8040 (15%)]\tLoss: 91.172567\n",
      "Train Epoch: 121 [2400/8040 (30%)]\tLoss: 91.218514\n",
      "Train Epoch: 121 [3600/8040 (45%)]\tLoss: 91.484359\n",
      "Train Epoch: 121 [4800/8040 (60%)]\tLoss: 91.643734\n",
      "Train Epoch: 121 [6000/8040 (75%)]\tLoss: 90.984082\n",
      "Train Epoch: 121 [7200/8040 (90%)]\tLoss: 91.340576\n",
      "====> Epoch: 121 Average loss: 91.5020\n",
      "epoch: 121, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 122 [0/8040 (0%)]\tLoss: 91.298169\n",
      "Train Epoch: 122 [1200/8040 (15%)]\tLoss: 91.284871\n",
      "Train Epoch: 122 [2400/8040 (30%)]\tLoss: 91.344010\n",
      "Train Epoch: 122 [3600/8040 (45%)]\tLoss: 91.282878\n",
      "Train Epoch: 122 [4800/8040 (60%)]\tLoss: 91.575911\n",
      "Train Epoch: 122 [6000/8040 (75%)]\tLoss: 92.102132\n",
      "Train Epoch: 122 [7200/8040 (90%)]\tLoss: 91.700415\n",
      "====> Epoch: 122 Average loss: 91.5218\n",
      "epoch: 122, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 123 [0/8040 (0%)]\tLoss: 91.325684\n",
      "Train Epoch: 123 [1200/8040 (15%)]\tLoss: 91.231087\n",
      "Train Epoch: 123 [2400/8040 (30%)]\tLoss: 91.277555\n",
      "Train Epoch: 123 [3600/8040 (45%)]\tLoss: 91.616382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 123 [4800/8040 (60%)]\tLoss: 91.146411\n",
      "Train Epoch: 123 [6000/8040 (75%)]\tLoss: 91.220866\n",
      "Train Epoch: 123 [7200/8040 (90%)]\tLoss: 91.555786\n",
      "====> Epoch: 123 Average loss: 91.5038\n",
      "epoch: 123, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 124 [0/8040 (0%)]\tLoss: 91.397965\n",
      "Train Epoch: 124 [1200/8040 (15%)]\tLoss: 92.105965\n",
      "Train Epoch: 124 [2400/8040 (30%)]\tLoss: 91.328369\n",
      "Train Epoch: 124 [3600/8040 (45%)]\tLoss: 90.960433\n",
      "Train Epoch: 124 [4800/8040 (60%)]\tLoss: 91.178247\n",
      "Train Epoch: 124 [6000/8040 (75%)]\tLoss: 91.540080\n",
      "Train Epoch: 124 [7200/8040 (90%)]\tLoss: 91.056698\n",
      "====> Epoch: 124 Average loss: 91.5285\n",
      "epoch: 124, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 125 [0/8040 (0%)]\tLoss: 91.958464\n",
      "Train Epoch: 125 [1200/8040 (15%)]\tLoss: 91.630037\n",
      "Train Epoch: 125 [2400/8040 (30%)]\tLoss: 91.186361\n",
      "Train Epoch: 125 [3600/8040 (45%)]\tLoss: 91.275252\n",
      "Train Epoch: 125 [4800/8040 (60%)]\tLoss: 91.598193\n",
      "Train Epoch: 125 [6000/8040 (75%)]\tLoss: 91.350968\n",
      "Train Epoch: 125 [7200/8040 (90%)]\tLoss: 91.407251\n",
      "====> Epoch: 125 Average loss: 91.4994\n",
      "epoch: 125, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 126 [0/8040 (0%)]\tLoss: 91.815373\n",
      "Train Epoch: 126 [1200/8040 (15%)]\tLoss: 91.302466\n",
      "Train Epoch: 126 [2400/8040 (30%)]\tLoss: 91.442814\n",
      "Train Epoch: 126 [3600/8040 (45%)]\tLoss: 91.425122\n",
      "Train Epoch: 126 [4800/8040 (60%)]\tLoss: 91.434969\n",
      "Train Epoch: 126 [6000/8040 (75%)]\tLoss: 91.489705\n",
      "Train Epoch: 126 [7200/8040 (90%)]\tLoss: 91.022819\n",
      "====> Epoch: 126 Average loss: 91.5073\n",
      "epoch: 126, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 127 [0/8040 (0%)]\tLoss: 91.496720\n",
      "Train Epoch: 127 [1200/8040 (15%)]\tLoss: 91.639657\n",
      "Train Epoch: 127 [2400/8040 (30%)]\tLoss: 91.716398\n",
      "Train Epoch: 127 [3600/8040 (45%)]\tLoss: 91.382804\n",
      "Train Epoch: 127 [4800/8040 (60%)]\tLoss: 91.672314\n",
      "Train Epoch: 127 [6000/8040 (75%)]\tLoss: 91.661678\n",
      "Train Epoch: 127 [7200/8040 (90%)]\tLoss: 91.561906\n",
      "====> Epoch: 127 Average loss: 91.5221\n",
      "epoch: 127, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 128 [0/8040 (0%)]\tLoss: 91.842391\n",
      "Train Epoch: 128 [1200/8040 (15%)]\tLoss: 91.706877\n",
      "Train Epoch: 128 [2400/8040 (30%)]\tLoss: 91.166056\n",
      "Train Epoch: 128 [3600/8040 (45%)]\tLoss: 90.876449\n",
      "Train Epoch: 128 [4800/8040 (60%)]\tLoss: 91.847404\n",
      "Train Epoch: 128 [6000/8040 (75%)]\tLoss: 91.694377\n",
      "Train Epoch: 128 [7200/8040 (90%)]\tLoss: 91.595068\n",
      "====> Epoch: 128 Average loss: 91.5259\n",
      "epoch: 128, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 129 [0/8040 (0%)]\tLoss: 91.228963\n",
      "Train Epoch: 129 [1200/8040 (15%)]\tLoss: 91.494002\n",
      "Train Epoch: 129 [2400/8040 (30%)]\tLoss: 91.254631\n",
      "Train Epoch: 129 [3600/8040 (45%)]\tLoss: 91.664225\n",
      "Train Epoch: 129 [4800/8040 (60%)]\tLoss: 91.611353\n",
      "Train Epoch: 129 [6000/8040 (75%)]\tLoss: 91.548454\n",
      "Train Epoch: 129 [7200/8040 (90%)]\tLoss: 91.408732\n",
      "====> Epoch: 129 Average loss: 91.5092\n",
      "epoch: 129, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 130 [0/8040 (0%)]\tLoss: 91.739640\n",
      "Train Epoch: 130 [1200/8040 (15%)]\tLoss: 91.351847\n",
      "Train Epoch: 130 [2400/8040 (30%)]\tLoss: 91.648739\n",
      "Train Epoch: 130 [3600/8040 (45%)]\tLoss: 91.968441\n",
      "Train Epoch: 130 [4800/8040 (60%)]\tLoss: 91.429582\n",
      "Train Epoch: 130 [6000/8040 (75%)]\tLoss: 91.649015\n",
      "Train Epoch: 130 [7200/8040 (90%)]\tLoss: 91.449333\n",
      "====> Epoch: 130 Average loss: 91.5339\n",
      "epoch: 130, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 131 [0/8040 (0%)]\tLoss: 90.982251\n",
      "Train Epoch: 131 [1200/8040 (15%)]\tLoss: 91.725496\n",
      "Train Epoch: 131 [2400/8040 (30%)]\tLoss: 91.554476\n",
      "Train Epoch: 131 [3600/8040 (45%)]\tLoss: 91.785628\n",
      "Train Epoch: 131 [4800/8040 (60%)]\tLoss: 91.458024\n",
      "Train Epoch: 131 [6000/8040 (75%)]\tLoss: 91.328752\n",
      "Train Epoch: 131 [7200/8040 (90%)]\tLoss: 91.171810\n",
      "====> Epoch: 131 Average loss: 91.4978\n",
      "epoch: 131, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 132 [0/8040 (0%)]\tLoss: 91.758472\n",
      "Train Epoch: 132 [1200/8040 (15%)]\tLoss: 91.882243\n",
      "Train Epoch: 132 [2400/8040 (30%)]\tLoss: 91.207324\n",
      "Train Epoch: 132 [3600/8040 (45%)]\tLoss: 91.554256\n",
      "Train Epoch: 132 [4800/8040 (60%)]\tLoss: 91.532520\n",
      "Train Epoch: 132 [6000/8040 (75%)]\tLoss: 91.720866\n",
      "Train Epoch: 132 [7200/8040 (90%)]\tLoss: 91.814079\n",
      "====> Epoch: 132 Average loss: 91.5145\n",
      "epoch: 132, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 133 [0/8040 (0%)]\tLoss: 91.544263\n",
      "Train Epoch: 133 [1200/8040 (15%)]\tLoss: 91.365470\n",
      "Train Epoch: 133 [2400/8040 (30%)]\tLoss: 91.717220\n",
      "Train Epoch: 133 [3600/8040 (45%)]\tLoss: 91.803353\n",
      "Train Epoch: 133 [4800/8040 (60%)]\tLoss: 91.635229\n",
      "Train Epoch: 133 [6000/8040 (75%)]\tLoss: 91.829085\n",
      "Train Epoch: 133 [7200/8040 (90%)]\tLoss: 91.358219\n",
      "====> Epoch: 133 Average loss: 91.5176\n",
      "epoch: 133, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 134 [0/8040 (0%)]\tLoss: 91.712492\n",
      "Train Epoch: 134 [1200/8040 (15%)]\tLoss: 91.653459\n",
      "Train Epoch: 134 [2400/8040 (30%)]\tLoss: 91.455615\n",
      "Train Epoch: 134 [3600/8040 (45%)]\tLoss: 91.701961\n",
      "Train Epoch: 134 [4800/8040 (60%)]\tLoss: 91.490723\n",
      "Train Epoch: 134 [6000/8040 (75%)]\tLoss: 91.193652\n",
      "Train Epoch: 134 [7200/8040 (90%)]\tLoss: 91.697819\n",
      "====> Epoch: 134 Average loss: 91.5007\n",
      "epoch: 134, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 135 [0/8040 (0%)]\tLoss: 91.415413\n",
      "Train Epoch: 135 [1200/8040 (15%)]\tLoss: 91.648161\n",
      "Train Epoch: 135 [2400/8040 (30%)]\tLoss: 91.766943\n",
      "Train Epoch: 135 [3600/8040 (45%)]\tLoss: 91.665234\n",
      "Train Epoch: 135 [4800/8040 (60%)]\tLoss: 91.672729\n",
      "Train Epoch: 135 [6000/8040 (75%)]\tLoss: 91.423869\n",
      "Train Epoch: 135 [7200/8040 (90%)]\tLoss: 91.803328\n",
      "====> Epoch: 135 Average loss: 91.5038\n",
      "epoch: 135, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 136 [0/8040 (0%)]\tLoss: 91.887736\n",
      "Train Epoch: 136 [1200/8040 (15%)]\tLoss: 91.637199\n",
      "Train Epoch: 136 [2400/8040 (30%)]\tLoss: 91.418262\n",
      "Train Epoch: 136 [3600/8040 (45%)]\tLoss: 91.370687\n",
      "Train Epoch: 136 [4800/8040 (60%)]\tLoss: 91.939681\n",
      "Train Epoch: 136 [6000/8040 (75%)]\tLoss: 91.441960\n",
      "Train Epoch: 136 [7200/8040 (90%)]\tLoss: 91.735173\n",
      "====> Epoch: 136 Average loss: 91.5171\n",
      "epoch: 136, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 137 [0/8040 (0%)]\tLoss: 91.720394\n",
      "Train Epoch: 137 [1200/8040 (15%)]\tLoss: 91.259220\n",
      "Train Epoch: 137 [2400/8040 (30%)]\tLoss: 91.584310\n",
      "Train Epoch: 137 [3600/8040 (45%)]\tLoss: 91.595125\n",
      "Train Epoch: 137 [4800/8040 (60%)]\tLoss: 91.608415\n",
      "Train Epoch: 137 [6000/8040 (75%)]\tLoss: 91.900838\n",
      "Train Epoch: 137 [7200/8040 (90%)]\tLoss: 91.236100\n",
      "====> Epoch: 137 Average loss: 91.5025\n",
      "epoch: 137, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 138 [0/8040 (0%)]\tLoss: 91.798918\n",
      "Train Epoch: 138 [1200/8040 (15%)]\tLoss: 91.504932\n",
      "Train Epoch: 138 [2400/8040 (30%)]\tLoss: 91.579289\n",
      "Train Epoch: 138 [3600/8040 (45%)]\tLoss: 91.354004\n",
      "Train Epoch: 138 [4800/8040 (60%)]\tLoss: 92.109749\n",
      "Train Epoch: 138 [6000/8040 (75%)]\tLoss: 91.499601\n",
      "Train Epoch: 138 [7200/8040 (90%)]\tLoss: 91.515568\n",
      "====> Epoch: 138 Average loss: 91.5110\n",
      "epoch: 138, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 139 [0/8040 (0%)]\tLoss: 91.251335\n",
      "Train Epoch: 139 [1200/8040 (15%)]\tLoss: 91.645793\n",
      "Train Epoch: 139 [2400/8040 (30%)]\tLoss: 91.600553\n",
      "Train Epoch: 139 [3600/8040 (45%)]\tLoss: 91.507430\n",
      "Train Epoch: 139 [4800/8040 (60%)]\tLoss: 91.489705\n",
      "Train Epoch: 139 [6000/8040 (75%)]\tLoss: 91.435498\n",
      "Train Epoch: 139 [7200/8040 (90%)]\tLoss: 91.370841\n",
      "====> Epoch: 139 Average loss: 91.4787\n",
      "epoch: 139, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 140 [0/8040 (0%)]\tLoss: 91.631144\n",
      "Train Epoch: 140 [1200/8040 (15%)]\tLoss: 91.779370\n",
      "Train Epoch: 140 [2400/8040 (30%)]\tLoss: 91.696501\n",
      "Train Epoch: 140 [3600/8040 (45%)]\tLoss: 91.292773\n",
      "Train Epoch: 140 [4800/8040 (60%)]\tLoss: 91.837565\n",
      "Train Epoch: 140 [6000/8040 (75%)]\tLoss: 91.520011\n",
      "Train Epoch: 140 [7200/8040 (90%)]\tLoss: 91.782609\n",
      "====> Epoch: 140 Average loss: 91.5131\n",
      "epoch: 140, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 141 [0/8040 (0%)]\tLoss: 91.203687\n",
      "Train Epoch: 141 [1200/8040 (15%)]\tLoss: 91.531868\n",
      "Train Epoch: 141 [2400/8040 (30%)]\tLoss: 91.524772\n",
      "Train Epoch: 141 [3600/8040 (45%)]\tLoss: 91.458398\n",
      "Train Epoch: 141 [4800/8040 (60%)]\tLoss: 91.417570\n",
      "Train Epoch: 141 [6000/8040 (75%)]\tLoss: 91.464038\n",
      "Train Epoch: 141 [7200/8040 (90%)]\tLoss: 91.311759\n",
      "====> Epoch: 141 Average loss: 91.4977\n",
      "epoch: 141, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 142 [0/8040 (0%)]\tLoss: 91.528109\n",
      "Train Epoch: 142 [1200/8040 (15%)]\tLoss: 91.592244\n",
      "Train Epoch: 142 [2400/8040 (30%)]\tLoss: 91.463965\n",
      "Train Epoch: 142 [3600/8040 (45%)]\tLoss: 91.307764\n",
      "Train Epoch: 142 [4800/8040 (60%)]\tLoss: 91.446802\n",
      "Train Epoch: 142 [6000/8040 (75%)]\tLoss: 91.754435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 142 [7200/8040 (90%)]\tLoss: 91.587606\n",
      "====> Epoch: 142 Average loss: 91.5058\n",
      "epoch: 142, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 143 [0/8040 (0%)]\tLoss: 91.217424\n",
      "Train Epoch: 143 [1200/8040 (15%)]\tLoss: 91.564575\n",
      "Train Epoch: 143 [2400/8040 (30%)]\tLoss: 91.295671\n",
      "Train Epoch: 143 [3600/8040 (45%)]\tLoss: 91.394938\n",
      "Train Epoch: 143 [4800/8040 (60%)]\tLoss: 91.748283\n",
      "Train Epoch: 143 [6000/8040 (75%)]\tLoss: 91.528141\n",
      "Train Epoch: 143 [7200/8040 (90%)]\tLoss: 91.615023\n",
      "====> Epoch: 143 Average loss: 91.4982\n",
      "epoch: 143, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 144 [0/8040 (0%)]\tLoss: 91.179525\n",
      "Train Epoch: 144 [1200/8040 (15%)]\tLoss: 91.846273\n",
      "Train Epoch: 144 [2400/8040 (30%)]\tLoss: 91.681771\n",
      "Train Epoch: 144 [3600/8040 (45%)]\tLoss: 91.513721\n",
      "Train Epoch: 144 [4800/8040 (60%)]\tLoss: 91.399259\n",
      "Train Epoch: 144 [6000/8040 (75%)]\tLoss: 91.673771\n",
      "Train Epoch: 144 [7200/8040 (90%)]\tLoss: 91.977702\n",
      "====> Epoch: 144 Average loss: 91.5049\n",
      "epoch: 144, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 145 [0/8040 (0%)]\tLoss: 91.349316\n",
      "Train Epoch: 145 [1200/8040 (15%)]\tLoss: 91.302726\n",
      "Train Epoch: 145 [2400/8040 (30%)]\tLoss: 91.553304\n",
      "Train Epoch: 145 [3600/8040 (45%)]\tLoss: 91.231014\n",
      "Train Epoch: 145 [4800/8040 (60%)]\tLoss: 91.342497\n",
      "Train Epoch: 145 [6000/8040 (75%)]\tLoss: 91.325814\n",
      "Train Epoch: 145 [7200/8040 (90%)]\tLoss: 91.258879\n",
      "====> Epoch: 145 Average loss: 91.4951\n",
      "epoch: 145, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 146 [0/8040 (0%)]\tLoss: 91.360726\n",
      "Train Epoch: 146 [1200/8040 (15%)]\tLoss: 91.602327\n",
      "Train Epoch: 146 [2400/8040 (30%)]\tLoss: 91.480941\n",
      "Train Epoch: 146 [3600/8040 (45%)]\tLoss: 91.317342\n",
      "Train Epoch: 146 [4800/8040 (60%)]\tLoss: 91.490788\n",
      "Train Epoch: 146 [6000/8040 (75%)]\tLoss: 91.777116\n",
      "Train Epoch: 146 [7200/8040 (90%)]\tLoss: 91.556307\n",
      "====> Epoch: 146 Average loss: 91.5111\n",
      "epoch: 146, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 147 [0/8040 (0%)]\tLoss: 91.540120\n",
      "Train Epoch: 147 [1200/8040 (15%)]\tLoss: 91.278133\n",
      "Train Epoch: 147 [2400/8040 (30%)]\tLoss: 91.277832\n",
      "Train Epoch: 147 [3600/8040 (45%)]\tLoss: 91.526953\n",
      "Train Epoch: 147 [4800/8040 (60%)]\tLoss: 91.711751\n",
      "Train Epoch: 147 [6000/8040 (75%)]\tLoss: 91.649544\n",
      "Train Epoch: 147 [7200/8040 (90%)]\tLoss: 91.732503\n",
      "====> Epoch: 147 Average loss: 91.5092\n",
      "epoch: 147, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 148 [0/8040 (0%)]\tLoss: 91.255322\n",
      "Train Epoch: 148 [1200/8040 (15%)]\tLoss: 91.252490\n",
      "Train Epoch: 148 [2400/8040 (30%)]\tLoss: 91.646541\n",
      "Train Epoch: 148 [3600/8040 (45%)]\tLoss: 91.703174\n",
      "Train Epoch: 148 [4800/8040 (60%)]\tLoss: 91.806779\n",
      "Train Epoch: 148 [6000/8040 (75%)]\tLoss: 91.471737\n",
      "Train Epoch: 148 [7200/8040 (90%)]\tLoss: 91.488403\n",
      "====> Epoch: 148 Average loss: 91.4947\n",
      "epoch: 148, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 149 [0/8040 (0%)]\tLoss: 91.615063\n",
      "Train Epoch: 149 [1200/8040 (15%)]\tLoss: 91.585189\n",
      "Train Epoch: 149 [2400/8040 (30%)]\tLoss: 91.412028\n",
      "Train Epoch: 149 [3600/8040 (45%)]\tLoss: 91.516252\n",
      "Train Epoch: 149 [4800/8040 (60%)]\tLoss: 91.297152\n",
      "Train Epoch: 149 [6000/8040 (75%)]\tLoss: 91.279305\n",
      "Train Epoch: 149 [7200/8040 (90%)]\tLoss: 91.881991\n",
      "====> Epoch: 149 Average loss: 91.5301\n",
      "epoch: 149, beta=0.01, frac_anom=0.01\n",
      "Train Epoch: 150 [0/8040 (0%)]\tLoss: 91.401896\n",
      "Train Epoch: 150 [1200/8040 (15%)]\tLoss: 91.292798\n",
      "Train Epoch: 150 [2400/8040 (30%)]\tLoss: 91.338623\n",
      "Train Epoch: 150 [3600/8040 (45%)]\tLoss: 91.380200\n",
      "Train Epoch: 150 [4800/8040 (60%)]\tLoss: 91.017350\n",
      "Train Epoch: 150 [6000/8040 (75%)]\tLoss: 91.521281\n",
      "Train Epoch: 150 [7200/8040 (90%)]\tLoss: 91.344173\n",
      "====> Epoch: 150 Average loss: 91.4938\n",
      "epoch: 150, beta=0.01, frac_anom=0.01\n",
      "====> Test set loss: 23.8267\n",
      "Train Epoch: 1 [0/8040 (0%)]\tLoss: 100.176164\n",
      "Train Epoch: 1 [1200/8040 (15%)]\tLoss: 93.943319\n",
      "Train Epoch: 1 [2400/8040 (30%)]\tLoss: 93.892993\n",
      "Train Epoch: 1 [3600/8040 (45%)]\tLoss: 93.904175\n",
      "Train Epoch: 1 [4800/8040 (60%)]\tLoss: 93.395378\n",
      "Train Epoch: 1 [6000/8040 (75%)]\tLoss: 93.470011\n",
      "Train Epoch: 1 [7200/8040 (90%)]\tLoss: 92.839876\n",
      "====> Epoch: 1 Average loss: 93.9053\n",
      "epoch: 1, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 2 [0/8040 (0%)]\tLoss: 92.863249\n",
      "Train Epoch: 2 [1200/8040 (15%)]\tLoss: 93.250155\n",
      "Train Epoch: 2 [2400/8040 (30%)]\tLoss: 93.029191\n",
      "Train Epoch: 2 [3600/8040 (45%)]\tLoss: 92.569312\n",
      "Train Epoch: 2 [4800/8040 (60%)]\tLoss: 93.074967\n",
      "Train Epoch: 2 [6000/8040 (75%)]\tLoss: 92.561434\n",
      "Train Epoch: 2 [7200/8040 (90%)]\tLoss: 93.153271\n",
      "====> Epoch: 2 Average loss: 92.9187\n",
      "epoch: 2, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 3 [0/8040 (0%)]\tLoss: 93.311068\n",
      "Train Epoch: 3 [1200/8040 (15%)]\tLoss: 92.586947\n",
      "Train Epoch: 3 [2400/8040 (30%)]\tLoss: 92.220874\n",
      "Train Epoch: 3 [3600/8040 (45%)]\tLoss: 92.705233\n",
      "Train Epoch: 3 [4800/8040 (60%)]\tLoss: 92.722201\n",
      "Train Epoch: 3 [6000/8040 (75%)]\tLoss: 92.422030\n",
      "Train Epoch: 3 [7200/8040 (90%)]\tLoss: 92.358382\n",
      "====> Epoch: 3 Average loss: 92.6563\n",
      "epoch: 3, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 4 [0/8040 (0%)]\tLoss: 92.546322\n",
      "Train Epoch: 4 [1200/8040 (15%)]\tLoss: 92.009196\n",
      "Train Epoch: 4 [2400/8040 (30%)]\tLoss: 92.195020\n",
      "Train Epoch: 4 [3600/8040 (45%)]\tLoss: 91.963192\n",
      "Train Epoch: 4 [4800/8040 (60%)]\tLoss: 92.234985\n",
      "Train Epoch: 4 [6000/8040 (75%)]\tLoss: 92.755428\n",
      "Train Epoch: 4 [7200/8040 (90%)]\tLoss: 92.076652\n",
      "====> Epoch: 4 Average loss: 92.4155\n",
      "epoch: 4, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 5 [0/8040 (0%)]\tLoss: 92.282796\n",
      "Train Epoch: 5 [1200/8040 (15%)]\tLoss: 92.213224\n",
      "Train Epoch: 5 [2400/8040 (30%)]\tLoss: 91.748674\n",
      "Train Epoch: 5 [3600/8040 (45%)]\tLoss: 92.301310\n",
      "Train Epoch: 5 [4800/8040 (60%)]\tLoss: 92.808065\n",
      "Train Epoch: 5 [6000/8040 (75%)]\tLoss: 92.848657\n",
      "Train Epoch: 5 [7200/8040 (90%)]\tLoss: 92.398389\n",
      "====> Epoch: 5 Average loss: 92.3095\n",
      "epoch: 5, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 6 [0/8040 (0%)]\tLoss: 92.307926\n",
      "Train Epoch: 6 [1200/8040 (15%)]\tLoss: 92.465519\n",
      "Train Epoch: 6 [2400/8040 (30%)]\tLoss: 92.472021\n",
      "Train Epoch: 6 [3600/8040 (45%)]\tLoss: 92.221948\n",
      "Train Epoch: 6 [4800/8040 (60%)]\tLoss: 91.797103\n",
      "Train Epoch: 6 [6000/8040 (75%)]\tLoss: 92.177157\n",
      "Train Epoch: 6 [7200/8040 (90%)]\tLoss: 92.074886\n",
      "====> Epoch: 6 Average loss: 92.2598\n",
      "epoch: 6, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 7 [0/8040 (0%)]\tLoss: 92.422119\n",
      "Train Epoch: 7 [1200/8040 (15%)]\tLoss: 92.165267\n",
      "Train Epoch: 7 [2400/8040 (30%)]\tLoss: 92.457951\n",
      "Train Epoch: 7 [3600/8040 (45%)]\tLoss: 92.587419\n",
      "Train Epoch: 7 [4800/8040 (60%)]\tLoss: 92.284652\n",
      "Train Epoch: 7 [6000/8040 (75%)]\tLoss: 92.449927\n",
      "Train Epoch: 7 [7200/8040 (90%)]\tLoss: 92.364502\n",
      "====> Epoch: 7 Average loss: 92.1923\n",
      "epoch: 7, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 8 [0/8040 (0%)]\tLoss: 91.876351\n",
      "Train Epoch: 8 [1200/8040 (15%)]\tLoss: 92.258643\n",
      "Train Epoch: 8 [2400/8040 (30%)]\tLoss: 92.457511\n",
      "Train Epoch: 8 [3600/8040 (45%)]\tLoss: 92.513867\n",
      "Train Epoch: 8 [4800/8040 (60%)]\tLoss: 92.017798\n",
      "Train Epoch: 8 [6000/8040 (75%)]\tLoss: 92.245288\n",
      "Train Epoch: 8 [7200/8040 (90%)]\tLoss: 91.761287\n",
      "====> Epoch: 8 Average loss: 92.1778\n",
      "epoch: 8, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 9 [0/8040 (0%)]\tLoss: 91.721802\n",
      "Train Epoch: 9 [1200/8040 (15%)]\tLoss: 92.220190\n",
      "Train Epoch: 9 [2400/8040 (30%)]\tLoss: 92.210384\n",
      "Train Epoch: 9 [3600/8040 (45%)]\tLoss: 92.093945\n",
      "Train Epoch: 9 [4800/8040 (60%)]\tLoss: 91.577010\n",
      "Train Epoch: 9 [6000/8040 (75%)]\tLoss: 92.077010\n",
      "Train Epoch: 9 [7200/8040 (90%)]\tLoss: 92.162240\n",
      "====> Epoch: 9 Average loss: 92.1444\n",
      "epoch: 9, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 10 [0/8040 (0%)]\tLoss: 92.459253\n",
      "Train Epoch: 10 [1200/8040 (15%)]\tLoss: 92.087533\n",
      "Train Epoch: 10 [2400/8040 (30%)]\tLoss: 92.298877\n",
      "Train Epoch: 10 [3600/8040 (45%)]\tLoss: 91.827384\n",
      "Train Epoch: 10 [4800/8040 (60%)]\tLoss: 92.251693\n",
      "Train Epoch: 10 [6000/8040 (75%)]\tLoss: 91.972843\n",
      "Train Epoch: 10 [7200/8040 (90%)]\tLoss: 92.110278\n",
      "====> Epoch: 10 Average loss: 92.1179\n",
      "epoch: 10, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 11 [0/8040 (0%)]\tLoss: 92.314990\n",
      "Train Epoch: 11 [1200/8040 (15%)]\tLoss: 92.290234\n",
      "Train Epoch: 11 [2400/8040 (30%)]\tLoss: 91.639559\n",
      "Train Epoch: 11 [3600/8040 (45%)]\tLoss: 92.237036\n",
      "Train Epoch: 11 [4800/8040 (60%)]\tLoss: 92.007902\n",
      "Train Epoch: 11 [6000/8040 (75%)]\tLoss: 92.334025\n",
      "Train Epoch: 11 [7200/8040 (90%)]\tLoss: 92.661442\n",
      "====> Epoch: 11 Average loss: 92.1101\n",
      "epoch: 11, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 12 [0/8040 (0%)]\tLoss: 92.133024\n",
      "Train Epoch: 12 [1200/8040 (15%)]\tLoss: 92.040308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [2400/8040 (30%)]\tLoss: 92.042334\n",
      "Train Epoch: 12 [3600/8040 (45%)]\tLoss: 91.987752\n",
      "Train Epoch: 12 [4800/8040 (60%)]\tLoss: 92.089355\n",
      "Train Epoch: 12 [6000/8040 (75%)]\tLoss: 91.817464\n",
      "Train Epoch: 12 [7200/8040 (90%)]\tLoss: 92.087492\n",
      "====> Epoch: 12 Average loss: 92.0820\n",
      "epoch: 12, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 13 [0/8040 (0%)]\tLoss: 92.306641\n",
      "Train Epoch: 13 [1200/8040 (15%)]\tLoss: 92.036776\n",
      "Train Epoch: 13 [2400/8040 (30%)]\tLoss: 92.076034\n",
      "Train Epoch: 13 [3600/8040 (45%)]\tLoss: 92.015503\n",
      "Train Epoch: 13 [4800/8040 (60%)]\tLoss: 92.129655\n",
      "Train Epoch: 13 [6000/8040 (75%)]\tLoss: 92.108822\n",
      "Train Epoch: 13 [7200/8040 (90%)]\tLoss: 91.775236\n",
      "====> Epoch: 13 Average loss: 92.0633\n",
      "epoch: 13, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 14 [0/8040 (0%)]\tLoss: 92.011792\n",
      "Train Epoch: 14 [1200/8040 (15%)]\tLoss: 92.039307\n",
      "Train Epoch: 14 [2400/8040 (30%)]\tLoss: 92.278703\n",
      "Train Epoch: 14 [3600/8040 (45%)]\tLoss: 92.059538\n",
      "Train Epoch: 14 [4800/8040 (60%)]\tLoss: 92.041105\n",
      "Train Epoch: 14 [6000/8040 (75%)]\tLoss: 91.950342\n",
      "Train Epoch: 14 [7200/8040 (90%)]\tLoss: 91.400155\n",
      "====> Epoch: 14 Average loss: 92.0253\n",
      "epoch: 14, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 15 [0/8040 (0%)]\tLoss: 91.702124\n",
      "Train Epoch: 15 [1200/8040 (15%)]\tLoss: 92.045760\n",
      "Train Epoch: 15 [2400/8040 (30%)]\tLoss: 92.430680\n",
      "Train Epoch: 15 [3600/8040 (45%)]\tLoss: 91.471818\n",
      "Train Epoch: 15 [4800/8040 (60%)]\tLoss: 92.506340\n",
      "Train Epoch: 15 [6000/8040 (75%)]\tLoss: 91.848926\n",
      "Train Epoch: 15 [7200/8040 (90%)]\tLoss: 91.562158\n",
      "====> Epoch: 15 Average loss: 92.0273\n",
      "epoch: 15, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 16 [0/8040 (0%)]\tLoss: 91.690999\n",
      "Train Epoch: 16 [1200/8040 (15%)]\tLoss: 92.095093\n",
      "Train Epoch: 16 [2400/8040 (30%)]\tLoss: 91.951074\n",
      "Train Epoch: 16 [3600/8040 (45%)]\tLoss: 92.333203\n",
      "Train Epoch: 16 [4800/8040 (60%)]\tLoss: 91.781925\n",
      "Train Epoch: 16 [6000/8040 (75%)]\tLoss: 92.087931\n",
      "Train Epoch: 16 [7200/8040 (90%)]\tLoss: 92.040096\n",
      "====> Epoch: 16 Average loss: 91.9989\n",
      "epoch: 16, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 17 [0/8040 (0%)]\tLoss: 91.810010\n",
      "Train Epoch: 17 [1200/8040 (15%)]\tLoss: 91.880794\n",
      "Train Epoch: 17 [2400/8040 (30%)]\tLoss: 91.710824\n",
      "Train Epoch: 17 [3600/8040 (45%)]\tLoss: 91.879313\n",
      "Train Epoch: 17 [4800/8040 (60%)]\tLoss: 91.660848\n",
      "Train Epoch: 17 [6000/8040 (75%)]\tLoss: 91.717806\n",
      "Train Epoch: 17 [7200/8040 (90%)]\tLoss: 91.880518\n",
      "====> Epoch: 17 Average loss: 92.0028\n",
      "epoch: 17, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 18 [0/8040 (0%)]\tLoss: 92.260197\n",
      "Train Epoch: 18 [1200/8040 (15%)]\tLoss: 91.747640\n",
      "Train Epoch: 18 [2400/8040 (30%)]\tLoss: 91.954761\n",
      "Train Epoch: 18 [3600/8040 (45%)]\tLoss: 91.735474\n",
      "Train Epoch: 18 [4800/8040 (60%)]\tLoss: 92.126929\n",
      "Train Epoch: 18 [6000/8040 (75%)]\tLoss: 92.016146\n",
      "Train Epoch: 18 [7200/8040 (90%)]\tLoss: 92.190487\n",
      "====> Epoch: 18 Average loss: 92.0007\n",
      "epoch: 18, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 19 [0/8040 (0%)]\tLoss: 91.911540\n",
      "Train Epoch: 19 [1200/8040 (15%)]\tLoss: 91.803906\n",
      "Train Epoch: 19 [2400/8040 (30%)]\tLoss: 91.891992\n",
      "Train Epoch: 19 [3600/8040 (45%)]\tLoss: 92.145980\n",
      "Train Epoch: 19 [4800/8040 (60%)]\tLoss: 91.895369\n",
      "Train Epoch: 19 [6000/8040 (75%)]\tLoss: 91.882910\n",
      "Train Epoch: 19 [7200/8040 (90%)]\tLoss: 92.363940\n",
      "====> Epoch: 19 Average loss: 92.0027\n",
      "epoch: 19, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 20 [0/8040 (0%)]\tLoss: 92.219604\n",
      "Train Epoch: 20 [1200/8040 (15%)]\tLoss: 91.972323\n",
      "Train Epoch: 20 [2400/8040 (30%)]\tLoss: 92.114632\n",
      "Train Epoch: 20 [3600/8040 (45%)]\tLoss: 92.125000\n",
      "Train Epoch: 20 [4800/8040 (60%)]\tLoss: 91.424992\n",
      "Train Epoch: 20 [6000/8040 (75%)]\tLoss: 92.158879\n",
      "Train Epoch: 20 [7200/8040 (90%)]\tLoss: 92.293481\n",
      "====> Epoch: 20 Average loss: 91.9891\n",
      "epoch: 20, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 21 [0/8040 (0%)]\tLoss: 91.987264\n",
      "Train Epoch: 21 [1200/8040 (15%)]\tLoss: 91.824748\n",
      "Train Epoch: 21 [2400/8040 (30%)]\tLoss: 91.988639\n",
      "Train Epoch: 21 [3600/8040 (45%)]\tLoss: 92.130078\n",
      "Train Epoch: 21 [4800/8040 (60%)]\tLoss: 91.869621\n",
      "Train Epoch: 21 [6000/8040 (75%)]\tLoss: 91.826546\n",
      "Train Epoch: 21 [7200/8040 (90%)]\tLoss: 91.900765\n",
      "====> Epoch: 21 Average loss: 91.9621\n",
      "epoch: 21, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 22 [0/8040 (0%)]\tLoss: 92.196883\n",
      "Train Epoch: 22 [1200/8040 (15%)]\tLoss: 91.783683\n",
      "Train Epoch: 22 [2400/8040 (30%)]\tLoss: 92.229655\n",
      "Train Epoch: 22 [3600/8040 (45%)]\tLoss: 92.025228\n",
      "Train Epoch: 22 [4800/8040 (60%)]\tLoss: 92.053564\n",
      "Train Epoch: 22 [6000/8040 (75%)]\tLoss: 91.864771\n",
      "Train Epoch: 22 [7200/8040 (90%)]\tLoss: 92.325789\n",
      "====> Epoch: 22 Average loss: 91.9760\n",
      "epoch: 22, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 23 [0/8040 (0%)]\tLoss: 91.929842\n",
      "Train Epoch: 23 [1200/8040 (15%)]\tLoss: 92.971493\n",
      "Train Epoch: 23 [2400/8040 (30%)]\tLoss: 91.679093\n",
      "Train Epoch: 23 [3600/8040 (45%)]\tLoss: 91.900366\n",
      "Train Epoch: 23 [4800/8040 (60%)]\tLoss: 92.417358\n",
      "Train Epoch: 23 [6000/8040 (75%)]\tLoss: 91.458610\n",
      "Train Epoch: 23 [7200/8040 (90%)]\tLoss: 92.129590\n",
      "====> Epoch: 23 Average loss: 91.9749\n",
      "epoch: 23, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 24 [0/8040 (0%)]\tLoss: 91.889347\n",
      "Train Epoch: 24 [1200/8040 (15%)]\tLoss: 91.190877\n",
      "Train Epoch: 24 [2400/8040 (30%)]\tLoss: 91.789966\n",
      "Train Epoch: 24 [3600/8040 (45%)]\tLoss: 91.544556\n",
      "Train Epoch: 24 [4800/8040 (60%)]\tLoss: 91.905371\n",
      "Train Epoch: 24 [6000/8040 (75%)]\tLoss: 92.056860\n",
      "Train Epoch: 24 [7200/8040 (90%)]\tLoss: 91.987410\n",
      "====> Epoch: 24 Average loss: 91.9595\n",
      "epoch: 24, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 25 [0/8040 (0%)]\tLoss: 91.947843\n",
      "Train Epoch: 25 [1200/8040 (15%)]\tLoss: 92.005534\n",
      "Train Epoch: 25 [2400/8040 (30%)]\tLoss: 91.351009\n",
      "Train Epoch: 25 [3600/8040 (45%)]\tLoss: 92.491195\n",
      "Train Epoch: 25 [4800/8040 (60%)]\tLoss: 91.925423\n",
      "Train Epoch: 25 [6000/8040 (75%)]\tLoss: 92.439136\n",
      "Train Epoch: 25 [7200/8040 (90%)]\tLoss: 91.808407\n",
      "====> Epoch: 25 Average loss: 91.9358\n",
      "epoch: 25, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 26 [0/8040 (0%)]\tLoss: 91.859880\n",
      "Train Epoch: 26 [1200/8040 (15%)]\tLoss: 91.933073\n",
      "Train Epoch: 26 [2400/8040 (30%)]\tLoss: 91.834945\n",
      "Train Epoch: 26 [3600/8040 (45%)]\tLoss: 92.294491\n",
      "Train Epoch: 26 [4800/8040 (60%)]\tLoss: 92.352954\n",
      "Train Epoch: 26 [6000/8040 (75%)]\tLoss: 91.987614\n",
      "Train Epoch: 26 [7200/8040 (90%)]\tLoss: 92.184497\n",
      "====> Epoch: 26 Average loss: 91.9218\n",
      "epoch: 26, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 27 [0/8040 (0%)]\tLoss: 91.972347\n",
      "Train Epoch: 27 [1200/8040 (15%)]\tLoss: 92.079696\n",
      "Train Epoch: 27 [2400/8040 (30%)]\tLoss: 91.555046\n",
      "Train Epoch: 27 [3600/8040 (45%)]\tLoss: 92.071191\n",
      "Train Epoch: 27 [4800/8040 (60%)]\tLoss: 92.088851\n",
      "Train Epoch: 27 [6000/8040 (75%)]\tLoss: 91.830054\n",
      "Train Epoch: 27 [7200/8040 (90%)]\tLoss: 91.953459\n",
      "====> Epoch: 27 Average loss: 91.9514\n",
      "epoch: 27, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 28 [0/8040 (0%)]\tLoss: 91.961776\n",
      "Train Epoch: 28 [1200/8040 (15%)]\tLoss: 92.169678\n",
      "Train Epoch: 28 [2400/8040 (30%)]\tLoss: 92.233716\n",
      "Train Epoch: 28 [3600/8040 (45%)]\tLoss: 91.563590\n",
      "Train Epoch: 28 [4800/8040 (60%)]\tLoss: 92.315934\n",
      "Train Epoch: 28 [6000/8040 (75%)]\tLoss: 91.755868\n",
      "Train Epoch: 28 [7200/8040 (90%)]\tLoss: 91.851823\n",
      "====> Epoch: 28 Average loss: 91.9383\n",
      "epoch: 28, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 29 [0/8040 (0%)]\tLoss: 91.696012\n",
      "Train Epoch: 29 [1200/8040 (15%)]\tLoss: 91.450130\n",
      "Train Epoch: 29 [2400/8040 (30%)]\tLoss: 92.406470\n",
      "Train Epoch: 29 [3600/8040 (45%)]\tLoss: 92.121387\n",
      "Train Epoch: 29 [4800/8040 (60%)]\tLoss: 91.576522\n",
      "Train Epoch: 29 [6000/8040 (75%)]\tLoss: 91.945085\n",
      "Train Epoch: 29 [7200/8040 (90%)]\tLoss: 91.590625\n",
      "====> Epoch: 29 Average loss: 91.9118\n",
      "epoch: 29, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 30 [0/8040 (0%)]\tLoss: 92.285547\n",
      "Train Epoch: 30 [1200/8040 (15%)]\tLoss: 91.753418\n",
      "Train Epoch: 30 [2400/8040 (30%)]\tLoss: 92.425765\n",
      "Train Epoch: 30 [3600/8040 (45%)]\tLoss: 92.000000\n",
      "Train Epoch: 30 [4800/8040 (60%)]\tLoss: 91.750472\n",
      "Train Epoch: 30 [6000/8040 (75%)]\tLoss: 92.370020\n",
      "Train Epoch: 30 [7200/8040 (90%)]\tLoss: 92.105094\n",
      "====> Epoch: 30 Average loss: 91.9361\n",
      "epoch: 30, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 31 [0/8040 (0%)]\tLoss: 91.293335\n",
      "Train Epoch: 31 [1200/8040 (15%)]\tLoss: 91.783171\n",
      "Train Epoch: 31 [2400/8040 (30%)]\tLoss: 91.953092\n",
      "Train Epoch: 31 [3600/8040 (45%)]\tLoss: 92.291992\n",
      "Train Epoch: 31 [4800/8040 (60%)]\tLoss: 91.543384\n",
      "Train Epoch: 31 [6000/8040 (75%)]\tLoss: 91.628613\n",
      "Train Epoch: 31 [7200/8040 (90%)]\tLoss: 91.909928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 31 Average loss: 91.9253\n",
      "epoch: 31, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 32 [0/8040 (0%)]\tLoss: 91.704492\n",
      "Train Epoch: 32 [1200/8040 (15%)]\tLoss: 91.986800\n",
      "Train Epoch: 32 [2400/8040 (30%)]\tLoss: 92.115584\n",
      "Train Epoch: 32 [3600/8040 (45%)]\tLoss: 92.095369\n",
      "Train Epoch: 32 [4800/8040 (60%)]\tLoss: 91.866960\n",
      "Train Epoch: 32 [6000/8040 (75%)]\tLoss: 91.201644\n",
      "Train Epoch: 32 [7200/8040 (90%)]\tLoss: 92.232300\n",
      "====> Epoch: 32 Average loss: 91.8999\n",
      "epoch: 32, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 33 [0/8040 (0%)]\tLoss: 91.608211\n",
      "Train Epoch: 33 [1200/8040 (15%)]\tLoss: 92.299080\n",
      "Train Epoch: 33 [2400/8040 (30%)]\tLoss: 92.131038\n",
      "Train Epoch: 33 [3600/8040 (45%)]\tLoss: 91.930876\n",
      "Train Epoch: 33 [4800/8040 (60%)]\tLoss: 92.011873\n",
      "Train Epoch: 33 [6000/8040 (75%)]\tLoss: 92.049780\n",
      "Train Epoch: 33 [7200/8040 (90%)]\tLoss: 91.705688\n",
      "====> Epoch: 33 Average loss: 91.8981\n",
      "epoch: 33, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 34 [0/8040 (0%)]\tLoss: 91.308683\n",
      "Train Epoch: 34 [1200/8040 (15%)]\tLoss: 91.767033\n",
      "Train Epoch: 34 [2400/8040 (30%)]\tLoss: 92.019059\n",
      "Train Epoch: 34 [3600/8040 (45%)]\tLoss: 92.052393\n",
      "Train Epoch: 34 [4800/8040 (60%)]\tLoss: 91.623055\n",
      "Train Epoch: 34 [6000/8040 (75%)]\tLoss: 91.294425\n",
      "Train Epoch: 34 [7200/8040 (90%)]\tLoss: 92.067627\n",
      "====> Epoch: 34 Average loss: 91.9030\n",
      "epoch: 34, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 35 [0/8040 (0%)]\tLoss: 91.862158\n",
      "Train Epoch: 35 [1200/8040 (15%)]\tLoss: 91.646362\n",
      "Train Epoch: 35 [2400/8040 (30%)]\tLoss: 91.807951\n",
      "Train Epoch: 35 [3600/8040 (45%)]\tLoss: 91.228996\n",
      "Train Epoch: 35 [4800/8040 (60%)]\tLoss: 92.359741\n",
      "Train Epoch: 35 [6000/8040 (75%)]\tLoss: 91.866789\n",
      "Train Epoch: 35 [7200/8040 (90%)]\tLoss: 91.872949\n",
      "====> Epoch: 35 Average loss: 91.9191\n",
      "epoch: 35, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 36 [0/8040 (0%)]\tLoss: 91.826489\n",
      "Train Epoch: 36 [1200/8040 (15%)]\tLoss: 91.957886\n",
      "Train Epoch: 36 [2400/8040 (30%)]\tLoss: 92.183675\n",
      "Train Epoch: 36 [3600/8040 (45%)]\tLoss: 91.689486\n",
      "Train Epoch: 36 [4800/8040 (60%)]\tLoss: 91.718612\n",
      "Train Epoch: 36 [6000/8040 (75%)]\tLoss: 92.115706\n",
      "Train Epoch: 36 [7200/8040 (90%)]\tLoss: 91.825830\n",
      "====> Epoch: 36 Average loss: 91.9152\n",
      "epoch: 36, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 37 [0/8040 (0%)]\tLoss: 91.711011\n",
      "Train Epoch: 37 [1200/8040 (15%)]\tLoss: 91.633317\n",
      "Train Epoch: 37 [2400/8040 (30%)]\tLoss: 92.074097\n",
      "Train Epoch: 37 [3600/8040 (45%)]\tLoss: 91.267049\n",
      "Train Epoch: 37 [4800/8040 (60%)]\tLoss: 91.795288\n",
      "Train Epoch: 37 [6000/8040 (75%)]\tLoss: 92.274845\n",
      "Train Epoch: 37 [7200/8040 (90%)]\tLoss: 91.428345\n",
      "====> Epoch: 37 Average loss: 91.9041\n",
      "epoch: 37, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 38 [0/8040 (0%)]\tLoss: 92.226636\n",
      "Train Epoch: 38 [1200/8040 (15%)]\tLoss: 91.610579\n",
      "Train Epoch: 38 [2400/8040 (30%)]\tLoss: 92.088794\n",
      "Train Epoch: 38 [3600/8040 (45%)]\tLoss: 92.057170\n",
      "Train Epoch: 38 [4800/8040 (60%)]\tLoss: 91.794352\n",
      "Train Epoch: 38 [6000/8040 (75%)]\tLoss: 92.137606\n",
      "Train Epoch: 38 [7200/8040 (90%)]\tLoss: 91.362174\n",
      "====> Epoch: 38 Average loss: 91.9036\n",
      "epoch: 38, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 39 [0/8040 (0%)]\tLoss: 91.838387\n",
      "Train Epoch: 39 [1200/8040 (15%)]\tLoss: 92.001294\n",
      "Train Epoch: 39 [2400/8040 (30%)]\tLoss: 91.605461\n",
      "Train Epoch: 39 [3600/8040 (45%)]\tLoss: 91.412996\n",
      "Train Epoch: 39 [4800/8040 (60%)]\tLoss: 91.847762\n",
      "Train Epoch: 39 [6000/8040 (75%)]\tLoss: 92.233773\n",
      "Train Epoch: 39 [7200/8040 (90%)]\tLoss: 91.646631\n",
      "====> Epoch: 39 Average loss: 91.8975\n",
      "epoch: 39, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 40 [0/8040 (0%)]\tLoss: 92.112004\n",
      "Train Epoch: 40 [1200/8040 (15%)]\tLoss: 92.111914\n",
      "Train Epoch: 40 [2400/8040 (30%)]\tLoss: 92.044661\n",
      "Train Epoch: 40 [3600/8040 (45%)]\tLoss: 91.584147\n",
      "Train Epoch: 40 [4800/8040 (60%)]\tLoss: 91.903052\n",
      "Train Epoch: 40 [6000/8040 (75%)]\tLoss: 92.044409\n",
      "Train Epoch: 40 [7200/8040 (90%)]\tLoss: 91.897843\n",
      "====> Epoch: 40 Average loss: 91.8944\n",
      "epoch: 40, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 41 [0/8040 (0%)]\tLoss: 92.064893\n",
      "Train Epoch: 41 [1200/8040 (15%)]\tLoss: 92.143132\n",
      "Train Epoch: 41 [2400/8040 (30%)]\tLoss: 91.851961\n",
      "Train Epoch: 41 [3600/8040 (45%)]\tLoss: 91.771891\n",
      "Train Epoch: 41 [4800/8040 (60%)]\tLoss: 91.410457\n",
      "Train Epoch: 41 [6000/8040 (75%)]\tLoss: 92.062378\n",
      "Train Epoch: 41 [7200/8040 (90%)]\tLoss: 91.498153\n",
      "====> Epoch: 41 Average loss: 91.9118\n",
      "epoch: 41, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 42 [0/8040 (0%)]\tLoss: 91.635693\n",
      "Train Epoch: 42 [1200/8040 (15%)]\tLoss: 91.770622\n",
      "Train Epoch: 42 [2400/8040 (30%)]\tLoss: 91.923975\n",
      "Train Epoch: 42 [3600/8040 (45%)]\tLoss: 92.207039\n",
      "Train Epoch: 42 [4800/8040 (60%)]\tLoss: 91.453914\n",
      "Train Epoch: 42 [6000/8040 (75%)]\tLoss: 91.927311\n",
      "Train Epoch: 42 [7200/8040 (90%)]\tLoss: 92.361979\n",
      "====> Epoch: 42 Average loss: 91.9185\n",
      "epoch: 42, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 43 [0/8040 (0%)]\tLoss: 91.583390\n",
      "Train Epoch: 43 [1200/8040 (15%)]\tLoss: 91.521549\n",
      "Train Epoch: 43 [2400/8040 (30%)]\tLoss: 91.932601\n",
      "Train Epoch: 43 [3600/8040 (45%)]\tLoss: 92.209920\n",
      "Train Epoch: 43 [4800/8040 (60%)]\tLoss: 91.768270\n",
      "Train Epoch: 43 [6000/8040 (75%)]\tLoss: 91.787850\n",
      "Train Epoch: 43 [7200/8040 (90%)]\tLoss: 92.101245\n",
      "====> Epoch: 43 Average loss: 91.9142\n",
      "epoch: 43, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 44 [0/8040 (0%)]\tLoss: 91.751343\n",
      "Train Epoch: 44 [1200/8040 (15%)]\tLoss: 91.749349\n",
      "Train Epoch: 44 [2400/8040 (30%)]\tLoss: 92.168066\n",
      "Train Epoch: 44 [3600/8040 (45%)]\tLoss: 91.828271\n",
      "Train Epoch: 44 [4800/8040 (60%)]\tLoss: 92.039323\n",
      "Train Epoch: 44 [6000/8040 (75%)]\tLoss: 91.759245\n",
      "Train Epoch: 44 [7200/8040 (90%)]\tLoss: 91.756991\n",
      "====> Epoch: 44 Average loss: 91.9034\n",
      "epoch: 44, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 45 [0/8040 (0%)]\tLoss: 92.197152\n",
      "Train Epoch: 45 [1200/8040 (15%)]\tLoss: 91.716569\n",
      "Train Epoch: 45 [2400/8040 (30%)]\tLoss: 92.191593\n",
      "Train Epoch: 45 [3600/8040 (45%)]\tLoss: 91.967814\n",
      "Train Epoch: 45 [4800/8040 (60%)]\tLoss: 91.840828\n",
      "Train Epoch: 45 [6000/8040 (75%)]\tLoss: 91.713550\n",
      "Train Epoch: 45 [7200/8040 (90%)]\tLoss: 91.676668\n",
      "====> Epoch: 45 Average loss: 91.8795\n",
      "epoch: 45, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 46 [0/8040 (0%)]\tLoss: 91.582406\n",
      "Train Epoch: 46 [1200/8040 (15%)]\tLoss: 92.147949\n",
      "Train Epoch: 46 [2400/8040 (30%)]\tLoss: 91.900602\n",
      "Train Epoch: 46 [3600/8040 (45%)]\tLoss: 91.303760\n",
      "Train Epoch: 46 [4800/8040 (60%)]\tLoss: 92.004451\n",
      "Train Epoch: 46 [6000/8040 (75%)]\tLoss: 91.769897\n",
      "Train Epoch: 46 [7200/8040 (90%)]\tLoss: 92.018709\n",
      "====> Epoch: 46 Average loss: 91.8822\n",
      "epoch: 46, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 47 [0/8040 (0%)]\tLoss: 91.361890\n",
      "Train Epoch: 47 [1200/8040 (15%)]\tLoss: 91.484831\n",
      "Train Epoch: 47 [2400/8040 (30%)]\tLoss: 92.205208\n",
      "Train Epoch: 47 [3600/8040 (45%)]\tLoss: 92.160173\n",
      "Train Epoch: 47 [4800/8040 (60%)]\tLoss: 92.196574\n",
      "Train Epoch: 47 [6000/8040 (75%)]\tLoss: 92.192977\n",
      "Train Epoch: 47 [7200/8040 (90%)]\tLoss: 91.978426\n",
      "====> Epoch: 47 Average loss: 91.8934\n",
      "epoch: 47, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 48 [0/8040 (0%)]\tLoss: 91.479980\n",
      "Train Epoch: 48 [1200/8040 (15%)]\tLoss: 91.671126\n",
      "Train Epoch: 48 [2400/8040 (30%)]\tLoss: 91.564852\n",
      "Train Epoch: 48 [3600/8040 (45%)]\tLoss: 92.018652\n",
      "Train Epoch: 48 [4800/8040 (60%)]\tLoss: 91.891089\n",
      "Train Epoch: 48 [6000/8040 (75%)]\tLoss: 92.470426\n",
      "Train Epoch: 48 [7200/8040 (90%)]\tLoss: 91.332642\n",
      "====> Epoch: 48 Average loss: 91.8665\n",
      "epoch: 48, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 49 [0/8040 (0%)]\tLoss: 92.059391\n",
      "Train Epoch: 49 [1200/8040 (15%)]\tLoss: 91.778613\n",
      "Train Epoch: 49 [2400/8040 (30%)]\tLoss: 91.745654\n",
      "Train Epoch: 49 [3600/8040 (45%)]\tLoss: 91.603084\n",
      "Train Epoch: 49 [4800/8040 (60%)]\tLoss: 92.119499\n",
      "Train Epoch: 49 [6000/8040 (75%)]\tLoss: 92.013224\n",
      "Train Epoch: 49 [7200/8040 (90%)]\tLoss: 91.769751\n",
      "====> Epoch: 49 Average loss: 91.8746\n",
      "epoch: 49, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 50 [0/8040 (0%)]\tLoss: 91.658480\n",
      "Train Epoch: 50 [1200/8040 (15%)]\tLoss: 91.923804\n",
      "Train Epoch: 50 [2400/8040 (30%)]\tLoss: 92.313981\n",
      "Train Epoch: 50 [3600/8040 (45%)]\tLoss: 91.202059\n",
      "Train Epoch: 50 [4800/8040 (60%)]\tLoss: 91.804443\n",
      "Train Epoch: 50 [6000/8040 (75%)]\tLoss: 92.177279\n",
      "Train Epoch: 50 [7200/8040 (90%)]\tLoss: 92.072428\n",
      "====> Epoch: 50 Average loss: 91.8762\n",
      "epoch: 50, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 51 [0/8040 (0%)]\tLoss: 92.119206\n",
      "Train Epoch: 51 [1200/8040 (15%)]\tLoss: 91.712435\n",
      "Train Epoch: 51 [2400/8040 (30%)]\tLoss: 91.794604\n",
      "Train Epoch: 51 [3600/8040 (45%)]\tLoss: 92.169954\n",
      "Train Epoch: 51 [4800/8040 (60%)]\tLoss: 91.499072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 51 [6000/8040 (75%)]\tLoss: 92.157609\n",
      "Train Epoch: 51 [7200/8040 (90%)]\tLoss: 91.607715\n",
      "====> Epoch: 51 Average loss: 91.8529\n",
      "epoch: 51, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 52 [0/8040 (0%)]\tLoss: 92.466943\n",
      "Train Epoch: 52 [1200/8040 (15%)]\tLoss: 92.026099\n",
      "Train Epoch: 52 [2400/8040 (30%)]\tLoss: 91.652132\n",
      "Train Epoch: 52 [3600/8040 (45%)]\tLoss: 91.262109\n",
      "Train Epoch: 52 [4800/8040 (60%)]\tLoss: 91.850602\n",
      "Train Epoch: 52 [6000/8040 (75%)]\tLoss: 91.739998\n",
      "Train Epoch: 52 [7200/8040 (90%)]\tLoss: 91.709717\n",
      "====> Epoch: 52 Average loss: 91.8856\n",
      "epoch: 52, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 53 [0/8040 (0%)]\tLoss: 91.710173\n",
      "Train Epoch: 53 [1200/8040 (15%)]\tLoss: 91.965649\n",
      "Train Epoch: 53 [2400/8040 (30%)]\tLoss: 91.791439\n",
      "Train Epoch: 53 [3600/8040 (45%)]\tLoss: 92.168953\n",
      "Train Epoch: 53 [4800/8040 (60%)]\tLoss: 91.743734\n",
      "Train Epoch: 53 [6000/8040 (75%)]\tLoss: 92.025252\n",
      "Train Epoch: 53 [7200/8040 (90%)]\tLoss: 91.903459\n",
      "====> Epoch: 53 Average loss: 91.8854\n",
      "epoch: 53, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 54 [0/8040 (0%)]\tLoss: 91.639144\n",
      "Train Epoch: 54 [1200/8040 (15%)]\tLoss: 92.175293\n",
      "Train Epoch: 54 [2400/8040 (30%)]\tLoss: 91.624577\n",
      "Train Epoch: 54 [3600/8040 (45%)]\tLoss: 91.942220\n",
      "Train Epoch: 54 [4800/8040 (60%)]\tLoss: 91.480355\n",
      "Train Epoch: 54 [6000/8040 (75%)]\tLoss: 91.691195\n",
      "Train Epoch: 54 [7200/8040 (90%)]\tLoss: 91.380477\n",
      "====> Epoch: 54 Average loss: 91.8787\n",
      "epoch: 54, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 55 [0/8040 (0%)]\tLoss: 92.075741\n",
      "Train Epoch: 55 [1200/8040 (15%)]\tLoss: 91.807731\n",
      "Train Epoch: 55 [2400/8040 (30%)]\tLoss: 91.896965\n",
      "Train Epoch: 55 [3600/8040 (45%)]\tLoss: 91.798608\n",
      "Train Epoch: 55 [4800/8040 (60%)]\tLoss: 91.816675\n",
      "Train Epoch: 55 [6000/8040 (75%)]\tLoss: 91.677108\n",
      "Train Epoch: 55 [7200/8040 (90%)]\tLoss: 91.950358\n",
      "====> Epoch: 55 Average loss: 91.8704\n",
      "epoch: 55, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 56 [0/8040 (0%)]\tLoss: 91.751286\n",
      "Train Epoch: 56 [1200/8040 (15%)]\tLoss: 91.536100\n",
      "Train Epoch: 56 [2400/8040 (30%)]\tLoss: 91.507194\n",
      "Train Epoch: 56 [3600/8040 (45%)]\tLoss: 92.311230\n",
      "Train Epoch: 56 [4800/8040 (60%)]\tLoss: 91.821403\n",
      "Train Epoch: 56 [6000/8040 (75%)]\tLoss: 91.865275\n",
      "Train Epoch: 56 [7200/8040 (90%)]\tLoss: 92.020329\n",
      "====> Epoch: 56 Average loss: 91.8546\n",
      "epoch: 56, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 57 [0/8040 (0%)]\tLoss: 91.344165\n",
      "Train Epoch: 57 [1200/8040 (15%)]\tLoss: 92.170475\n",
      "Train Epoch: 57 [2400/8040 (30%)]\tLoss: 92.191479\n",
      "Train Epoch: 57 [3600/8040 (45%)]\tLoss: 91.661255\n",
      "Train Epoch: 57 [4800/8040 (60%)]\tLoss: 91.867586\n",
      "Train Epoch: 57 [6000/8040 (75%)]\tLoss: 92.087899\n",
      "Train Epoch: 57 [7200/8040 (90%)]\tLoss: 91.670109\n",
      "====> Epoch: 57 Average loss: 91.8612\n",
      "epoch: 57, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 58 [0/8040 (0%)]\tLoss: 91.619173\n",
      "Train Epoch: 58 [1200/8040 (15%)]\tLoss: 91.710392\n",
      "Train Epoch: 58 [2400/8040 (30%)]\tLoss: 92.181478\n",
      "Train Epoch: 58 [3600/8040 (45%)]\tLoss: 91.991203\n",
      "Train Epoch: 58 [4800/8040 (60%)]\tLoss: 91.950724\n",
      "Train Epoch: 58 [6000/8040 (75%)]\tLoss: 91.926465\n",
      "Train Epoch: 58 [7200/8040 (90%)]\tLoss: 92.067985\n",
      "====> Epoch: 58 Average loss: 91.8690\n",
      "epoch: 58, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 59 [0/8040 (0%)]\tLoss: 91.779525\n",
      "Train Epoch: 59 [1200/8040 (15%)]\tLoss: 91.574479\n",
      "Train Epoch: 59 [2400/8040 (30%)]\tLoss: 91.523153\n",
      "Train Epoch: 59 [3600/8040 (45%)]\tLoss: 92.299536\n",
      "Train Epoch: 59 [4800/8040 (60%)]\tLoss: 91.595589\n",
      "Train Epoch: 59 [6000/8040 (75%)]\tLoss: 91.788159\n",
      "Train Epoch: 59 [7200/8040 (90%)]\tLoss: 91.450448\n",
      "====> Epoch: 59 Average loss: 91.8789\n",
      "epoch: 59, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 60 [0/8040 (0%)]\tLoss: 92.170345\n",
      "Train Epoch: 60 [1200/8040 (15%)]\tLoss: 92.011247\n",
      "Train Epoch: 60 [2400/8040 (30%)]\tLoss: 91.668490\n",
      "Train Epoch: 60 [3600/8040 (45%)]\tLoss: 92.097778\n",
      "Train Epoch: 60 [4800/8040 (60%)]\tLoss: 91.846053\n",
      "Train Epoch: 60 [6000/8040 (75%)]\tLoss: 91.615723\n",
      "Train Epoch: 60 [7200/8040 (90%)]\tLoss: 91.895256\n",
      "====> Epoch: 60 Average loss: 91.8994\n",
      "epoch: 60, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 61 [0/8040 (0%)]\tLoss: 91.585083\n",
      "Train Epoch: 61 [1200/8040 (15%)]\tLoss: 91.952376\n",
      "Train Epoch: 61 [2400/8040 (30%)]\tLoss: 92.509359\n",
      "Train Epoch: 61 [3600/8040 (45%)]\tLoss: 91.829150\n",
      "Train Epoch: 61 [4800/8040 (60%)]\tLoss: 92.093132\n",
      "Train Epoch: 61 [6000/8040 (75%)]\tLoss: 91.694548\n",
      "Train Epoch: 61 [7200/8040 (90%)]\tLoss: 91.835173\n",
      "====> Epoch: 61 Average loss: 91.8823\n",
      "epoch: 61, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 62 [0/8040 (0%)]\tLoss: 91.456787\n",
      "Train Epoch: 62 [1200/8040 (15%)]\tLoss: 91.770459\n",
      "Train Epoch: 62 [2400/8040 (30%)]\tLoss: 91.789364\n",
      "Train Epoch: 62 [3600/8040 (45%)]\tLoss: 91.669873\n",
      "Train Epoch: 62 [4800/8040 (60%)]\tLoss: 91.510449\n",
      "Train Epoch: 62 [6000/8040 (75%)]\tLoss: 92.058887\n",
      "Train Epoch: 62 [7200/8040 (90%)]\tLoss: 91.857471\n",
      "====> Epoch: 62 Average loss: 91.8329\n",
      "epoch: 62, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 63 [0/8040 (0%)]\tLoss: 91.627580\n",
      "Train Epoch: 63 [1200/8040 (15%)]\tLoss: 91.914632\n",
      "Train Epoch: 63 [2400/8040 (30%)]\tLoss: 91.545882\n",
      "Train Epoch: 63 [3600/8040 (45%)]\tLoss: 91.871493\n",
      "Train Epoch: 63 [4800/8040 (60%)]\tLoss: 91.914201\n",
      "Train Epoch: 63 [6000/8040 (75%)]\tLoss: 92.205754\n",
      "Train Epoch: 63 [7200/8040 (90%)]\tLoss: 91.742171\n",
      "====> Epoch: 63 Average loss: 91.8445\n",
      "epoch: 63, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 64 [0/8040 (0%)]\tLoss: 92.061328\n",
      "Train Epoch: 64 [1200/8040 (15%)]\tLoss: 91.684530\n",
      "Train Epoch: 64 [2400/8040 (30%)]\tLoss: 91.942912\n",
      "Train Epoch: 64 [3600/8040 (45%)]\tLoss: 91.328760\n",
      "Train Epoch: 64 [4800/8040 (60%)]\tLoss: 92.118514\n",
      "Train Epoch: 64 [6000/8040 (75%)]\tLoss: 91.544971\n",
      "Train Epoch: 64 [7200/8040 (90%)]\tLoss: 91.907682\n",
      "====> Epoch: 64 Average loss: 91.8381\n",
      "epoch: 64, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 65 [0/8040 (0%)]\tLoss: 92.248226\n",
      "Train Epoch: 65 [1200/8040 (15%)]\tLoss: 91.751465\n",
      "Train Epoch: 65 [2400/8040 (30%)]\tLoss: 92.034749\n",
      "Train Epoch: 65 [3600/8040 (45%)]\tLoss: 91.881787\n",
      "Train Epoch: 65 [4800/8040 (60%)]\tLoss: 92.179989\n",
      "Train Epoch: 65 [6000/8040 (75%)]\tLoss: 92.441854\n",
      "Train Epoch: 65 [7200/8040 (90%)]\tLoss: 91.457031\n",
      "====> Epoch: 65 Average loss: 91.8591\n",
      "epoch: 65, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 66 [0/8040 (0%)]\tLoss: 92.037866\n",
      "Train Epoch: 66 [1200/8040 (15%)]\tLoss: 91.446110\n",
      "Train Epoch: 66 [2400/8040 (30%)]\tLoss: 91.625977\n",
      "Train Epoch: 66 [3600/8040 (45%)]\tLoss: 92.063981\n",
      "Train Epoch: 66 [4800/8040 (60%)]\tLoss: 91.784928\n",
      "Train Epoch: 66 [6000/8040 (75%)]\tLoss: 91.425545\n",
      "Train Epoch: 66 [7200/8040 (90%)]\tLoss: 91.605941\n",
      "====> Epoch: 66 Average loss: 91.8592\n",
      "epoch: 66, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 67 [0/8040 (0%)]\tLoss: 92.401400\n",
      "Train Epoch: 67 [1200/8040 (15%)]\tLoss: 91.720142\n",
      "Train Epoch: 67 [2400/8040 (30%)]\tLoss: 91.717220\n",
      "Train Epoch: 67 [3600/8040 (45%)]\tLoss: 91.610465\n",
      "Train Epoch: 67 [4800/8040 (60%)]\tLoss: 91.699276\n",
      "Train Epoch: 67 [6000/8040 (75%)]\tLoss: 92.240576\n",
      "Train Epoch: 67 [7200/8040 (90%)]\tLoss: 92.498104\n",
      "====> Epoch: 67 Average loss: 91.8776\n",
      "epoch: 67, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 68 [0/8040 (0%)]\tLoss: 91.139600\n",
      "Train Epoch: 68 [1200/8040 (15%)]\tLoss: 91.515251\n",
      "Train Epoch: 68 [2400/8040 (30%)]\tLoss: 92.274316\n",
      "Train Epoch: 68 [3600/8040 (45%)]\tLoss: 91.947949\n",
      "Train Epoch: 68 [4800/8040 (60%)]\tLoss: 91.678914\n",
      "Train Epoch: 68 [6000/8040 (75%)]\tLoss: 92.279338\n",
      "Train Epoch: 68 [7200/8040 (90%)]\tLoss: 91.881738\n",
      "====> Epoch: 68 Average loss: 91.8565\n",
      "epoch: 68, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 69 [0/8040 (0%)]\tLoss: 91.901676\n",
      "Train Epoch: 69 [1200/8040 (15%)]\tLoss: 91.778068\n",
      "Train Epoch: 69 [2400/8040 (30%)]\tLoss: 91.818831\n",
      "Train Epoch: 69 [3600/8040 (45%)]\tLoss: 91.845915\n",
      "Train Epoch: 69 [4800/8040 (60%)]\tLoss: 92.091821\n",
      "Train Epoch: 69 [6000/8040 (75%)]\tLoss: 91.981576\n",
      "Train Epoch: 69 [7200/8040 (90%)]\tLoss: 92.104818\n",
      "====> Epoch: 69 Average loss: 91.8578\n",
      "epoch: 69, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 70 [0/8040 (0%)]\tLoss: 92.092098\n",
      "Train Epoch: 70 [1200/8040 (15%)]\tLoss: 91.681169\n",
      "Train Epoch: 70 [2400/8040 (30%)]\tLoss: 92.264014\n",
      "Train Epoch: 70 [3600/8040 (45%)]\tLoss: 91.949642\n",
      "Train Epoch: 70 [4800/8040 (60%)]\tLoss: 91.514006\n",
      "Train Epoch: 70 [6000/8040 (75%)]\tLoss: 91.753320\n",
      "Train Epoch: 70 [7200/8040 (90%)]\tLoss: 91.335978\n",
      "====> Epoch: 70 Average loss: 91.8564\n",
      "epoch: 70, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 71 [0/8040 (0%)]\tLoss: 91.364412\n",
      "Train Epoch: 71 [1200/8040 (15%)]\tLoss: 92.045988\n",
      "Train Epoch: 71 [2400/8040 (30%)]\tLoss: 91.726367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 71 [3600/8040 (45%)]\tLoss: 91.901839\n",
      "Train Epoch: 71 [4800/8040 (60%)]\tLoss: 92.013029\n",
      "Train Epoch: 71 [6000/8040 (75%)]\tLoss: 91.686580\n",
      "Train Epoch: 71 [7200/8040 (90%)]\tLoss: 92.069043\n",
      "====> Epoch: 71 Average loss: 91.8664\n",
      "epoch: 71, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 72 [0/8040 (0%)]\tLoss: 91.562557\n",
      "Train Epoch: 72 [1200/8040 (15%)]\tLoss: 91.628605\n",
      "Train Epoch: 72 [2400/8040 (30%)]\tLoss: 91.498730\n",
      "Train Epoch: 72 [3600/8040 (45%)]\tLoss: 92.420776\n",
      "Train Epoch: 72 [4800/8040 (60%)]\tLoss: 91.935124\n",
      "Train Epoch: 72 [6000/8040 (75%)]\tLoss: 91.447046\n",
      "Train Epoch: 72 [7200/8040 (90%)]\tLoss: 91.851855\n",
      "====> Epoch: 72 Average loss: 91.8548\n",
      "epoch: 72, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 73 [0/8040 (0%)]\tLoss: 91.668685\n",
      "Train Epoch: 73 [1200/8040 (15%)]\tLoss: 92.144556\n",
      "Train Epoch: 73 [2400/8040 (30%)]\tLoss: 92.120931\n",
      "Train Epoch: 73 [3600/8040 (45%)]\tLoss: 91.984635\n",
      "Train Epoch: 73 [4800/8040 (60%)]\tLoss: 92.079240\n",
      "Train Epoch: 73 [6000/8040 (75%)]\tLoss: 91.847909\n",
      "Train Epoch: 73 [7200/8040 (90%)]\tLoss: 91.621200\n",
      "====> Epoch: 73 Average loss: 91.8505\n",
      "epoch: 73, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 74 [0/8040 (0%)]\tLoss: 91.739819\n",
      "Train Epoch: 74 [1200/8040 (15%)]\tLoss: 92.168823\n",
      "Train Epoch: 74 [2400/8040 (30%)]\tLoss: 91.996452\n",
      "Train Epoch: 74 [3600/8040 (45%)]\tLoss: 92.197559\n",
      "Train Epoch: 74 [4800/8040 (60%)]\tLoss: 91.695646\n",
      "Train Epoch: 74 [6000/8040 (75%)]\tLoss: 91.262166\n",
      "Train Epoch: 74 [7200/8040 (90%)]\tLoss: 91.734619\n",
      "====> Epoch: 74 Average loss: 91.8577\n",
      "epoch: 74, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 75 [0/8040 (0%)]\tLoss: 91.938550\n",
      "Train Epoch: 75 [1200/8040 (15%)]\tLoss: 91.899561\n",
      "Train Epoch: 75 [2400/8040 (30%)]\tLoss: 92.091007\n",
      "Train Epoch: 75 [3600/8040 (45%)]\tLoss: 91.791431\n",
      "Train Epoch: 75 [4800/8040 (60%)]\tLoss: 91.825675\n",
      "Train Epoch: 75 [6000/8040 (75%)]\tLoss: 91.719580\n",
      "Train Epoch: 75 [7200/8040 (90%)]\tLoss: 92.331494\n",
      "====> Epoch: 75 Average loss: 91.8661\n",
      "epoch: 75, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 76 [0/8040 (0%)]\tLoss: 91.585563\n",
      "Train Epoch: 76 [1200/8040 (15%)]\tLoss: 91.706868\n",
      "Train Epoch: 76 [2400/8040 (30%)]\tLoss: 92.086190\n",
      "Train Epoch: 76 [3600/8040 (45%)]\tLoss: 91.941699\n",
      "Train Epoch: 76 [4800/8040 (60%)]\tLoss: 91.830225\n",
      "Train Epoch: 76 [6000/8040 (75%)]\tLoss: 91.732568\n",
      "Train Epoch: 76 [7200/8040 (90%)]\tLoss: 92.081828\n",
      "====> Epoch: 76 Average loss: 91.8510\n",
      "epoch: 76, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 77 [0/8040 (0%)]\tLoss: 91.467651\n",
      "Train Epoch: 77 [1200/8040 (15%)]\tLoss: 92.137687\n",
      "Train Epoch: 77 [2400/8040 (30%)]\tLoss: 91.748796\n",
      "Train Epoch: 77 [3600/8040 (45%)]\tLoss: 91.815047\n",
      "Train Epoch: 77 [4800/8040 (60%)]\tLoss: 92.143905\n",
      "Train Epoch: 77 [6000/8040 (75%)]\tLoss: 92.110457\n",
      "Train Epoch: 77 [7200/8040 (90%)]\tLoss: 92.141976\n",
      "====> Epoch: 77 Average loss: 91.8597\n",
      "epoch: 77, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 78 [0/8040 (0%)]\tLoss: 91.821566\n",
      "Train Epoch: 78 [1200/8040 (15%)]\tLoss: 91.574780\n",
      "Train Epoch: 78 [2400/8040 (30%)]\tLoss: 91.685392\n",
      "Train Epoch: 78 [3600/8040 (45%)]\tLoss: 91.692179\n",
      "Train Epoch: 78 [4800/8040 (60%)]\tLoss: 91.953337\n",
      "Train Epoch: 78 [6000/8040 (75%)]\tLoss: 92.085978\n",
      "Train Epoch: 78 [7200/8040 (90%)]\tLoss: 91.642424\n",
      "====> Epoch: 78 Average loss: 91.8448\n",
      "epoch: 78, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 79 [0/8040 (0%)]\tLoss: 91.784229\n",
      "Train Epoch: 79 [1200/8040 (15%)]\tLoss: 91.438680\n",
      "Train Epoch: 79 [2400/8040 (30%)]\tLoss: 91.450456\n",
      "Train Epoch: 79 [3600/8040 (45%)]\tLoss: 91.765519\n",
      "Train Epoch: 79 [4800/8040 (60%)]\tLoss: 91.917269\n",
      "Train Epoch: 79 [6000/8040 (75%)]\tLoss: 91.987240\n",
      "Train Epoch: 79 [7200/8040 (90%)]\tLoss: 91.500952\n",
      "====> Epoch: 79 Average loss: 91.8605\n",
      "epoch: 79, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 80 [0/8040 (0%)]\tLoss: 91.546102\n",
      "Train Epoch: 80 [1200/8040 (15%)]\tLoss: 91.737695\n",
      "Train Epoch: 80 [2400/8040 (30%)]\tLoss: 91.847087\n",
      "Train Epoch: 80 [3600/8040 (45%)]\tLoss: 92.028418\n",
      "Train Epoch: 80 [4800/8040 (60%)]\tLoss: 91.331274\n",
      "Train Epoch: 80 [6000/8040 (75%)]\tLoss: 91.657902\n",
      "Train Epoch: 80 [7200/8040 (90%)]\tLoss: 91.666846\n",
      "====> Epoch: 80 Average loss: 91.8349\n",
      "epoch: 80, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 81 [0/8040 (0%)]\tLoss: 91.927417\n",
      "Train Epoch: 81 [1200/8040 (15%)]\tLoss: 91.285775\n",
      "Train Epoch: 81 [2400/8040 (30%)]\tLoss: 91.758065\n",
      "Train Epoch: 81 [3600/8040 (45%)]\tLoss: 91.784977\n",
      "Train Epoch: 81 [4800/8040 (60%)]\tLoss: 91.676676\n",
      "Train Epoch: 81 [6000/8040 (75%)]\tLoss: 91.907259\n",
      "Train Epoch: 81 [7200/8040 (90%)]\tLoss: 91.639551\n",
      "====> Epoch: 81 Average loss: 91.8471\n",
      "epoch: 81, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 82 [0/8040 (0%)]\tLoss: 92.015365\n",
      "Train Epoch: 82 [1200/8040 (15%)]\tLoss: 91.650757\n",
      "Train Epoch: 82 [2400/8040 (30%)]\tLoss: 92.276099\n",
      "Train Epoch: 82 [3600/8040 (45%)]\tLoss: 91.989950\n",
      "Train Epoch: 82 [4800/8040 (60%)]\tLoss: 91.380884\n",
      "Train Epoch: 82 [6000/8040 (75%)]\tLoss: 91.923071\n",
      "Train Epoch: 82 [7200/8040 (90%)]\tLoss: 92.345426\n",
      "====> Epoch: 82 Average loss: 91.8397\n",
      "epoch: 82, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 83 [0/8040 (0%)]\tLoss: 92.035913\n",
      "Train Epoch: 83 [1200/8040 (15%)]\tLoss: 91.535905\n",
      "Train Epoch: 83 [2400/8040 (30%)]\tLoss: 91.598910\n",
      "Train Epoch: 83 [3600/8040 (45%)]\tLoss: 92.214640\n",
      "Train Epoch: 83 [4800/8040 (60%)]\tLoss: 91.918327\n",
      "Train Epoch: 83 [6000/8040 (75%)]\tLoss: 91.921330\n",
      "Train Epoch: 83 [7200/8040 (90%)]\tLoss: 92.113688\n",
      "====> Epoch: 83 Average loss: 91.8467\n",
      "epoch: 83, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 84 [0/8040 (0%)]\tLoss: 92.103133\n",
      "Train Epoch: 84 [1200/8040 (15%)]\tLoss: 91.849683\n",
      "Train Epoch: 84 [2400/8040 (30%)]\tLoss: 92.246297\n",
      "Train Epoch: 84 [3600/8040 (45%)]\tLoss: 91.811287\n",
      "Train Epoch: 84 [4800/8040 (60%)]\tLoss: 91.924154\n",
      "Train Epoch: 84 [6000/8040 (75%)]\tLoss: 91.755786\n",
      "Train Epoch: 84 [7200/8040 (90%)]\tLoss: 91.662052\n",
      "====> Epoch: 84 Average loss: 91.8655\n",
      "epoch: 84, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 85 [0/8040 (0%)]\tLoss: 91.368774\n",
      "Train Epoch: 85 [1200/8040 (15%)]\tLoss: 91.662834\n",
      "Train Epoch: 85 [2400/8040 (30%)]\tLoss: 92.061328\n",
      "Train Epoch: 85 [3600/8040 (45%)]\tLoss: 91.584359\n",
      "Train Epoch: 85 [4800/8040 (60%)]\tLoss: 92.000366\n",
      "Train Epoch: 85 [6000/8040 (75%)]\tLoss: 91.650635\n",
      "Train Epoch: 85 [7200/8040 (90%)]\tLoss: 92.226823\n",
      "====> Epoch: 85 Average loss: 91.8423\n",
      "epoch: 85, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 86 [0/8040 (0%)]\tLoss: 92.262573\n",
      "Train Epoch: 86 [1200/8040 (15%)]\tLoss: 92.170101\n",
      "Train Epoch: 86 [2400/8040 (30%)]\tLoss: 91.517928\n",
      "Train Epoch: 86 [3600/8040 (45%)]\tLoss: 91.862158\n",
      "Train Epoch: 86 [4800/8040 (60%)]\tLoss: 91.809106\n",
      "Train Epoch: 86 [6000/8040 (75%)]\tLoss: 91.905509\n",
      "Train Epoch: 86 [7200/8040 (90%)]\tLoss: 91.948478\n",
      "====> Epoch: 86 Average loss: 91.8474\n",
      "epoch: 86, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 87 [0/8040 (0%)]\tLoss: 92.084855\n",
      "Train Epoch: 87 [1200/8040 (15%)]\tLoss: 91.553516\n",
      "Train Epoch: 87 [2400/8040 (30%)]\tLoss: 92.186971\n",
      "Train Epoch: 87 [3600/8040 (45%)]\tLoss: 91.867863\n",
      "Train Epoch: 87 [4800/8040 (60%)]\tLoss: 91.525651\n",
      "Train Epoch: 87 [6000/8040 (75%)]\tLoss: 91.570703\n",
      "Train Epoch: 87 [7200/8040 (90%)]\tLoss: 91.781641\n",
      "====> Epoch: 87 Average loss: 91.8258\n",
      "epoch: 87, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 88 [0/8040 (0%)]\tLoss: 91.464119\n",
      "Train Epoch: 88 [1200/8040 (15%)]\tLoss: 91.700464\n",
      "Train Epoch: 88 [2400/8040 (30%)]\tLoss: 91.656722\n",
      "Train Epoch: 88 [3600/8040 (45%)]\tLoss: 91.963395\n",
      "Train Epoch: 88 [4800/8040 (60%)]\tLoss: 91.459269\n",
      "Train Epoch: 88 [6000/8040 (75%)]\tLoss: 91.492904\n",
      "Train Epoch: 88 [7200/8040 (90%)]\tLoss: 91.958577\n",
      "====> Epoch: 88 Average loss: 91.8420\n",
      "epoch: 88, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 89 [0/8040 (0%)]\tLoss: 91.323096\n",
      "Train Epoch: 89 [1200/8040 (15%)]\tLoss: 91.352620\n",
      "Train Epoch: 89 [2400/8040 (30%)]\tLoss: 91.847331\n",
      "Train Epoch: 89 [3600/8040 (45%)]\tLoss: 91.527393\n",
      "Train Epoch: 89 [4800/8040 (60%)]\tLoss: 91.894816\n",
      "Train Epoch: 89 [6000/8040 (75%)]\tLoss: 92.145785\n",
      "Train Epoch: 89 [7200/8040 (90%)]\tLoss: 91.086344\n",
      "====> Epoch: 89 Average loss: 91.8259\n",
      "epoch: 89, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 90 [0/8040 (0%)]\tLoss: 91.396029\n",
      "Train Epoch: 90 [1200/8040 (15%)]\tLoss: 91.775391\n",
      "Train Epoch: 90 [2400/8040 (30%)]\tLoss: 91.642570\n",
      "Train Epoch: 90 [3600/8040 (45%)]\tLoss: 91.602767\n",
      "Train Epoch: 90 [4800/8040 (60%)]\tLoss: 91.753019\n",
      "Train Epoch: 90 [6000/8040 (75%)]\tLoss: 91.665934\n",
      "Train Epoch: 90 [7200/8040 (90%)]\tLoss: 91.709261\n",
      "====> Epoch: 90 Average loss: 91.8353\n",
      "epoch: 90, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 91 [0/8040 (0%)]\tLoss: 91.988770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 91 [1200/8040 (15%)]\tLoss: 91.643685\n",
      "Train Epoch: 91 [2400/8040 (30%)]\tLoss: 91.857837\n",
      "Train Epoch: 91 [3600/8040 (45%)]\tLoss: 91.677262\n",
      "Train Epoch: 91 [4800/8040 (60%)]\tLoss: 91.660913\n",
      "Train Epoch: 91 [6000/8040 (75%)]\tLoss: 92.491683\n",
      "Train Epoch: 91 [7200/8040 (90%)]\tLoss: 91.291496\n",
      "====> Epoch: 91 Average loss: 91.8255\n",
      "epoch: 91, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 92 [0/8040 (0%)]\tLoss: 91.820174\n",
      "Train Epoch: 92 [1200/8040 (15%)]\tLoss: 91.759310\n",
      "Train Epoch: 92 [2400/8040 (30%)]\tLoss: 91.936230\n",
      "Train Epoch: 92 [3600/8040 (45%)]\tLoss: 91.413916\n",
      "Train Epoch: 92 [4800/8040 (60%)]\tLoss: 92.359481\n",
      "Train Epoch: 92 [6000/8040 (75%)]\tLoss: 91.667822\n",
      "Train Epoch: 92 [7200/8040 (90%)]\tLoss: 91.752051\n",
      "====> Epoch: 92 Average loss: 91.8129\n",
      "epoch: 92, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 93 [0/8040 (0%)]\tLoss: 91.887606\n",
      "Train Epoch: 93 [1200/8040 (15%)]\tLoss: 91.951676\n",
      "Train Epoch: 93 [2400/8040 (30%)]\tLoss: 92.162150\n",
      "Train Epoch: 93 [3600/8040 (45%)]\tLoss: 91.906405\n",
      "Train Epoch: 93 [4800/8040 (60%)]\tLoss: 91.757528\n",
      "Train Epoch: 93 [6000/8040 (75%)]\tLoss: 91.108366\n",
      "Train Epoch: 93 [7200/8040 (90%)]\tLoss: 91.770475\n",
      "====> Epoch: 93 Average loss: 91.8279\n",
      "epoch: 93, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 94 [0/8040 (0%)]\tLoss: 91.800317\n",
      "Train Epoch: 94 [1200/8040 (15%)]\tLoss: 91.553979\n",
      "Train Epoch: 94 [2400/8040 (30%)]\tLoss: 91.929769\n",
      "Train Epoch: 94 [3600/8040 (45%)]\tLoss: 91.232739\n",
      "Train Epoch: 94 [4800/8040 (60%)]\tLoss: 91.656022\n",
      "Train Epoch: 94 [6000/8040 (75%)]\tLoss: 91.188607\n",
      "Train Epoch: 94 [7200/8040 (90%)]\tLoss: 91.916561\n",
      "====> Epoch: 94 Average loss: 91.8231\n",
      "epoch: 94, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 95 [0/8040 (0%)]\tLoss: 91.950317\n",
      "Train Epoch: 95 [1200/8040 (15%)]\tLoss: 91.998755\n",
      "Train Epoch: 95 [2400/8040 (30%)]\tLoss: 91.564307\n",
      "Train Epoch: 95 [3600/8040 (45%)]\tLoss: 91.686686\n",
      "Train Epoch: 95 [4800/8040 (60%)]\tLoss: 91.800171\n",
      "Train Epoch: 95 [6000/8040 (75%)]\tLoss: 91.518669\n",
      "Train Epoch: 95 [7200/8040 (90%)]\tLoss: 91.695817\n",
      "====> Epoch: 95 Average loss: 91.8253\n",
      "epoch: 95, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 96 [0/8040 (0%)]\tLoss: 91.813143\n",
      "Train Epoch: 96 [1200/8040 (15%)]\tLoss: 91.723763\n",
      "Train Epoch: 96 [2400/8040 (30%)]\tLoss: 92.050618\n",
      "Train Epoch: 96 [3600/8040 (45%)]\tLoss: 91.695166\n",
      "Train Epoch: 96 [4800/8040 (60%)]\tLoss: 91.359049\n",
      "Train Epoch: 96 [6000/8040 (75%)]\tLoss: 91.771159\n",
      "Train Epoch: 96 [7200/8040 (90%)]\tLoss: 91.749569\n",
      "====> Epoch: 96 Average loss: 91.8101\n",
      "epoch: 96, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 97 [0/8040 (0%)]\tLoss: 91.721200\n",
      "Train Epoch: 97 [1200/8040 (15%)]\tLoss: 91.722192\n",
      "Train Epoch: 97 [2400/8040 (30%)]\tLoss: 91.550366\n",
      "Train Epoch: 97 [3600/8040 (45%)]\tLoss: 91.689518\n",
      "Train Epoch: 97 [4800/8040 (60%)]\tLoss: 91.557381\n",
      "Train Epoch: 97 [6000/8040 (75%)]\tLoss: 91.433862\n",
      "Train Epoch: 97 [7200/8040 (90%)]\tLoss: 91.612345\n",
      "====> Epoch: 97 Average loss: 91.8135\n",
      "epoch: 97, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 98 [0/8040 (0%)]\tLoss: 92.223942\n",
      "Train Epoch: 98 [1200/8040 (15%)]\tLoss: 91.950203\n",
      "Train Epoch: 98 [2400/8040 (30%)]\tLoss: 91.592887\n",
      "Train Epoch: 98 [3600/8040 (45%)]\tLoss: 92.015145\n",
      "Train Epoch: 98 [4800/8040 (60%)]\tLoss: 91.838647\n",
      "Train Epoch: 98 [6000/8040 (75%)]\tLoss: 91.864933\n",
      "Train Epoch: 98 [7200/8040 (90%)]\tLoss: 91.741309\n",
      "====> Epoch: 98 Average loss: 91.8256\n",
      "epoch: 98, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 99 [0/8040 (0%)]\tLoss: 91.742204\n",
      "Train Epoch: 99 [1200/8040 (15%)]\tLoss: 91.861304\n",
      "Train Epoch: 99 [2400/8040 (30%)]\tLoss: 91.713184\n",
      "Train Epoch: 99 [3600/8040 (45%)]\tLoss: 91.393986\n",
      "Train Epoch: 99 [4800/8040 (60%)]\tLoss: 92.046362\n",
      "Train Epoch: 99 [6000/8040 (75%)]\tLoss: 91.627897\n",
      "Train Epoch: 99 [7200/8040 (90%)]\tLoss: 91.627327\n",
      "====> Epoch: 99 Average loss: 91.8107\n",
      "epoch: 99, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 100 [0/8040 (0%)]\tLoss: 91.651253\n",
      "Train Epoch: 100 [1200/8040 (15%)]\tLoss: 91.646265\n",
      "Train Epoch: 100 [2400/8040 (30%)]\tLoss: 91.441121\n",
      "Train Epoch: 100 [3600/8040 (45%)]\tLoss: 91.953605\n",
      "Train Epoch: 100 [4800/8040 (60%)]\tLoss: 92.112118\n",
      "Train Epoch: 100 [6000/8040 (75%)]\tLoss: 91.968604\n",
      "Train Epoch: 100 [7200/8040 (90%)]\tLoss: 92.185612\n",
      "====> Epoch: 100 Average loss: 91.8033\n",
      "epoch: 100, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 101 [0/8040 (0%)]\tLoss: 92.284237\n",
      "Train Epoch: 101 [1200/8040 (15%)]\tLoss: 91.883740\n",
      "Train Epoch: 101 [2400/8040 (30%)]\tLoss: 91.942790\n",
      "Train Epoch: 101 [3600/8040 (45%)]\tLoss: 91.924902\n",
      "Train Epoch: 101 [4800/8040 (60%)]\tLoss: 91.807056\n",
      "Train Epoch: 101 [6000/8040 (75%)]\tLoss: 91.567407\n",
      "Train Epoch: 101 [7200/8040 (90%)]\tLoss: 91.709001\n",
      "====> Epoch: 101 Average loss: 91.8191\n",
      "epoch: 101, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 102 [0/8040 (0%)]\tLoss: 91.382389\n",
      "Train Epoch: 102 [1200/8040 (15%)]\tLoss: 92.219840\n",
      "Train Epoch: 102 [2400/8040 (30%)]\tLoss: 91.837744\n",
      "Train Epoch: 102 [3600/8040 (45%)]\tLoss: 91.919906\n",
      "Train Epoch: 102 [4800/8040 (60%)]\tLoss: 91.591797\n",
      "Train Epoch: 102 [6000/8040 (75%)]\tLoss: 91.227865\n",
      "Train Epoch: 102 [7200/8040 (90%)]\tLoss: 91.673079\n",
      "====> Epoch: 102 Average loss: 91.7960\n",
      "epoch: 102, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 103 [0/8040 (0%)]\tLoss: 92.255371\n",
      "Train Epoch: 103 [1200/8040 (15%)]\tLoss: 91.855876\n",
      "Train Epoch: 103 [2400/8040 (30%)]\tLoss: 91.805208\n",
      "Train Epoch: 103 [3600/8040 (45%)]\tLoss: 91.954036\n",
      "Train Epoch: 103 [4800/8040 (60%)]\tLoss: 91.622453\n",
      "Train Epoch: 103 [6000/8040 (75%)]\tLoss: 91.631803\n",
      "Train Epoch: 103 [7200/8040 (90%)]\tLoss: 91.974772\n",
      "====> Epoch: 103 Average loss: 91.8116\n",
      "epoch: 103, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 104 [0/8040 (0%)]\tLoss: 92.103662\n",
      "Train Epoch: 104 [1200/8040 (15%)]\tLoss: 91.768376\n",
      "Train Epoch: 104 [2400/8040 (30%)]\tLoss: 91.933122\n",
      "Train Epoch: 104 [3600/8040 (45%)]\tLoss: 91.821476\n",
      "Train Epoch: 104 [4800/8040 (60%)]\tLoss: 92.119385\n",
      "Train Epoch: 104 [6000/8040 (75%)]\tLoss: 91.991927\n",
      "Train Epoch: 104 [7200/8040 (90%)]\tLoss: 91.775505\n",
      "====> Epoch: 104 Average loss: 91.8094\n",
      "epoch: 104, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 105 [0/8040 (0%)]\tLoss: 91.404720\n",
      "Train Epoch: 105 [1200/8040 (15%)]\tLoss: 91.631795\n",
      "Train Epoch: 105 [2400/8040 (30%)]\tLoss: 91.997811\n",
      "Train Epoch: 105 [3600/8040 (45%)]\tLoss: 91.512614\n",
      "Train Epoch: 105 [4800/8040 (60%)]\tLoss: 91.458919\n",
      "Train Epoch: 105 [6000/8040 (75%)]\tLoss: 92.061816\n",
      "Train Epoch: 105 [7200/8040 (90%)]\tLoss: 92.531323\n",
      "====> Epoch: 105 Average loss: 91.8088\n",
      "epoch: 105, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 106 [0/8040 (0%)]\tLoss: 91.939665\n",
      "Train Epoch: 106 [1200/8040 (15%)]\tLoss: 91.693140\n",
      "Train Epoch: 106 [2400/8040 (30%)]\tLoss: 92.197257\n",
      "Train Epoch: 106 [3600/8040 (45%)]\tLoss: 91.643571\n",
      "Train Epoch: 106 [4800/8040 (60%)]\tLoss: 91.834220\n",
      "Train Epoch: 106 [6000/8040 (75%)]\tLoss: 92.249284\n",
      "Train Epoch: 106 [7200/8040 (90%)]\tLoss: 92.019295\n",
      "====> Epoch: 106 Average loss: 91.8119\n",
      "epoch: 106, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 107 [0/8040 (0%)]\tLoss: 91.836165\n",
      "Train Epoch: 107 [1200/8040 (15%)]\tLoss: 91.703133\n",
      "Train Epoch: 107 [2400/8040 (30%)]\tLoss: 91.810954\n",
      "Train Epoch: 107 [3600/8040 (45%)]\tLoss: 91.651497\n",
      "Train Epoch: 107 [4800/8040 (60%)]\tLoss: 91.686068\n",
      "Train Epoch: 107 [6000/8040 (75%)]\tLoss: 92.353898\n",
      "Train Epoch: 107 [7200/8040 (90%)]\tLoss: 92.187874\n",
      "====> Epoch: 107 Average loss: 91.7998\n",
      "epoch: 107, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 108 [0/8040 (0%)]\tLoss: 91.736808\n",
      "Train Epoch: 108 [1200/8040 (15%)]\tLoss: 91.824146\n",
      "Train Epoch: 108 [2400/8040 (30%)]\tLoss: 91.497640\n",
      "Train Epoch: 108 [3600/8040 (45%)]\tLoss: 91.400382\n",
      "Train Epoch: 108 [4800/8040 (60%)]\tLoss: 91.915682\n",
      "Train Epoch: 108 [6000/8040 (75%)]\tLoss: 92.042171\n",
      "Train Epoch: 108 [7200/8040 (90%)]\tLoss: 91.936003\n",
      "====> Epoch: 108 Average loss: 91.7882\n",
      "epoch: 108, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 109 [0/8040 (0%)]\tLoss: 92.107625\n",
      "Train Epoch: 109 [1200/8040 (15%)]\tLoss: 92.009440\n",
      "Train Epoch: 109 [2400/8040 (30%)]\tLoss: 91.979972\n",
      "Train Epoch: 109 [3600/8040 (45%)]\tLoss: 91.576001\n",
      "Train Epoch: 109 [4800/8040 (60%)]\tLoss: 91.868929\n",
      "Train Epoch: 109 [6000/8040 (75%)]\tLoss: 91.893669\n",
      "Train Epoch: 109 [7200/8040 (90%)]\tLoss: 91.528898\n",
      "====> Epoch: 109 Average loss: 91.8141\n",
      "epoch: 109, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 110 [0/8040 (0%)]\tLoss: 91.518962\n",
      "Train Epoch: 110 [1200/8040 (15%)]\tLoss: 92.186849\n",
      "Train Epoch: 110 [2400/8040 (30%)]\tLoss: 91.728044\n",
      "Train Epoch: 110 [3600/8040 (45%)]\tLoss: 91.832267\n",
      "Train Epoch: 110 [4800/8040 (60%)]\tLoss: 91.668840\n",
      "Train Epoch: 110 [6000/8040 (75%)]\tLoss: 91.883724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 110 [7200/8040 (90%)]\tLoss: 91.839421\n",
      "====> Epoch: 110 Average loss: 91.7995\n",
      "epoch: 110, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 111 [0/8040 (0%)]\tLoss: 92.278923\n",
      "Train Epoch: 111 [1200/8040 (15%)]\tLoss: 91.449805\n",
      "Train Epoch: 111 [2400/8040 (30%)]\tLoss: 92.569832\n",
      "Train Epoch: 111 [3600/8040 (45%)]\tLoss: 91.446777\n",
      "Train Epoch: 111 [4800/8040 (60%)]\tLoss: 91.598153\n",
      "Train Epoch: 111 [6000/8040 (75%)]\tLoss: 91.726408\n",
      "Train Epoch: 111 [7200/8040 (90%)]\tLoss: 91.240902\n",
      "====> Epoch: 111 Average loss: 91.7979\n",
      "epoch: 111, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 112 [0/8040 (0%)]\tLoss: 92.169255\n",
      "Train Epoch: 112 [1200/8040 (15%)]\tLoss: 92.091935\n",
      "Train Epoch: 112 [2400/8040 (30%)]\tLoss: 92.036263\n",
      "Train Epoch: 112 [3600/8040 (45%)]\tLoss: 91.610653\n",
      "Train Epoch: 112 [4800/8040 (60%)]\tLoss: 92.082983\n",
      "Train Epoch: 112 [6000/8040 (75%)]\tLoss: 91.572176\n",
      "Train Epoch: 112 [7200/8040 (90%)]\tLoss: 91.377238\n",
      "====> Epoch: 112 Average loss: 91.8034\n",
      "epoch: 112, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 113 [0/8040 (0%)]\tLoss: 92.006543\n",
      "Train Epoch: 113 [1200/8040 (15%)]\tLoss: 92.008439\n",
      "Train Epoch: 113 [2400/8040 (30%)]\tLoss: 92.371533\n",
      "Train Epoch: 113 [3600/8040 (45%)]\tLoss: 91.344604\n",
      "Train Epoch: 113 [4800/8040 (60%)]\tLoss: 91.371281\n",
      "Train Epoch: 113 [6000/8040 (75%)]\tLoss: 91.927083\n",
      "Train Epoch: 113 [7200/8040 (90%)]\tLoss: 92.028996\n",
      "====> Epoch: 113 Average loss: 91.8047\n",
      "epoch: 113, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 114 [0/8040 (0%)]\tLoss: 91.753442\n",
      "Train Epoch: 114 [1200/8040 (15%)]\tLoss: 91.854468\n",
      "Train Epoch: 114 [2400/8040 (30%)]\tLoss: 92.231234\n",
      "Train Epoch: 114 [3600/8040 (45%)]\tLoss: 92.146460\n",
      "Train Epoch: 114 [4800/8040 (60%)]\tLoss: 91.895378\n",
      "Train Epoch: 114 [6000/8040 (75%)]\tLoss: 91.907406\n",
      "Train Epoch: 114 [7200/8040 (90%)]\tLoss: 91.617228\n",
      "====> Epoch: 114 Average loss: 91.7972\n",
      "epoch: 114, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 115 [0/8040 (0%)]\tLoss: 91.773177\n",
      "Train Epoch: 115 [1200/8040 (15%)]\tLoss: 91.902856\n",
      "Train Epoch: 115 [2400/8040 (30%)]\tLoss: 91.339583\n",
      "Train Epoch: 115 [3600/8040 (45%)]\tLoss: 91.738525\n",
      "Train Epoch: 115 [4800/8040 (60%)]\tLoss: 92.008097\n",
      "Train Epoch: 115 [6000/8040 (75%)]\tLoss: 91.939364\n",
      "Train Epoch: 115 [7200/8040 (90%)]\tLoss: 91.751335\n",
      "====> Epoch: 115 Average loss: 91.7920\n",
      "epoch: 115, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 116 [0/8040 (0%)]\tLoss: 91.682756\n",
      "Train Epoch: 116 [1200/8040 (15%)]\tLoss: 91.985482\n",
      "Train Epoch: 116 [2400/8040 (30%)]\tLoss: 91.801229\n",
      "Train Epoch: 116 [3600/8040 (45%)]\tLoss: 91.458455\n",
      "Train Epoch: 116 [4800/8040 (60%)]\tLoss: 91.751172\n",
      "Train Epoch: 116 [6000/8040 (75%)]\tLoss: 92.118424\n",
      "Train Epoch: 116 [7200/8040 (90%)]\tLoss: 91.728467\n",
      "====> Epoch: 116 Average loss: 91.8024\n",
      "epoch: 116, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 117 [0/8040 (0%)]\tLoss: 91.528947\n",
      "Train Epoch: 117 [1200/8040 (15%)]\tLoss: 91.551847\n",
      "Train Epoch: 117 [2400/8040 (30%)]\tLoss: 91.696476\n",
      "Train Epoch: 117 [3600/8040 (45%)]\tLoss: 91.815951\n",
      "Train Epoch: 117 [4800/8040 (60%)]\tLoss: 91.979907\n",
      "Train Epoch: 117 [6000/8040 (75%)]\tLoss: 91.520760\n",
      "Train Epoch: 117 [7200/8040 (90%)]\tLoss: 91.448853\n",
      "====> Epoch: 117 Average loss: 91.8030\n",
      "epoch: 117, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 118 [0/8040 (0%)]\tLoss: 91.698568\n",
      "Train Epoch: 118 [1200/8040 (15%)]\tLoss: 91.946899\n",
      "Train Epoch: 118 [2400/8040 (30%)]\tLoss: 92.109432\n",
      "Train Epoch: 118 [3600/8040 (45%)]\tLoss: 91.719637\n",
      "Train Epoch: 118 [4800/8040 (60%)]\tLoss: 91.963078\n",
      "Train Epoch: 118 [6000/8040 (75%)]\tLoss: 91.685677\n",
      "Train Epoch: 118 [7200/8040 (90%)]\tLoss: 92.061190\n",
      "====> Epoch: 118 Average loss: 91.7868\n",
      "epoch: 118, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 119 [0/8040 (0%)]\tLoss: 91.718953\n",
      "Train Epoch: 119 [1200/8040 (15%)]\tLoss: 91.927311\n",
      "Train Epoch: 119 [2400/8040 (30%)]\tLoss: 91.755949\n",
      "Train Epoch: 119 [3600/8040 (45%)]\tLoss: 91.660270\n",
      "Train Epoch: 119 [4800/8040 (60%)]\tLoss: 92.131982\n",
      "Train Epoch: 119 [6000/8040 (75%)]\tLoss: 91.764827\n",
      "Train Epoch: 119 [7200/8040 (90%)]\tLoss: 91.487402\n",
      "====> Epoch: 119 Average loss: 91.7940\n",
      "epoch: 119, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 120 [0/8040 (0%)]\tLoss: 91.656331\n",
      "Train Epoch: 120 [1200/8040 (15%)]\tLoss: 91.914412\n",
      "Train Epoch: 120 [2400/8040 (30%)]\tLoss: 91.483382\n",
      "Train Epoch: 120 [3600/8040 (45%)]\tLoss: 92.021965\n",
      "Train Epoch: 120 [4800/8040 (60%)]\tLoss: 91.771265\n",
      "Train Epoch: 120 [6000/8040 (75%)]\tLoss: 91.740723\n",
      "Train Epoch: 120 [7200/8040 (90%)]\tLoss: 91.832796\n",
      "====> Epoch: 120 Average loss: 91.8106\n",
      "epoch: 120, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 121 [0/8040 (0%)]\tLoss: 91.060392\n",
      "Train Epoch: 121 [1200/8040 (15%)]\tLoss: 91.466911\n",
      "Train Epoch: 121 [2400/8040 (30%)]\tLoss: 91.316854\n",
      "Train Epoch: 121 [3600/8040 (45%)]\tLoss: 92.045239\n",
      "Train Epoch: 121 [4800/8040 (60%)]\tLoss: 91.884880\n",
      "Train Epoch: 121 [6000/8040 (75%)]\tLoss: 91.356217\n",
      "Train Epoch: 121 [7200/8040 (90%)]\tLoss: 91.481095\n",
      "====> Epoch: 121 Average loss: 91.8130\n",
      "epoch: 121, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 122 [0/8040 (0%)]\tLoss: 91.532511\n",
      "Train Epoch: 122 [1200/8040 (15%)]\tLoss: 91.684660\n",
      "Train Epoch: 122 [2400/8040 (30%)]\tLoss: 91.363346\n",
      "Train Epoch: 122 [3600/8040 (45%)]\tLoss: 91.471606\n",
      "Train Epoch: 122 [4800/8040 (60%)]\tLoss: 92.004875\n",
      "Train Epoch: 122 [6000/8040 (75%)]\tLoss: 92.380770\n",
      "Train Epoch: 122 [7200/8040 (90%)]\tLoss: 91.976099\n",
      "====> Epoch: 122 Average loss: 91.7869\n",
      "epoch: 122, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 123 [0/8040 (0%)]\tLoss: 91.741292\n",
      "Train Epoch: 123 [1200/8040 (15%)]\tLoss: 91.397965\n",
      "Train Epoch: 123 [2400/8040 (30%)]\tLoss: 91.402173\n",
      "Train Epoch: 123 [3600/8040 (45%)]\tLoss: 92.227751\n",
      "Train Epoch: 123 [4800/8040 (60%)]\tLoss: 91.747518\n",
      "Train Epoch: 123 [6000/8040 (75%)]\tLoss: 91.575285\n",
      "Train Epoch: 123 [7200/8040 (90%)]\tLoss: 91.948022\n",
      "====> Epoch: 123 Average loss: 91.7893\n",
      "epoch: 123, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 124 [0/8040 (0%)]\tLoss: 91.705501\n",
      "Train Epoch: 124 [1200/8040 (15%)]\tLoss: 92.483504\n",
      "Train Epoch: 124 [2400/8040 (30%)]\tLoss: 91.564722\n",
      "Train Epoch: 124 [3600/8040 (45%)]\tLoss: 91.229867\n",
      "Train Epoch: 124 [4800/8040 (60%)]\tLoss: 91.769019\n",
      "Train Epoch: 124 [6000/8040 (75%)]\tLoss: 91.755859\n",
      "Train Epoch: 124 [7200/8040 (90%)]\tLoss: 91.334163\n",
      "====> Epoch: 124 Average loss: 91.7868\n",
      "epoch: 124, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 125 [0/8040 (0%)]\tLoss: 92.393205\n",
      "Train Epoch: 125 [1200/8040 (15%)]\tLoss: 91.846053\n",
      "Train Epoch: 125 [2400/8040 (30%)]\tLoss: 91.529419\n",
      "Train Epoch: 125 [3600/8040 (45%)]\tLoss: 91.557202\n",
      "Train Epoch: 125 [4800/8040 (60%)]\tLoss: 91.769800\n",
      "Train Epoch: 125 [6000/8040 (75%)]\tLoss: 91.678320\n",
      "Train Epoch: 125 [7200/8040 (90%)]\tLoss: 91.600146\n",
      "====> Epoch: 125 Average loss: 91.7854\n",
      "epoch: 125, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 126 [0/8040 (0%)]\tLoss: 91.954940\n",
      "Train Epoch: 126 [1200/8040 (15%)]\tLoss: 91.517814\n",
      "Train Epoch: 126 [2400/8040 (30%)]\tLoss: 91.510807\n",
      "Train Epoch: 126 [3600/8040 (45%)]\tLoss: 91.804484\n",
      "Train Epoch: 126 [4800/8040 (60%)]\tLoss: 91.770947\n",
      "Train Epoch: 126 [6000/8040 (75%)]\tLoss: 91.714714\n",
      "Train Epoch: 126 [7200/8040 (90%)]\tLoss: 91.545231\n",
      "====> Epoch: 126 Average loss: 91.7881\n",
      "epoch: 126, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 127 [0/8040 (0%)]\tLoss: 91.575065\n",
      "Train Epoch: 127 [1200/8040 (15%)]\tLoss: 91.942179\n",
      "Train Epoch: 127 [2400/8040 (30%)]\tLoss: 92.136011\n",
      "Train Epoch: 127 [3600/8040 (45%)]\tLoss: 91.563997\n",
      "Train Epoch: 127 [4800/8040 (60%)]\tLoss: 91.985701\n",
      "Train Epoch: 127 [6000/8040 (75%)]\tLoss: 91.673901\n",
      "Train Epoch: 127 [7200/8040 (90%)]\tLoss: 91.900008\n",
      "====> Epoch: 127 Average loss: 91.7836\n",
      "epoch: 127, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 128 [0/8040 (0%)]\tLoss: 92.124845\n",
      "Train Epoch: 128 [1200/8040 (15%)]\tLoss: 92.290535\n",
      "Train Epoch: 128 [2400/8040 (30%)]\tLoss: 91.542049\n",
      "Train Epoch: 128 [3600/8040 (45%)]\tLoss: 91.526921\n",
      "Train Epoch: 128 [4800/8040 (60%)]\tLoss: 91.818449\n",
      "Train Epoch: 128 [6000/8040 (75%)]\tLoss: 91.965951\n",
      "Train Epoch: 128 [7200/8040 (90%)]\tLoss: 92.047656\n",
      "====> Epoch: 128 Average loss: 91.7836\n",
      "epoch: 128, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 129 [0/8040 (0%)]\tLoss: 91.590625\n",
      "Train Epoch: 129 [1200/8040 (15%)]\tLoss: 91.569173\n",
      "Train Epoch: 129 [2400/8040 (30%)]\tLoss: 91.360889\n",
      "Train Epoch: 129 [3600/8040 (45%)]\tLoss: 91.921558\n",
      "Train Epoch: 129 [4800/8040 (60%)]\tLoss: 92.095182\n",
      "Train Epoch: 129 [6000/8040 (75%)]\tLoss: 91.684652\n",
      "Train Epoch: 129 [7200/8040 (90%)]\tLoss: 91.738249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 129 Average loss: 91.8018\n",
      "epoch: 129, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 130 [0/8040 (0%)]\tLoss: 91.926522\n",
      "Train Epoch: 130 [1200/8040 (15%)]\tLoss: 91.605070\n",
      "Train Epoch: 130 [2400/8040 (30%)]\tLoss: 91.836035\n",
      "Train Epoch: 130 [3600/8040 (45%)]\tLoss: 92.302246\n",
      "Train Epoch: 130 [4800/8040 (60%)]\tLoss: 91.922249\n",
      "Train Epoch: 130 [6000/8040 (75%)]\tLoss: 92.202987\n",
      "Train Epoch: 130 [7200/8040 (90%)]\tLoss: 91.678630\n",
      "====> Epoch: 130 Average loss: 91.8096\n",
      "epoch: 130, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 131 [0/8040 (0%)]\tLoss: 91.298730\n",
      "Train Epoch: 131 [1200/8040 (15%)]\tLoss: 91.852743\n",
      "Train Epoch: 131 [2400/8040 (30%)]\tLoss: 91.837150\n",
      "Train Epoch: 131 [3600/8040 (45%)]\tLoss: 92.052368\n",
      "Train Epoch: 131 [4800/8040 (60%)]\tLoss: 91.679647\n",
      "Train Epoch: 131 [6000/8040 (75%)]\tLoss: 91.372900\n",
      "Train Epoch: 131 [7200/8040 (90%)]\tLoss: 91.572428\n",
      "====> Epoch: 131 Average loss: 91.7824\n",
      "epoch: 131, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 132 [0/8040 (0%)]\tLoss: 91.944995\n",
      "Train Epoch: 132 [1200/8040 (15%)]\tLoss: 92.033219\n",
      "Train Epoch: 132 [2400/8040 (30%)]\tLoss: 91.434432\n",
      "Train Epoch: 132 [3600/8040 (45%)]\tLoss: 92.153906\n",
      "Train Epoch: 132 [4800/8040 (60%)]\tLoss: 91.909432\n",
      "Train Epoch: 132 [6000/8040 (75%)]\tLoss: 91.878182\n",
      "Train Epoch: 132 [7200/8040 (90%)]\tLoss: 92.000594\n",
      "====> Epoch: 132 Average loss: 91.7831\n",
      "epoch: 132, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 133 [0/8040 (0%)]\tLoss: 91.957267\n",
      "Train Epoch: 133 [1200/8040 (15%)]\tLoss: 91.556120\n",
      "Train Epoch: 133 [2400/8040 (30%)]\tLoss: 92.018978\n",
      "Train Epoch: 133 [3600/8040 (45%)]\tLoss: 91.998364\n",
      "Train Epoch: 133 [4800/8040 (60%)]\tLoss: 91.874618\n",
      "Train Epoch: 133 [6000/8040 (75%)]\tLoss: 91.879972\n",
      "Train Epoch: 133 [7200/8040 (90%)]\tLoss: 91.328255\n",
      "====> Epoch: 133 Average loss: 91.8051\n",
      "epoch: 133, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 134 [0/8040 (0%)]\tLoss: 91.762565\n",
      "Train Epoch: 134 [1200/8040 (15%)]\tLoss: 91.828727\n",
      "Train Epoch: 134 [2400/8040 (30%)]\tLoss: 91.609782\n",
      "Train Epoch: 134 [3600/8040 (45%)]\tLoss: 91.940975\n",
      "Train Epoch: 134 [4800/8040 (60%)]\tLoss: 91.549349\n",
      "Train Epoch: 134 [6000/8040 (75%)]\tLoss: 91.275936\n",
      "Train Epoch: 134 [7200/8040 (90%)]\tLoss: 92.025008\n",
      "====> Epoch: 134 Average loss: 91.7655\n",
      "epoch: 134, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 135 [0/8040 (0%)]\tLoss: 91.671346\n",
      "Train Epoch: 135 [1200/8040 (15%)]\tLoss: 91.731877\n",
      "Train Epoch: 135 [2400/8040 (30%)]\tLoss: 92.356812\n",
      "Train Epoch: 135 [3600/8040 (45%)]\tLoss: 91.922900\n",
      "Train Epoch: 135 [4800/8040 (60%)]\tLoss: 91.896818\n",
      "Train Epoch: 135 [6000/8040 (75%)]\tLoss: 91.618831\n",
      "Train Epoch: 135 [7200/8040 (90%)]\tLoss: 92.068213\n",
      "====> Epoch: 135 Average loss: 91.7710\n",
      "epoch: 135, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 136 [0/8040 (0%)]\tLoss: 92.265820\n",
      "Train Epoch: 136 [1200/8040 (15%)]\tLoss: 91.651473\n",
      "Train Epoch: 136 [2400/8040 (30%)]\tLoss: 91.625179\n",
      "Train Epoch: 136 [3600/8040 (45%)]\tLoss: 91.809660\n",
      "Train Epoch: 136 [4800/8040 (60%)]\tLoss: 92.184766\n",
      "Train Epoch: 136 [6000/8040 (75%)]\tLoss: 91.877979\n",
      "Train Epoch: 136 [7200/8040 (90%)]\tLoss: 92.146134\n",
      "====> Epoch: 136 Average loss: 91.8037\n",
      "epoch: 136, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 137 [0/8040 (0%)]\tLoss: 91.827116\n",
      "Train Epoch: 137 [1200/8040 (15%)]\tLoss: 91.638314\n",
      "Train Epoch: 137 [2400/8040 (30%)]\tLoss: 91.773918\n",
      "Train Epoch: 137 [3600/8040 (45%)]\tLoss: 92.239551\n",
      "Train Epoch: 137 [4800/8040 (60%)]\tLoss: 91.913224\n",
      "Train Epoch: 137 [6000/8040 (75%)]\tLoss: 92.084627\n",
      "Train Epoch: 137 [7200/8040 (90%)]\tLoss: 91.807983\n",
      "====> Epoch: 137 Average loss: 91.7793\n",
      "epoch: 137, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 138 [0/8040 (0%)]\tLoss: 92.104704\n",
      "Train Epoch: 138 [1200/8040 (15%)]\tLoss: 92.059831\n",
      "Train Epoch: 138 [2400/8040 (30%)]\tLoss: 91.941219\n",
      "Train Epoch: 138 [3600/8040 (45%)]\tLoss: 91.877555\n",
      "Train Epoch: 138 [4800/8040 (60%)]\tLoss: 92.185400\n",
      "Train Epoch: 138 [6000/8040 (75%)]\tLoss: 91.768620\n",
      "Train Epoch: 138 [7200/8040 (90%)]\tLoss: 91.847144\n",
      "====> Epoch: 138 Average loss: 91.7969\n",
      "epoch: 138, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 139 [0/8040 (0%)]\tLoss: 91.427539\n",
      "Train Epoch: 139 [1200/8040 (15%)]\tLoss: 91.832251\n",
      "Train Epoch: 139 [2400/8040 (30%)]\tLoss: 91.757853\n",
      "Train Epoch: 139 [3600/8040 (45%)]\tLoss: 91.791154\n",
      "Train Epoch: 139 [4800/8040 (60%)]\tLoss: 91.890430\n",
      "Train Epoch: 139 [6000/8040 (75%)]\tLoss: 91.753288\n",
      "Train Epoch: 139 [7200/8040 (90%)]\tLoss: 91.789404\n",
      "====> Epoch: 139 Average loss: 91.7626\n",
      "epoch: 139, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 140 [0/8040 (0%)]\tLoss: 92.011434\n",
      "Train Epoch: 140 [1200/8040 (15%)]\tLoss: 91.895964\n",
      "Train Epoch: 140 [2400/8040 (30%)]\tLoss: 91.705542\n",
      "Train Epoch: 140 [3600/8040 (45%)]\tLoss: 92.028174\n",
      "Train Epoch: 140 [4800/8040 (60%)]\tLoss: 92.334937\n",
      "Train Epoch: 140 [6000/8040 (75%)]\tLoss: 91.558529\n",
      "Train Epoch: 140 [7200/8040 (90%)]\tLoss: 91.916423\n",
      "====> Epoch: 140 Average loss: 91.7905\n",
      "epoch: 140, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 141 [0/8040 (0%)]\tLoss: 91.607389\n",
      "Train Epoch: 141 [1200/8040 (15%)]\tLoss: 91.791463\n",
      "Train Epoch: 141 [2400/8040 (30%)]\tLoss: 91.716659\n",
      "Train Epoch: 141 [3600/8040 (45%)]\tLoss: 91.593563\n",
      "Train Epoch: 141 [4800/8040 (60%)]\tLoss: 91.460189\n",
      "Train Epoch: 141 [6000/8040 (75%)]\tLoss: 91.743880\n",
      "Train Epoch: 141 [7200/8040 (90%)]\tLoss: 91.848779\n",
      "====> Epoch: 141 Average loss: 91.7890\n",
      "epoch: 141, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 142 [0/8040 (0%)]\tLoss: 91.754574\n",
      "Train Epoch: 142 [1200/8040 (15%)]\tLoss: 91.979793\n",
      "Train Epoch: 142 [2400/8040 (30%)]\tLoss: 91.549398\n",
      "Train Epoch: 142 [3600/8040 (45%)]\tLoss: 91.616626\n",
      "Train Epoch: 142 [4800/8040 (60%)]\tLoss: 91.728247\n",
      "Train Epoch: 142 [6000/8040 (75%)]\tLoss: 91.831649\n",
      "Train Epoch: 142 [7200/8040 (90%)]\tLoss: 92.041097\n",
      "====> Epoch: 142 Average loss: 91.7677\n",
      "epoch: 142, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 143 [0/8040 (0%)]\tLoss: 91.341626\n",
      "Train Epoch: 143 [1200/8040 (15%)]\tLoss: 91.984033\n",
      "Train Epoch: 143 [2400/8040 (30%)]\tLoss: 91.430599\n",
      "Train Epoch: 143 [3600/8040 (45%)]\tLoss: 91.843929\n",
      "Train Epoch: 143 [4800/8040 (60%)]\tLoss: 92.070215\n",
      "Train Epoch: 143 [6000/8040 (75%)]\tLoss: 91.842277\n",
      "Train Epoch: 143 [7200/8040 (90%)]\tLoss: 92.081421\n",
      "====> Epoch: 143 Average loss: 91.7797\n",
      "epoch: 143, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 144 [0/8040 (0%)]\tLoss: 91.630168\n",
      "Train Epoch: 144 [1200/8040 (15%)]\tLoss: 91.980428\n",
      "Train Epoch: 144 [2400/8040 (30%)]\tLoss: 92.004858\n",
      "Train Epoch: 144 [3600/8040 (45%)]\tLoss: 91.977515\n",
      "Train Epoch: 144 [4800/8040 (60%)]\tLoss: 91.506527\n",
      "Train Epoch: 144 [6000/8040 (75%)]\tLoss: 92.197843\n",
      "Train Epoch: 144 [7200/8040 (90%)]\tLoss: 92.150627\n",
      "====> Epoch: 144 Average loss: 91.7972\n",
      "epoch: 144, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 145 [0/8040 (0%)]\tLoss: 91.706689\n",
      "Train Epoch: 145 [1200/8040 (15%)]\tLoss: 91.526221\n",
      "Train Epoch: 145 [2400/8040 (30%)]\tLoss: 91.923633\n",
      "Train Epoch: 145 [3600/8040 (45%)]\tLoss: 91.397819\n",
      "Train Epoch: 145 [4800/8040 (60%)]\tLoss: 91.731364\n",
      "Train Epoch: 145 [6000/8040 (75%)]\tLoss: 91.411784\n",
      "Train Epoch: 145 [7200/8040 (90%)]\tLoss: 91.453646\n",
      "====> Epoch: 145 Average loss: 91.7729\n",
      "epoch: 145, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 146 [0/8040 (0%)]\tLoss: 91.557812\n",
      "Train Epoch: 146 [1200/8040 (15%)]\tLoss: 91.826017\n",
      "Train Epoch: 146 [2400/8040 (30%)]\tLoss: 91.783341\n",
      "Train Epoch: 146 [3600/8040 (45%)]\tLoss: 91.787882\n",
      "Train Epoch: 146 [4800/8040 (60%)]\tLoss: 91.902718\n",
      "Train Epoch: 146 [6000/8040 (75%)]\tLoss: 91.699691\n",
      "Train Epoch: 146 [7200/8040 (90%)]\tLoss: 92.019735\n",
      "====> Epoch: 146 Average loss: 91.8070\n",
      "epoch: 146, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 147 [0/8040 (0%)]\tLoss: 91.706803\n",
      "Train Epoch: 147 [1200/8040 (15%)]\tLoss: 91.869548\n",
      "Train Epoch: 147 [2400/8040 (30%)]\tLoss: 91.882951\n",
      "Train Epoch: 147 [3600/8040 (45%)]\tLoss: 91.947510\n",
      "Train Epoch: 147 [4800/8040 (60%)]\tLoss: 91.994987\n",
      "Train Epoch: 147 [6000/8040 (75%)]\tLoss: 92.561727\n",
      "Train Epoch: 147 [7200/8040 (90%)]\tLoss: 91.891764\n",
      "====> Epoch: 147 Average loss: 91.7940\n",
      "epoch: 147, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 148 [0/8040 (0%)]\tLoss: 91.665145\n",
      "Train Epoch: 148 [1200/8040 (15%)]\tLoss: 91.260124\n",
      "Train Epoch: 148 [2400/8040 (30%)]\tLoss: 91.827368\n",
      "Train Epoch: 148 [3600/8040 (45%)]\tLoss: 91.861214\n",
      "Train Epoch: 148 [4800/8040 (60%)]\tLoss: 91.867228\n",
      "Train Epoch: 148 [6000/8040 (75%)]\tLoss: 91.860750\n",
      "Train Epoch: 148 [7200/8040 (90%)]\tLoss: 91.808130\n",
      "====> Epoch: 148 Average loss: 91.7687\n",
      "epoch: 148, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 149 [0/8040 (0%)]\tLoss: 91.778011\n",
      "Train Epoch: 149 [1200/8040 (15%)]\tLoss: 91.787402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 149 [2400/8040 (30%)]\tLoss: 91.889128\n",
      "Train Epoch: 149 [3600/8040 (45%)]\tLoss: 91.767936\n",
      "Train Epoch: 149 [4800/8040 (60%)]\tLoss: 91.156315\n",
      "Train Epoch: 149 [6000/8040 (75%)]\tLoss: 91.577246\n",
      "Train Epoch: 149 [7200/8040 (90%)]\tLoss: 92.270443\n",
      "====> Epoch: 149 Average loss: 91.7889\n",
      "epoch: 149, beta=0.01, frac_anom=0.05\n",
      "Train Epoch: 150 [0/8040 (0%)]\tLoss: 91.782023\n",
      "Train Epoch: 150 [1200/8040 (15%)]\tLoss: 91.697835\n",
      "Train Epoch: 150 [2400/8040 (30%)]\tLoss: 91.586369\n",
      "Train Epoch: 150 [3600/8040 (45%)]\tLoss: 91.651326\n",
      "Train Epoch: 150 [4800/8040 (60%)]\tLoss: 91.141455\n",
      "Train Epoch: 150 [6000/8040 (75%)]\tLoss: 91.667977\n",
      "Train Epoch: 150 [7200/8040 (90%)]\tLoss: 91.594645\n",
      "====> Epoch: 150 Average loss: 91.7887\n",
      "epoch: 150, beta=0.01, frac_anom=0.05\n",
      "====> Test set loss: 27.8960\n",
      "Train Epoch: 1 [0/8040 (0%)]\tLoss: 100.166056\n",
      "Train Epoch: 1 [1200/8040 (15%)]\tLoss: 94.247909\n",
      "Train Epoch: 1 [2400/8040 (30%)]\tLoss: 94.060278\n",
      "Train Epoch: 1 [3600/8040 (45%)]\tLoss: 94.292741\n",
      "Train Epoch: 1 [4800/8040 (60%)]\tLoss: 93.563387\n",
      "Train Epoch: 1 [6000/8040 (75%)]\tLoss: 93.706576\n",
      "Train Epoch: 1 [7200/8040 (90%)]\tLoss: 93.165015\n",
      "====> Epoch: 1 Average loss: 94.1675\n",
      "epoch: 1, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 2 [0/8040 (0%)]\tLoss: 93.270923\n",
      "Train Epoch: 2 [1200/8040 (15%)]\tLoss: 93.922176\n",
      "Train Epoch: 2 [2400/8040 (30%)]\tLoss: 93.354948\n",
      "Train Epoch: 2 [3600/8040 (45%)]\tLoss: 92.804386\n",
      "Train Epoch: 2 [4800/8040 (60%)]\tLoss: 93.490112\n",
      "Train Epoch: 2 [6000/8040 (75%)]\tLoss: 92.980916\n",
      "Train Epoch: 2 [7200/8040 (90%)]\tLoss: 93.370239\n",
      "====> Epoch: 2 Average loss: 93.2467\n",
      "epoch: 2, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 3 [0/8040 (0%)]\tLoss: 93.425936\n",
      "Train Epoch: 3 [1200/8040 (15%)]\tLoss: 92.987240\n",
      "Train Epoch: 3 [2400/8040 (30%)]\tLoss: 92.674674\n",
      "Train Epoch: 3 [3600/8040 (45%)]\tLoss: 93.228743\n",
      "Train Epoch: 3 [4800/8040 (60%)]\tLoss: 92.877930\n",
      "Train Epoch: 3 [6000/8040 (75%)]\tLoss: 92.696094\n",
      "Train Epoch: 3 [7200/8040 (90%)]\tLoss: 92.275480\n",
      "====> Epoch: 3 Average loss: 92.9976\n",
      "epoch: 3, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 4 [0/8040 (0%)]\tLoss: 93.049528\n",
      "Train Epoch: 4 [1200/8040 (15%)]\tLoss: 92.608480\n",
      "Train Epoch: 4 [2400/8040 (30%)]\tLoss: 92.450480\n",
      "Train Epoch: 4 [3600/8040 (45%)]\tLoss: 92.375659\n",
      "Train Epoch: 4 [4800/8040 (60%)]\tLoss: 92.688175\n",
      "Train Epoch: 4 [6000/8040 (75%)]\tLoss: 92.803507\n",
      "Train Epoch: 4 [7200/8040 (90%)]\tLoss: 92.212590\n",
      "====> Epoch: 4 Average loss: 92.7611\n",
      "epoch: 4, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 5 [0/8040 (0%)]\tLoss: 92.517700\n",
      "Train Epoch: 5 [1200/8040 (15%)]\tLoss: 92.508805\n",
      "Train Epoch: 5 [2400/8040 (30%)]\tLoss: 92.175212\n",
      "Train Epoch: 5 [3600/8040 (45%)]\tLoss: 92.643066\n",
      "Train Epoch: 5 [4800/8040 (60%)]\tLoss: 93.115609\n",
      "Train Epoch: 5 [6000/8040 (75%)]\tLoss: 93.245264\n",
      "Train Epoch: 5 [7200/8040 (90%)]\tLoss: 92.695532\n",
      "====> Epoch: 5 Average loss: 92.6527\n",
      "epoch: 5, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 6 [0/8040 (0%)]\tLoss: 92.832951\n",
      "Train Epoch: 6 [1200/8040 (15%)]\tLoss: 92.448730\n",
      "Train Epoch: 6 [2400/8040 (30%)]\tLoss: 92.818823\n",
      "Train Epoch: 6 [3600/8040 (45%)]\tLoss: 92.400667\n",
      "Train Epoch: 6 [4800/8040 (60%)]\tLoss: 92.141707\n",
      "Train Epoch: 6 [6000/8040 (75%)]\tLoss: 92.460246\n",
      "Train Epoch: 6 [7200/8040 (90%)]\tLoss: 92.270662\n",
      "====> Epoch: 6 Average loss: 92.5596\n",
      "epoch: 6, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 7 [0/8040 (0%)]\tLoss: 92.866243\n",
      "Train Epoch: 7 [1200/8040 (15%)]\tLoss: 92.504403\n",
      "Train Epoch: 7 [2400/8040 (30%)]\tLoss: 92.705233\n",
      "Train Epoch: 7 [3600/8040 (45%)]\tLoss: 93.095076\n",
      "Train Epoch: 7 [4800/8040 (60%)]\tLoss: 92.896509\n",
      "Train Epoch: 7 [6000/8040 (75%)]\tLoss: 92.918376\n",
      "Train Epoch: 7 [7200/8040 (90%)]\tLoss: 92.803898\n",
      "====> Epoch: 7 Average loss: 92.5255\n",
      "epoch: 7, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 8 [0/8040 (0%)]\tLoss: 92.268115\n",
      "Train Epoch: 8 [1200/8040 (15%)]\tLoss: 92.497339\n",
      "Train Epoch: 8 [2400/8040 (30%)]\tLoss: 92.725326\n",
      "Train Epoch: 8 [3600/8040 (45%)]\tLoss: 92.858781\n",
      "Train Epoch: 8 [4800/8040 (60%)]\tLoss: 92.187671\n",
      "Train Epoch: 8 [6000/8040 (75%)]\tLoss: 92.580444\n",
      "Train Epoch: 8 [7200/8040 (90%)]\tLoss: 92.497738\n",
      "====> Epoch: 8 Average loss: 92.4800\n",
      "epoch: 8, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 9 [0/8040 (0%)]\tLoss: 92.542171\n",
      "Train Epoch: 9 [1200/8040 (15%)]\tLoss: 92.719954\n",
      "Train Epoch: 9 [2400/8040 (30%)]\tLoss: 92.908740\n",
      "Train Epoch: 9 [3600/8040 (45%)]\tLoss: 92.048071\n",
      "Train Epoch: 9 [4800/8040 (60%)]\tLoss: 92.195589\n",
      "Train Epoch: 9 [6000/8040 (75%)]\tLoss: 92.517594\n",
      "Train Epoch: 9 [7200/8040 (90%)]\tLoss: 92.493864\n",
      "====> Epoch: 9 Average loss: 92.4667\n",
      "epoch: 9, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 10 [0/8040 (0%)]\tLoss: 92.408341\n",
      "Train Epoch: 10 [1200/8040 (15%)]\tLoss: 92.616895\n",
      "Train Epoch: 10 [2400/8040 (30%)]\tLoss: 92.565226\n",
      "Train Epoch: 10 [3600/8040 (45%)]\tLoss: 92.234285\n",
      "Train Epoch: 10 [4800/8040 (60%)]\tLoss: 92.569181\n",
      "Train Epoch: 10 [6000/8040 (75%)]\tLoss: 92.289762\n",
      "Train Epoch: 10 [7200/8040 (90%)]\tLoss: 92.212492\n",
      "====> Epoch: 10 Average loss: 92.4671\n",
      "epoch: 10, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 11 [0/8040 (0%)]\tLoss: 92.532178\n",
      "Train Epoch: 11 [1200/8040 (15%)]\tLoss: 92.359554\n",
      "Train Epoch: 11 [2400/8040 (30%)]\tLoss: 92.215438\n",
      "Train Epoch: 11 [3600/8040 (45%)]\tLoss: 92.615837\n",
      "Train Epoch: 11 [4800/8040 (60%)]\tLoss: 92.098836\n",
      "Train Epoch: 11 [6000/8040 (75%)]\tLoss: 92.782739\n",
      "Train Epoch: 11 [7200/8040 (90%)]\tLoss: 92.782910\n",
      "====> Epoch: 11 Average loss: 92.4279\n",
      "epoch: 11, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 12 [0/8040 (0%)]\tLoss: 92.836922\n",
      "Train Epoch: 12 [1200/8040 (15%)]\tLoss: 92.166691\n",
      "Train Epoch: 12 [2400/8040 (30%)]\tLoss: 92.250562\n",
      "Train Epoch: 12 [3600/8040 (45%)]\tLoss: 92.425057\n",
      "Train Epoch: 12 [4800/8040 (60%)]\tLoss: 92.393123\n",
      "Train Epoch: 12 [6000/8040 (75%)]\tLoss: 92.121102\n",
      "Train Epoch: 12 [7200/8040 (90%)]\tLoss: 92.436580\n",
      "====> Epoch: 12 Average loss: 92.4313\n",
      "epoch: 12, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 13 [0/8040 (0%)]\tLoss: 92.434123\n",
      "Train Epoch: 13 [1200/8040 (15%)]\tLoss: 92.452490\n",
      "Train Epoch: 13 [2400/8040 (30%)]\tLoss: 92.474536\n",
      "Train Epoch: 13 [3600/8040 (45%)]\tLoss: 92.695036\n",
      "Train Epoch: 13 [4800/8040 (60%)]\tLoss: 92.262240\n",
      "Train Epoch: 13 [6000/8040 (75%)]\tLoss: 92.576839\n",
      "Train Epoch: 13 [7200/8040 (90%)]\tLoss: 92.269792\n",
      "====> Epoch: 13 Average loss: 92.4124\n",
      "epoch: 13, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 14 [0/8040 (0%)]\tLoss: 92.524471\n",
      "Train Epoch: 14 [1200/8040 (15%)]\tLoss: 92.640348\n",
      "Train Epoch: 14 [2400/8040 (30%)]\tLoss: 92.922160\n",
      "Train Epoch: 14 [3600/8040 (45%)]\tLoss: 92.515666\n",
      "Train Epoch: 14 [4800/8040 (60%)]\tLoss: 92.188468\n",
      "Train Epoch: 14 [6000/8040 (75%)]\tLoss: 92.136800\n",
      "Train Epoch: 14 [7200/8040 (90%)]\tLoss: 92.034798\n",
      "====> Epoch: 14 Average loss: 92.3996\n",
      "epoch: 14, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 15 [0/8040 (0%)]\tLoss: 92.252596\n",
      "Train Epoch: 15 [1200/8040 (15%)]\tLoss: 92.348446\n",
      "Train Epoch: 15 [2400/8040 (30%)]\tLoss: 92.724032\n",
      "Train Epoch: 15 [3600/8040 (45%)]\tLoss: 91.956584\n",
      "Train Epoch: 15 [4800/8040 (60%)]\tLoss: 92.414535\n",
      "Train Epoch: 15 [6000/8040 (75%)]\tLoss: 92.194149\n",
      "Train Epoch: 15 [7200/8040 (90%)]\tLoss: 92.428882\n",
      "====> Epoch: 15 Average loss: 92.3673\n",
      "epoch: 15, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 16 [0/8040 (0%)]\tLoss: 92.354053\n",
      "Train Epoch: 16 [1200/8040 (15%)]\tLoss: 92.508472\n",
      "Train Epoch: 16 [2400/8040 (30%)]\tLoss: 92.400342\n",
      "Train Epoch: 16 [3600/8040 (45%)]\tLoss: 92.987492\n",
      "Train Epoch: 16 [4800/8040 (60%)]\tLoss: 91.941431\n",
      "Train Epoch: 16 [6000/8040 (75%)]\tLoss: 92.271615\n",
      "Train Epoch: 16 [7200/8040 (90%)]\tLoss: 92.271362\n",
      "====> Epoch: 16 Average loss: 92.3937\n",
      "epoch: 16, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 17 [0/8040 (0%)]\tLoss: 92.308708\n",
      "Train Epoch: 17 [1200/8040 (15%)]\tLoss: 92.579020\n",
      "Train Epoch: 17 [2400/8040 (30%)]\tLoss: 92.035978\n",
      "Train Epoch: 17 [3600/8040 (45%)]\tLoss: 92.202995\n",
      "Train Epoch: 17 [4800/8040 (60%)]\tLoss: 92.233138\n",
      "Train Epoch: 17 [6000/8040 (75%)]\tLoss: 92.334814\n",
      "Train Epoch: 17 [7200/8040 (90%)]\tLoss: 92.374455\n",
      "====> Epoch: 17 Average loss: 92.3867\n",
      "epoch: 17, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 18 [0/8040 (0%)]\tLoss: 92.566805\n",
      "Train Epoch: 18 [1200/8040 (15%)]\tLoss: 92.077417\n",
      "Train Epoch: 18 [2400/8040 (30%)]\tLoss: 92.501343\n",
      "Train Epoch: 18 [3600/8040 (45%)]\tLoss: 91.976465\n",
      "Train Epoch: 18 [4800/8040 (60%)]\tLoss: 92.584098\n",
      "Train Epoch: 18 [6000/8040 (75%)]\tLoss: 92.171623\n",
      "Train Epoch: 18 [7200/8040 (90%)]\tLoss: 92.343091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 18 Average loss: 92.3414\n",
      "epoch: 18, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 19 [0/8040 (0%)]\tLoss: 91.680127\n",
      "Train Epoch: 19 [1200/8040 (15%)]\tLoss: 92.125431\n",
      "Train Epoch: 19 [2400/8040 (30%)]\tLoss: 92.093766\n",
      "Train Epoch: 19 [3600/8040 (45%)]\tLoss: 92.240820\n",
      "Train Epoch: 19 [4800/8040 (60%)]\tLoss: 92.265112\n",
      "Train Epoch: 19 [6000/8040 (75%)]\tLoss: 92.249894\n",
      "Train Epoch: 19 [7200/8040 (90%)]\tLoss: 92.663656\n",
      "====> Epoch: 19 Average loss: 92.3308\n",
      "epoch: 19, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 20 [0/8040 (0%)]\tLoss: 92.556348\n",
      "Train Epoch: 20 [1200/8040 (15%)]\tLoss: 92.109326\n",
      "Train Epoch: 20 [2400/8040 (30%)]\tLoss: 92.321680\n",
      "Train Epoch: 20 [3600/8040 (45%)]\tLoss: 92.785734\n",
      "Train Epoch: 20 [4800/8040 (60%)]\tLoss: 91.506169\n",
      "Train Epoch: 20 [6000/8040 (75%)]\tLoss: 92.695068\n",
      "Train Epoch: 20 [7200/8040 (90%)]\tLoss: 92.402393\n",
      "====> Epoch: 20 Average loss: 92.3566\n",
      "epoch: 20, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 21 [0/8040 (0%)]\tLoss: 92.247567\n",
      "Train Epoch: 21 [1200/8040 (15%)]\tLoss: 92.526383\n",
      "Train Epoch: 21 [2400/8040 (30%)]\tLoss: 92.454248\n",
      "Train Epoch: 21 [3600/8040 (45%)]\tLoss: 92.300431\n",
      "Train Epoch: 21 [4800/8040 (60%)]\tLoss: 92.407039\n",
      "Train Epoch: 21 [6000/8040 (75%)]\tLoss: 91.966740\n",
      "Train Epoch: 21 [7200/8040 (90%)]\tLoss: 92.219377\n",
      "====> Epoch: 21 Average loss: 92.3460\n",
      "epoch: 21, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 22 [0/8040 (0%)]\tLoss: 92.348511\n",
      "Train Epoch: 22 [1200/8040 (15%)]\tLoss: 92.061466\n",
      "Train Epoch: 22 [2400/8040 (30%)]\tLoss: 92.596248\n",
      "Train Epoch: 22 [3600/8040 (45%)]\tLoss: 92.452775\n",
      "Train Epoch: 22 [4800/8040 (60%)]\tLoss: 92.387549\n",
      "Train Epoch: 22 [6000/8040 (75%)]\tLoss: 91.957397\n",
      "Train Epoch: 22 [7200/8040 (90%)]\tLoss: 92.732479\n",
      "====> Epoch: 22 Average loss: 92.3209\n",
      "epoch: 22, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 23 [0/8040 (0%)]\tLoss: 92.317033\n",
      "Train Epoch: 23 [1200/8040 (15%)]\tLoss: 93.430558\n",
      "Train Epoch: 23 [2400/8040 (30%)]\tLoss: 92.001025\n",
      "Train Epoch: 23 [3600/8040 (45%)]\tLoss: 92.126237\n",
      "Train Epoch: 23 [4800/8040 (60%)]\tLoss: 93.008407\n",
      "Train Epoch: 23 [6000/8040 (75%)]\tLoss: 92.011377\n",
      "Train Epoch: 23 [7200/8040 (90%)]\tLoss: 92.392342\n",
      "====> Epoch: 23 Average loss: 92.3120\n",
      "epoch: 23, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 24 [0/8040 (0%)]\tLoss: 92.745272\n",
      "Train Epoch: 24 [1200/8040 (15%)]\tLoss: 91.135498\n",
      "Train Epoch: 24 [2400/8040 (30%)]\tLoss: 92.291073\n",
      "Train Epoch: 24 [3600/8040 (45%)]\tLoss: 91.697290\n",
      "Train Epoch: 24 [4800/8040 (60%)]\tLoss: 92.376343\n",
      "Train Epoch: 24 [6000/8040 (75%)]\tLoss: 92.678857\n",
      "Train Epoch: 24 [7200/8040 (90%)]\tLoss: 92.315991\n",
      "====> Epoch: 24 Average loss: 92.3128\n",
      "epoch: 24, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 25 [0/8040 (0%)]\tLoss: 92.574756\n",
      "Train Epoch: 25 [1200/8040 (15%)]\tLoss: 92.205542\n",
      "Train Epoch: 25 [2400/8040 (30%)]\tLoss: 91.747542\n",
      "Train Epoch: 25 [3600/8040 (45%)]\tLoss: 92.535124\n",
      "Train Epoch: 25 [4800/8040 (60%)]\tLoss: 92.637410\n",
      "Train Epoch: 25 [6000/8040 (75%)]\tLoss: 92.753678\n",
      "Train Epoch: 25 [7200/8040 (90%)]\tLoss: 92.030680\n",
      "====> Epoch: 25 Average loss: 92.3193\n",
      "epoch: 25, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 26 [0/8040 (0%)]\tLoss: 92.465218\n",
      "Train Epoch: 26 [1200/8040 (15%)]\tLoss: 92.037329\n",
      "Train Epoch: 26 [2400/8040 (30%)]\tLoss: 92.205900\n",
      "Train Epoch: 26 [3600/8040 (45%)]\tLoss: 92.656698\n",
      "Train Epoch: 26 [4800/8040 (60%)]\tLoss: 92.844385\n",
      "Train Epoch: 26 [6000/8040 (75%)]\tLoss: 92.512655\n",
      "Train Epoch: 26 [7200/8040 (90%)]\tLoss: 92.585758\n",
      "====> Epoch: 26 Average loss: 92.3021\n",
      "epoch: 26, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 27 [0/8040 (0%)]\tLoss: 92.192106\n",
      "Train Epoch: 27 [1200/8040 (15%)]\tLoss: 92.331372\n",
      "Train Epoch: 27 [2400/8040 (30%)]\tLoss: 91.882048\n",
      "Train Epoch: 27 [3600/8040 (45%)]\tLoss: 92.325374\n",
      "Train Epoch: 27 [4800/8040 (60%)]\tLoss: 92.371631\n",
      "Train Epoch: 27 [6000/8040 (75%)]\tLoss: 92.351180\n",
      "Train Epoch: 27 [7200/8040 (90%)]\tLoss: 92.422681\n",
      "====> Epoch: 27 Average loss: 92.3032\n",
      "epoch: 27, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 28 [0/8040 (0%)]\tLoss: 92.314046\n",
      "Train Epoch: 28 [1200/8040 (15%)]\tLoss: 92.110189\n",
      "Train Epoch: 28 [2400/8040 (30%)]\tLoss: 92.370394\n",
      "Train Epoch: 28 [3600/8040 (45%)]\tLoss: 91.752555\n",
      "Train Epoch: 28 [4800/8040 (60%)]\tLoss: 92.687549\n",
      "Train Epoch: 28 [6000/8040 (75%)]\tLoss: 92.235661\n",
      "Train Epoch: 28 [7200/8040 (90%)]\tLoss: 92.101294\n",
      "====> Epoch: 28 Average loss: 92.2712\n",
      "epoch: 28, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 29 [0/8040 (0%)]\tLoss: 92.104061\n",
      "Train Epoch: 29 [1200/8040 (15%)]\tLoss: 91.796216\n",
      "Train Epoch: 29 [2400/8040 (30%)]\tLoss: 92.690153\n",
      "Train Epoch: 29 [3600/8040 (45%)]\tLoss: 92.620898\n",
      "Train Epoch: 29 [4800/8040 (60%)]\tLoss: 92.313574\n",
      "Train Epoch: 29 [6000/8040 (75%)]\tLoss: 92.652783\n",
      "Train Epoch: 29 [7200/8040 (90%)]\tLoss: 92.260343\n",
      "====> Epoch: 29 Average loss: 92.2864\n",
      "epoch: 29, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 30 [0/8040 (0%)]\tLoss: 92.543506\n",
      "Train Epoch: 30 [1200/8040 (15%)]\tLoss: 92.158374\n",
      "Train Epoch: 30 [2400/8040 (30%)]\tLoss: 92.850822\n",
      "Train Epoch: 30 [3600/8040 (45%)]\tLoss: 92.050659\n",
      "Train Epoch: 30 [4800/8040 (60%)]\tLoss: 92.292139\n",
      "Train Epoch: 30 [6000/8040 (75%)]\tLoss: 92.476383\n",
      "Train Epoch: 30 [7200/8040 (90%)]\tLoss: 92.458016\n",
      "====> Epoch: 30 Average loss: 92.2859\n",
      "epoch: 30, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 31 [0/8040 (0%)]\tLoss: 91.488883\n",
      "Train Epoch: 31 [1200/8040 (15%)]\tLoss: 92.538118\n",
      "Train Epoch: 31 [2400/8040 (30%)]\tLoss: 91.852433\n",
      "Train Epoch: 31 [3600/8040 (45%)]\tLoss: 92.746663\n",
      "Train Epoch: 31 [4800/8040 (60%)]\tLoss: 91.937020\n",
      "Train Epoch: 31 [6000/8040 (75%)]\tLoss: 92.076204\n",
      "Train Epoch: 31 [7200/8040 (90%)]\tLoss: 92.561800\n",
      "====> Epoch: 31 Average loss: 92.2762\n",
      "epoch: 31, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 32 [0/8040 (0%)]\tLoss: 92.622852\n",
      "Train Epoch: 32 [1200/8040 (15%)]\tLoss: 92.280208\n",
      "Train Epoch: 32 [2400/8040 (30%)]\tLoss: 92.667407\n",
      "Train Epoch: 32 [3600/8040 (45%)]\tLoss: 92.296086\n",
      "Train Epoch: 32 [4800/8040 (60%)]\tLoss: 92.122510\n",
      "Train Epoch: 32 [6000/8040 (75%)]\tLoss: 91.396362\n",
      "Train Epoch: 32 [7200/8040 (90%)]\tLoss: 92.595931\n",
      "====> Epoch: 32 Average loss: 92.2918\n",
      "epoch: 32, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 33 [0/8040 (0%)]\tLoss: 91.999382\n",
      "Train Epoch: 33 [1200/8040 (15%)]\tLoss: 92.506421\n",
      "Train Epoch: 33 [2400/8040 (30%)]\tLoss: 92.164225\n",
      "Train Epoch: 33 [3600/8040 (45%)]\tLoss: 92.180762\n",
      "Train Epoch: 33 [4800/8040 (60%)]\tLoss: 92.242521\n",
      "Train Epoch: 33 [6000/8040 (75%)]\tLoss: 92.383049\n",
      "Train Epoch: 33 [7200/8040 (90%)]\tLoss: 91.844767\n",
      "====> Epoch: 33 Average loss: 92.2682\n",
      "epoch: 33, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 34 [0/8040 (0%)]\tLoss: 91.573055\n",
      "Train Epoch: 34 [1200/8040 (15%)]\tLoss: 92.136589\n",
      "Train Epoch: 34 [2400/8040 (30%)]\tLoss: 92.378955\n",
      "Train Epoch: 34 [3600/8040 (45%)]\tLoss: 92.507902\n",
      "Train Epoch: 34 [4800/8040 (60%)]\tLoss: 92.229378\n",
      "Train Epoch: 34 [6000/8040 (75%)]\tLoss: 91.901514\n",
      "Train Epoch: 34 [7200/8040 (90%)]\tLoss: 92.164648\n",
      "====> Epoch: 34 Average loss: 92.2943\n",
      "epoch: 34, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 35 [0/8040 (0%)]\tLoss: 92.075269\n",
      "Train Epoch: 35 [1200/8040 (15%)]\tLoss: 92.172591\n",
      "Train Epoch: 35 [2400/8040 (30%)]\tLoss: 92.209440\n",
      "Train Epoch: 35 [3600/8040 (45%)]\tLoss: 91.886711\n",
      "Train Epoch: 35 [4800/8040 (60%)]\tLoss: 92.499658\n",
      "Train Epoch: 35 [6000/8040 (75%)]\tLoss: 92.060555\n",
      "Train Epoch: 35 [7200/8040 (90%)]\tLoss: 92.264128\n",
      "====> Epoch: 35 Average loss: 92.2824\n",
      "epoch: 35, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 36 [0/8040 (0%)]\tLoss: 92.488485\n",
      "Train Epoch: 36 [1200/8040 (15%)]\tLoss: 92.445133\n",
      "Train Epoch: 36 [2400/8040 (30%)]\tLoss: 92.605282\n",
      "Train Epoch: 36 [3600/8040 (45%)]\tLoss: 91.801440\n",
      "Train Epoch: 36 [4800/8040 (60%)]\tLoss: 91.540308\n",
      "Train Epoch: 36 [6000/8040 (75%)]\tLoss: 92.326799\n",
      "Train Epoch: 36 [7200/8040 (90%)]\tLoss: 92.150798\n",
      "====> Epoch: 36 Average loss: 92.2642\n",
      "epoch: 36, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 37 [0/8040 (0%)]\tLoss: 92.282503\n",
      "Train Epoch: 37 [1200/8040 (15%)]\tLoss: 92.369556\n",
      "Train Epoch: 37 [2400/8040 (30%)]\tLoss: 92.645060\n",
      "Train Epoch: 37 [3600/8040 (45%)]\tLoss: 91.624813\n",
      "Train Epoch: 37 [4800/8040 (60%)]\tLoss: 92.126815\n",
      "Train Epoch: 37 [6000/8040 (75%)]\tLoss: 92.703328\n",
      "Train Epoch: 37 [7200/8040 (90%)]\tLoss: 92.191691\n",
      "====> Epoch: 37 Average loss: 92.2786\n",
      "epoch: 37, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 38 [0/8040 (0%)]\tLoss: 92.665592\n",
      "Train Epoch: 38 [1200/8040 (15%)]\tLoss: 92.313892\n",
      "Train Epoch: 38 [2400/8040 (30%)]\tLoss: 92.203385\n",
      "Train Epoch: 38 [3600/8040 (45%)]\tLoss: 92.386947\n",
      "Train Epoch: 38 [4800/8040 (60%)]\tLoss: 92.271509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 38 [6000/8040 (75%)]\tLoss: 92.214339\n",
      "Train Epoch: 38 [7200/8040 (90%)]\tLoss: 91.948324\n",
      "====> Epoch: 38 Average loss: 92.2620\n",
      "epoch: 38, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 39 [0/8040 (0%)]\tLoss: 92.054264\n",
      "Train Epoch: 39 [1200/8040 (15%)]\tLoss: 92.531217\n",
      "Train Epoch: 39 [2400/8040 (30%)]\tLoss: 91.794963\n",
      "Train Epoch: 39 [3600/8040 (45%)]\tLoss: 92.097347\n",
      "Train Epoch: 39 [4800/8040 (60%)]\tLoss: 92.222201\n",
      "Train Epoch: 39 [6000/8040 (75%)]\tLoss: 92.877026\n",
      "Train Epoch: 39 [7200/8040 (90%)]\tLoss: 91.964510\n",
      "====> Epoch: 39 Average loss: 92.2507\n",
      "epoch: 39, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 40 [0/8040 (0%)]\tLoss: 92.532121\n",
      "Train Epoch: 40 [1200/8040 (15%)]\tLoss: 92.300944\n",
      "Train Epoch: 40 [2400/8040 (30%)]\tLoss: 92.187956\n",
      "Train Epoch: 40 [3600/8040 (45%)]\tLoss: 92.049854\n",
      "Train Epoch: 40 [4800/8040 (60%)]\tLoss: 92.215617\n",
      "Train Epoch: 40 [6000/8040 (75%)]\tLoss: 92.028581\n",
      "Train Epoch: 40 [7200/8040 (90%)]\tLoss: 92.284188\n",
      "====> Epoch: 40 Average loss: 92.2733\n",
      "epoch: 40, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 41 [0/8040 (0%)]\tLoss: 92.174333\n",
      "Train Epoch: 41 [1200/8040 (15%)]\tLoss: 92.399438\n",
      "Train Epoch: 41 [2400/8040 (30%)]\tLoss: 92.230632\n",
      "Train Epoch: 41 [3600/8040 (45%)]\tLoss: 92.210653\n",
      "Train Epoch: 41 [4800/8040 (60%)]\tLoss: 91.774080\n",
      "Train Epoch: 41 [6000/8040 (75%)]\tLoss: 92.209928\n",
      "Train Epoch: 41 [7200/8040 (90%)]\tLoss: 91.741455\n",
      "====> Epoch: 41 Average loss: 92.2616\n",
      "epoch: 41, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 42 [0/8040 (0%)]\tLoss: 92.359741\n",
      "Train Epoch: 42 [1200/8040 (15%)]\tLoss: 92.134139\n",
      "Train Epoch: 42 [2400/8040 (30%)]\tLoss: 92.084294\n",
      "Train Epoch: 42 [3600/8040 (45%)]\tLoss: 92.911873\n",
      "Train Epoch: 42 [4800/8040 (60%)]\tLoss: 91.613525\n",
      "Train Epoch: 42 [6000/8040 (75%)]\tLoss: 92.403695\n",
      "Train Epoch: 42 [7200/8040 (90%)]\tLoss: 92.423096\n",
      "====> Epoch: 42 Average loss: 92.2599\n",
      "epoch: 42, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 43 [0/8040 (0%)]\tLoss: 92.175366\n",
      "Train Epoch: 43 [1200/8040 (15%)]\tLoss: 91.733244\n",
      "Train Epoch: 43 [2400/8040 (30%)]\tLoss: 92.378768\n",
      "Train Epoch: 43 [3600/8040 (45%)]\tLoss: 92.362256\n",
      "Train Epoch: 43 [4800/8040 (60%)]\tLoss: 91.948470\n",
      "Train Epoch: 43 [6000/8040 (75%)]\tLoss: 92.278979\n",
      "Train Epoch: 43 [7200/8040 (90%)]\tLoss: 92.332601\n",
      "====> Epoch: 43 Average loss: 92.2486\n",
      "epoch: 43, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 44 [0/8040 (0%)]\tLoss: 92.323999\n",
      "Train Epoch: 44 [1200/8040 (15%)]\tLoss: 91.928638\n",
      "Train Epoch: 44 [2400/8040 (30%)]\tLoss: 92.436605\n",
      "Train Epoch: 44 [3600/8040 (45%)]\tLoss: 92.331299\n",
      "Train Epoch: 44 [4800/8040 (60%)]\tLoss: 92.274349\n",
      "Train Epoch: 44 [6000/8040 (75%)]\tLoss: 92.169873\n",
      "Train Epoch: 44 [7200/8040 (90%)]\tLoss: 91.980591\n",
      "====> Epoch: 44 Average loss: 92.2314\n",
      "epoch: 44, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 45 [0/8040 (0%)]\tLoss: 92.464152\n",
      "Train Epoch: 45 [1200/8040 (15%)]\tLoss: 91.626221\n",
      "Train Epoch: 45 [2400/8040 (30%)]\tLoss: 92.186629\n",
      "Train Epoch: 45 [3600/8040 (45%)]\tLoss: 92.260588\n",
      "Train Epoch: 45 [4800/8040 (60%)]\tLoss: 91.961629\n",
      "Train Epoch: 45 [6000/8040 (75%)]\tLoss: 92.443734\n",
      "Train Epoch: 45 [7200/8040 (90%)]\tLoss: 92.134782\n",
      "====> Epoch: 45 Average loss: 92.2300\n",
      "epoch: 45, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 46 [0/8040 (0%)]\tLoss: 92.082935\n",
      "Train Epoch: 46 [1200/8040 (15%)]\tLoss: 92.309904\n",
      "Train Epoch: 46 [2400/8040 (30%)]\tLoss: 92.204061\n",
      "Train Epoch: 46 [3600/8040 (45%)]\tLoss: 91.756258\n",
      "Train Epoch: 46 [4800/8040 (60%)]\tLoss: 92.377620\n",
      "Train Epoch: 46 [6000/8040 (75%)]\tLoss: 91.762012\n",
      "Train Epoch: 46 [7200/8040 (90%)]\tLoss: 92.152238\n",
      "====> Epoch: 46 Average loss: 92.2334\n",
      "epoch: 46, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 47 [0/8040 (0%)]\tLoss: 92.349373\n",
      "Train Epoch: 47 [1200/8040 (15%)]\tLoss: 91.816325\n",
      "Train Epoch: 47 [2400/8040 (30%)]\tLoss: 92.683073\n",
      "Train Epoch: 47 [3600/8040 (45%)]\tLoss: 92.357129\n",
      "Train Epoch: 47 [4800/8040 (60%)]\tLoss: 92.817253\n",
      "Train Epoch: 47 [6000/8040 (75%)]\tLoss: 92.913892\n",
      "Train Epoch: 47 [7200/8040 (90%)]\tLoss: 92.532715\n",
      "====> Epoch: 47 Average loss: 92.2529\n",
      "epoch: 47, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 48 [0/8040 (0%)]\tLoss: 91.831364\n",
      "Train Epoch: 48 [1200/8040 (15%)]\tLoss: 92.334082\n",
      "Train Epoch: 48 [2400/8040 (30%)]\tLoss: 92.016602\n",
      "Train Epoch: 48 [3600/8040 (45%)]\tLoss: 92.363973\n",
      "Train Epoch: 48 [4800/8040 (60%)]\tLoss: 92.308610\n",
      "Train Epoch: 48 [6000/8040 (75%)]\tLoss: 92.541927\n",
      "Train Epoch: 48 [7200/8040 (90%)]\tLoss: 91.445630\n",
      "====> Epoch: 48 Average loss: 92.2175\n",
      "epoch: 48, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 49 [0/8040 (0%)]\tLoss: 92.167603\n",
      "Train Epoch: 49 [1200/8040 (15%)]\tLoss: 91.920142\n",
      "Train Epoch: 49 [2400/8040 (30%)]\tLoss: 92.088924\n",
      "Train Epoch: 49 [3600/8040 (45%)]\tLoss: 92.002100\n",
      "Train Epoch: 49 [4800/8040 (60%)]\tLoss: 92.551253\n",
      "Train Epoch: 49 [6000/8040 (75%)]\tLoss: 92.220231\n",
      "Train Epoch: 49 [7200/8040 (90%)]\tLoss: 92.209847\n",
      "====> Epoch: 49 Average loss: 92.2301\n",
      "epoch: 49, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 50 [0/8040 (0%)]\tLoss: 92.105827\n",
      "Train Epoch: 50 [1200/8040 (15%)]\tLoss: 92.018245\n",
      "Train Epoch: 50 [2400/8040 (30%)]\tLoss: 92.570117\n",
      "Train Epoch: 50 [3600/8040 (45%)]\tLoss: 91.914925\n",
      "Train Epoch: 50 [4800/8040 (60%)]\tLoss: 92.204541\n",
      "Train Epoch: 50 [6000/8040 (75%)]\tLoss: 92.359505\n",
      "Train Epoch: 50 [7200/8040 (90%)]\tLoss: 92.399040\n",
      "====> Epoch: 50 Average loss: 92.2130\n",
      "epoch: 50, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 51 [0/8040 (0%)]\tLoss: 92.079997\n",
      "Train Epoch: 51 [1200/8040 (15%)]\tLoss: 92.448698\n",
      "Train Epoch: 51 [2400/8040 (30%)]\tLoss: 92.210295\n",
      "Train Epoch: 51 [3600/8040 (45%)]\tLoss: 92.390340\n",
      "Train Epoch: 51 [4800/8040 (60%)]\tLoss: 91.527726\n",
      "Train Epoch: 51 [6000/8040 (75%)]\tLoss: 92.559774\n",
      "Train Epoch: 51 [7200/8040 (90%)]\tLoss: 91.672412\n",
      "====> Epoch: 51 Average loss: 92.2303\n",
      "epoch: 51, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 52 [0/8040 (0%)]\tLoss: 92.805754\n",
      "Train Epoch: 52 [1200/8040 (15%)]\tLoss: 92.320858\n",
      "Train Epoch: 52 [2400/8040 (30%)]\tLoss: 91.983122\n",
      "Train Epoch: 52 [3600/8040 (45%)]\tLoss: 91.722485\n",
      "Train Epoch: 52 [4800/8040 (60%)]\tLoss: 92.286922\n",
      "Train Epoch: 52 [6000/8040 (75%)]\tLoss: 92.333879\n",
      "Train Epoch: 52 [7200/8040 (90%)]\tLoss: 92.108976\n",
      "====> Epoch: 52 Average loss: 92.2241\n",
      "epoch: 52, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 53 [0/8040 (0%)]\tLoss: 91.952856\n",
      "Train Epoch: 53 [1200/8040 (15%)]\tLoss: 92.177897\n",
      "Train Epoch: 53 [2400/8040 (30%)]\tLoss: 92.770011\n",
      "Train Epoch: 53 [3600/8040 (45%)]\tLoss: 92.289852\n",
      "Train Epoch: 53 [4800/8040 (60%)]\tLoss: 92.151912\n",
      "Train Epoch: 53 [6000/8040 (75%)]\tLoss: 92.422380\n",
      "Train Epoch: 53 [7200/8040 (90%)]\tLoss: 92.491756\n",
      "====> Epoch: 53 Average loss: 92.2567\n",
      "epoch: 53, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 54 [0/8040 (0%)]\tLoss: 91.974756\n",
      "Train Epoch: 54 [1200/8040 (15%)]\tLoss: 92.436637\n",
      "Train Epoch: 54 [2400/8040 (30%)]\tLoss: 91.833537\n",
      "Train Epoch: 54 [3600/8040 (45%)]\tLoss: 92.411906\n",
      "Train Epoch: 54 [4800/8040 (60%)]\tLoss: 91.874797\n",
      "Train Epoch: 54 [6000/8040 (75%)]\tLoss: 92.061353\n",
      "Train Epoch: 54 [7200/8040 (90%)]\tLoss: 91.641113\n",
      "====> Epoch: 54 Average loss: 92.2078\n",
      "epoch: 54, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 55 [0/8040 (0%)]\tLoss: 92.256673\n",
      "Train Epoch: 55 [1200/8040 (15%)]\tLoss: 92.162183\n",
      "Train Epoch: 55 [2400/8040 (30%)]\tLoss: 92.373722\n",
      "Train Epoch: 55 [3600/8040 (45%)]\tLoss: 92.296444\n",
      "Train Epoch: 55 [4800/8040 (60%)]\tLoss: 92.167863\n",
      "Train Epoch: 55 [6000/8040 (75%)]\tLoss: 91.828166\n",
      "Train Epoch: 55 [7200/8040 (90%)]\tLoss: 92.211076\n",
      "====> Epoch: 55 Average loss: 92.1949\n",
      "epoch: 55, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 56 [0/8040 (0%)]\tLoss: 92.160221\n",
      "Train Epoch: 56 [1200/8040 (15%)]\tLoss: 91.641854\n",
      "Train Epoch: 56 [2400/8040 (30%)]\tLoss: 91.774609\n",
      "Train Epoch: 56 [3600/8040 (45%)]\tLoss: 93.463346\n",
      "Train Epoch: 56 [4800/8040 (60%)]\tLoss: 91.981991\n",
      "Train Epoch: 56 [6000/8040 (75%)]\tLoss: 92.709424\n",
      "Train Epoch: 56 [7200/8040 (90%)]\tLoss: 92.422729\n",
      "====> Epoch: 56 Average loss: 92.2272\n",
      "epoch: 56, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 57 [0/8040 (0%)]\tLoss: 91.575594\n",
      "Train Epoch: 57 [1200/8040 (15%)]\tLoss: 92.348006\n",
      "Train Epoch: 57 [2400/8040 (30%)]\tLoss: 92.286084\n",
      "Train Epoch: 57 [3600/8040 (45%)]\tLoss: 91.606641\n",
      "Train Epoch: 57 [4800/8040 (60%)]\tLoss: 92.177799\n",
      "Train Epoch: 57 [6000/8040 (75%)]\tLoss: 92.407829\n",
      "Train Epoch: 57 [7200/8040 (90%)]\tLoss: 91.966170\n",
      "====> Epoch: 57 Average loss: 92.2237\n",
      "epoch: 57, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 58 [0/8040 (0%)]\tLoss: 92.033537\n",
      "Train Epoch: 58 [1200/8040 (15%)]\tLoss: 92.526823\n",
      "Train Epoch: 58 [2400/8040 (30%)]\tLoss: 92.317122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 58 [3600/8040 (45%)]\tLoss: 92.411320\n",
      "Train Epoch: 58 [4800/8040 (60%)]\tLoss: 92.315332\n",
      "Train Epoch: 58 [6000/8040 (75%)]\tLoss: 92.541740\n",
      "Train Epoch: 58 [7200/8040 (90%)]\tLoss: 92.473006\n",
      "====> Epoch: 58 Average loss: 92.2281\n",
      "epoch: 58, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 59 [0/8040 (0%)]\tLoss: 92.279443\n",
      "Train Epoch: 59 [1200/8040 (15%)]\tLoss: 91.886800\n",
      "Train Epoch: 59 [2400/8040 (30%)]\tLoss: 91.838647\n",
      "Train Epoch: 59 [3600/8040 (45%)]\tLoss: 92.523714\n",
      "Train Epoch: 59 [4800/8040 (60%)]\tLoss: 92.069312\n",
      "Train Epoch: 59 [6000/8040 (75%)]\tLoss: 92.124967\n",
      "Train Epoch: 59 [7200/8040 (90%)]\tLoss: 91.932609\n",
      "====> Epoch: 59 Average loss: 92.2340\n",
      "epoch: 59, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 60 [0/8040 (0%)]\tLoss: 92.318075\n",
      "Train Epoch: 60 [1200/8040 (15%)]\tLoss: 92.085864\n",
      "Train Epoch: 60 [2400/8040 (30%)]\tLoss: 92.361133\n",
      "Train Epoch: 60 [3600/8040 (45%)]\tLoss: 92.499910\n",
      "Train Epoch: 60 [4800/8040 (60%)]\tLoss: 92.172656\n",
      "Train Epoch: 60 [6000/8040 (75%)]\tLoss: 91.660075\n",
      "Train Epoch: 60 [7200/8040 (90%)]\tLoss: 92.148812\n",
      "====> Epoch: 60 Average loss: 92.1897\n",
      "epoch: 60, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 61 [0/8040 (0%)]\tLoss: 91.995809\n",
      "Train Epoch: 61 [1200/8040 (15%)]\tLoss: 92.128988\n",
      "Train Epoch: 61 [2400/8040 (30%)]\tLoss: 92.967944\n",
      "Train Epoch: 61 [3600/8040 (45%)]\tLoss: 92.424634\n",
      "Train Epoch: 61 [4800/8040 (60%)]\tLoss: 92.346053\n",
      "Train Epoch: 61 [6000/8040 (75%)]\tLoss: 92.049097\n",
      "Train Epoch: 61 [7200/8040 (90%)]\tLoss: 92.302458\n",
      "====> Epoch: 61 Average loss: 92.2059\n",
      "epoch: 61, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 62 [0/8040 (0%)]\tLoss: 92.068766\n",
      "Train Epoch: 62 [1200/8040 (15%)]\tLoss: 92.162655\n",
      "Train Epoch: 62 [2400/8040 (30%)]\tLoss: 92.273185\n",
      "Train Epoch: 62 [3600/8040 (45%)]\tLoss: 92.016423\n",
      "Train Epoch: 62 [4800/8040 (60%)]\tLoss: 92.019434\n",
      "Train Epoch: 62 [6000/8040 (75%)]\tLoss: 92.362866\n",
      "Train Epoch: 62 [7200/8040 (90%)]\tLoss: 92.214909\n",
      "====> Epoch: 62 Average loss: 92.1883\n",
      "epoch: 62, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 63 [0/8040 (0%)]\tLoss: 91.876945\n",
      "Train Epoch: 63 [1200/8040 (15%)]\tLoss: 92.066927\n",
      "Train Epoch: 63 [2400/8040 (30%)]\tLoss: 91.962508\n",
      "Train Epoch: 63 [3600/8040 (45%)]\tLoss: 92.065226\n",
      "Train Epoch: 63 [4800/8040 (60%)]\tLoss: 92.372819\n",
      "Train Epoch: 63 [6000/8040 (75%)]\tLoss: 92.609155\n",
      "Train Epoch: 63 [7200/8040 (90%)]\tLoss: 91.885563\n",
      "====> Epoch: 63 Average loss: 92.1898\n",
      "epoch: 63, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 64 [0/8040 (0%)]\tLoss: 92.299251\n",
      "Train Epoch: 64 [1200/8040 (15%)]\tLoss: 92.249194\n",
      "Train Epoch: 64 [2400/8040 (30%)]\tLoss: 92.404028\n",
      "Train Epoch: 64 [3600/8040 (45%)]\tLoss: 91.828524\n",
      "Train Epoch: 64 [4800/8040 (60%)]\tLoss: 92.442188\n",
      "Train Epoch: 64 [6000/8040 (75%)]\tLoss: 91.724886\n",
      "Train Epoch: 64 [7200/8040 (90%)]\tLoss: 92.363371\n",
      "====> Epoch: 64 Average loss: 92.1721\n",
      "epoch: 64, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 65 [0/8040 (0%)]\tLoss: 92.621777\n",
      "Train Epoch: 65 [1200/8040 (15%)]\tLoss: 92.051579\n",
      "Train Epoch: 65 [2400/8040 (30%)]\tLoss: 92.483195\n",
      "Train Epoch: 65 [3600/8040 (45%)]\tLoss: 91.937142\n",
      "Train Epoch: 65 [4800/8040 (60%)]\tLoss: 92.535083\n",
      "Train Epoch: 65 [6000/8040 (75%)]\tLoss: 92.702840\n",
      "Train Epoch: 65 [7200/8040 (90%)]\tLoss: 92.179004\n",
      "====> Epoch: 65 Average loss: 92.1817\n",
      "epoch: 65, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 66 [0/8040 (0%)]\tLoss: 92.131356\n",
      "Train Epoch: 66 [1200/8040 (15%)]\tLoss: 91.824829\n",
      "Train Epoch: 66 [2400/8040 (30%)]\tLoss: 92.336955\n",
      "Train Epoch: 66 [3600/8040 (45%)]\tLoss: 92.202783\n",
      "Train Epoch: 66 [4800/8040 (60%)]\tLoss: 92.198218\n",
      "Train Epoch: 66 [6000/8040 (75%)]\tLoss: 91.718612\n",
      "Train Epoch: 66 [7200/8040 (90%)]\tLoss: 92.374577\n",
      "====> Epoch: 66 Average loss: 92.1976\n",
      "epoch: 66, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 67 [0/8040 (0%)]\tLoss: 92.657674\n",
      "Train Epoch: 67 [1200/8040 (15%)]\tLoss: 91.745532\n",
      "Train Epoch: 67 [2400/8040 (30%)]\tLoss: 91.769401\n",
      "Train Epoch: 67 [3600/8040 (45%)]\tLoss: 91.791292\n",
      "Train Epoch: 67 [4800/8040 (60%)]\tLoss: 91.633024\n",
      "Train Epoch: 67 [6000/8040 (75%)]\tLoss: 92.340007\n",
      "Train Epoch: 67 [7200/8040 (90%)]\tLoss: 92.644092\n",
      "====> Epoch: 67 Average loss: 92.1818\n",
      "epoch: 67, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 68 [0/8040 (0%)]\tLoss: 91.449406\n",
      "Train Epoch: 68 [1200/8040 (15%)]\tLoss: 92.046208\n",
      "Train Epoch: 68 [2400/8040 (30%)]\tLoss: 92.598983\n",
      "Train Epoch: 68 [3600/8040 (45%)]\tLoss: 92.662484\n",
      "Train Epoch: 68 [4800/8040 (60%)]\tLoss: 91.909009\n",
      "Train Epoch: 68 [6000/8040 (75%)]\tLoss: 92.474056\n",
      "Train Epoch: 68 [7200/8040 (90%)]\tLoss: 92.442057\n",
      "====> Epoch: 68 Average loss: 92.1846\n",
      "epoch: 68, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 69 [0/8040 (0%)]\tLoss: 91.884562\n",
      "Train Epoch: 69 [1200/8040 (15%)]\tLoss: 92.004720\n",
      "Train Epoch: 69 [2400/8040 (30%)]\tLoss: 92.255811\n",
      "Train Epoch: 69 [3600/8040 (45%)]\tLoss: 92.192773\n",
      "Train Epoch: 69 [4800/8040 (60%)]\tLoss: 92.630697\n",
      "Train Epoch: 69 [6000/8040 (75%)]\tLoss: 92.278361\n",
      "Train Epoch: 69 [7200/8040 (90%)]\tLoss: 92.160807\n",
      "====> Epoch: 69 Average loss: 92.1832\n",
      "epoch: 69, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 70 [0/8040 (0%)]\tLoss: 92.404867\n",
      "Train Epoch: 70 [1200/8040 (15%)]\tLoss: 92.080835\n",
      "Train Epoch: 70 [2400/8040 (30%)]\tLoss: 91.932446\n",
      "Train Epoch: 70 [3600/8040 (45%)]\tLoss: 92.352856\n",
      "Train Epoch: 70 [4800/8040 (60%)]\tLoss: 92.228646\n",
      "Train Epoch: 70 [6000/8040 (75%)]\tLoss: 91.950643\n",
      "Train Epoch: 70 [7200/8040 (90%)]\tLoss: 91.410002\n",
      "====> Epoch: 70 Average loss: 92.1568\n",
      "epoch: 70, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 71 [0/8040 (0%)]\tLoss: 92.141789\n",
      "Train Epoch: 71 [1200/8040 (15%)]\tLoss: 92.366178\n",
      "Train Epoch: 71 [2400/8040 (30%)]\tLoss: 91.903711\n",
      "Train Epoch: 71 [3600/8040 (45%)]\tLoss: 92.299504\n",
      "Train Epoch: 71 [4800/8040 (60%)]\tLoss: 92.034123\n",
      "Train Epoch: 71 [6000/8040 (75%)]\tLoss: 92.198511\n",
      "Train Epoch: 71 [7200/8040 (90%)]\tLoss: 92.344637\n",
      "====> Epoch: 71 Average loss: 92.1917\n",
      "epoch: 71, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 72 [0/8040 (0%)]\tLoss: 91.644971\n",
      "Train Epoch: 72 [1200/8040 (15%)]\tLoss: 91.963118\n",
      "Train Epoch: 72 [2400/8040 (30%)]\tLoss: 91.499740\n",
      "Train Epoch: 72 [3600/8040 (45%)]\tLoss: 92.575798\n",
      "Train Epoch: 72 [4800/8040 (60%)]\tLoss: 92.044539\n",
      "Train Epoch: 72 [6000/8040 (75%)]\tLoss: 91.815503\n",
      "Train Epoch: 72 [7200/8040 (90%)]\tLoss: 92.471061\n",
      "====> Epoch: 72 Average loss: 92.1774\n",
      "epoch: 72, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 73 [0/8040 (0%)]\tLoss: 92.045133\n",
      "Train Epoch: 73 [1200/8040 (15%)]\tLoss: 92.232821\n",
      "Train Epoch: 73 [2400/8040 (30%)]\tLoss: 92.308114\n",
      "Train Epoch: 73 [3600/8040 (45%)]\tLoss: 92.346525\n",
      "Train Epoch: 73 [4800/8040 (60%)]\tLoss: 92.410685\n",
      "Train Epoch: 73 [6000/8040 (75%)]\tLoss: 92.057845\n",
      "Train Epoch: 73 [7200/8040 (90%)]\tLoss: 91.877523\n",
      "====> Epoch: 73 Average loss: 92.1683\n",
      "epoch: 73, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 74 [0/8040 (0%)]\tLoss: 91.949780\n",
      "Train Epoch: 74 [1200/8040 (15%)]\tLoss: 92.373193\n",
      "Train Epoch: 74 [2400/8040 (30%)]\tLoss: 92.201416\n",
      "Train Epoch: 74 [3600/8040 (45%)]\tLoss: 92.042912\n",
      "Train Epoch: 74 [4800/8040 (60%)]\tLoss: 92.118628\n",
      "Train Epoch: 74 [6000/8040 (75%)]\tLoss: 92.181356\n",
      "Train Epoch: 74 [7200/8040 (90%)]\tLoss: 92.407202\n",
      "====> Epoch: 74 Average loss: 92.1659\n",
      "epoch: 74, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 75 [0/8040 (0%)]\tLoss: 92.242871\n",
      "Train Epoch: 75 [1200/8040 (15%)]\tLoss: 91.921126\n",
      "Train Epoch: 75 [2400/8040 (30%)]\tLoss: 92.465983\n",
      "Train Epoch: 75 [3600/8040 (45%)]\tLoss: 91.774593\n",
      "Train Epoch: 75 [4800/8040 (60%)]\tLoss: 92.186930\n",
      "Train Epoch: 75 [6000/8040 (75%)]\tLoss: 92.070557\n",
      "Train Epoch: 75 [7200/8040 (90%)]\tLoss: 92.361060\n",
      "====> Epoch: 75 Average loss: 92.1509\n",
      "epoch: 75, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 76 [0/8040 (0%)]\tLoss: 91.794320\n",
      "Train Epoch: 76 [1200/8040 (15%)]\tLoss: 91.937451\n",
      "Train Epoch: 76 [2400/8040 (30%)]\tLoss: 92.172396\n",
      "Train Epoch: 76 [3600/8040 (45%)]\tLoss: 92.064388\n",
      "Train Epoch: 76 [4800/8040 (60%)]\tLoss: 92.066650\n",
      "Train Epoch: 76 [6000/8040 (75%)]\tLoss: 91.806470\n",
      "Train Epoch: 76 [7200/8040 (90%)]\tLoss: 92.328857\n",
      "====> Epoch: 76 Average loss: 92.1709\n",
      "epoch: 76, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 77 [0/8040 (0%)]\tLoss: 91.693840\n",
      "Train Epoch: 77 [1200/8040 (15%)]\tLoss: 92.544238\n",
      "Train Epoch: 77 [2400/8040 (30%)]\tLoss: 91.870475\n",
      "Train Epoch: 77 [3600/8040 (45%)]\tLoss: 92.524821\n",
      "Train Epoch: 77 [4800/8040 (60%)]\tLoss: 92.419352\n",
      "Train Epoch: 77 [6000/8040 (75%)]\tLoss: 92.159644\n",
      "Train Epoch: 77 [7200/8040 (90%)]\tLoss: 92.789046\n",
      "====> Epoch: 77 Average loss: 92.1544\n",
      "epoch: 77, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 78 [0/8040 (0%)]\tLoss: 92.435937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 78 [1200/8040 (15%)]\tLoss: 91.891496\n",
      "Train Epoch: 78 [2400/8040 (30%)]\tLoss: 91.865153\n",
      "Train Epoch: 78 [3600/8040 (45%)]\tLoss: 91.960889\n",
      "Train Epoch: 78 [4800/8040 (60%)]\tLoss: 92.735474\n",
      "Train Epoch: 78 [6000/8040 (75%)]\tLoss: 92.138574\n",
      "Train Epoch: 78 [7200/8040 (90%)]\tLoss: 92.257804\n",
      "====> Epoch: 78 Average loss: 92.1660\n",
      "epoch: 78, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 79 [0/8040 (0%)]\tLoss: 92.407227\n",
      "Train Epoch: 79 [1200/8040 (15%)]\tLoss: 91.769751\n",
      "Train Epoch: 79 [2400/8040 (30%)]\tLoss: 91.866919\n",
      "Train Epoch: 79 [3600/8040 (45%)]\tLoss: 91.876107\n",
      "Train Epoch: 79 [4800/8040 (60%)]\tLoss: 92.192277\n",
      "Train Epoch: 79 [6000/8040 (75%)]\tLoss: 92.691789\n",
      "Train Epoch: 79 [7200/8040 (90%)]\tLoss: 92.007153\n",
      "====> Epoch: 79 Average loss: 92.1851\n",
      "epoch: 79, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 80 [0/8040 (0%)]\tLoss: 91.784049\n",
      "Train Epoch: 80 [1200/8040 (15%)]\tLoss: 92.101571\n",
      "Train Epoch: 80 [2400/8040 (30%)]\tLoss: 91.821094\n",
      "Train Epoch: 80 [3600/8040 (45%)]\tLoss: 92.667367\n",
      "Train Epoch: 80 [4800/8040 (60%)]\tLoss: 92.035099\n",
      "Train Epoch: 80 [6000/8040 (75%)]\tLoss: 91.765177\n",
      "Train Epoch: 80 [7200/8040 (90%)]\tLoss: 92.054492\n",
      "====> Epoch: 80 Average loss: 92.1679\n",
      "epoch: 80, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 81 [0/8040 (0%)]\tLoss: 92.087972\n",
      "Train Epoch: 81 [1200/8040 (15%)]\tLoss: 91.655924\n",
      "Train Epoch: 81 [2400/8040 (30%)]\tLoss: 92.476213\n",
      "Train Epoch: 81 [3600/8040 (45%)]\tLoss: 92.074813\n",
      "Train Epoch: 81 [4800/8040 (60%)]\tLoss: 91.846248\n",
      "Train Epoch: 81 [6000/8040 (75%)]\tLoss: 92.442269\n",
      "Train Epoch: 81 [7200/8040 (90%)]\tLoss: 92.303084\n",
      "====> Epoch: 81 Average loss: 92.1644\n",
      "epoch: 81, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 82 [0/8040 (0%)]\tLoss: 92.581885\n",
      "Train Epoch: 82 [1200/8040 (15%)]\tLoss: 91.520020\n",
      "Train Epoch: 82 [2400/8040 (30%)]\tLoss: 92.348861\n",
      "Train Epoch: 82 [3600/8040 (45%)]\tLoss: 92.469857\n",
      "Train Epoch: 82 [4800/8040 (60%)]\tLoss: 91.564144\n",
      "Train Epoch: 82 [6000/8040 (75%)]\tLoss: 92.385905\n",
      "Train Epoch: 82 [7200/8040 (90%)]\tLoss: 92.861589\n",
      "====> Epoch: 82 Average loss: 92.1385\n",
      "epoch: 82, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 83 [0/8040 (0%)]\tLoss: 92.359920\n",
      "Train Epoch: 83 [1200/8040 (15%)]\tLoss: 91.590381\n",
      "Train Epoch: 83 [2400/8040 (30%)]\tLoss: 92.066732\n",
      "Train Epoch: 83 [3600/8040 (45%)]\tLoss: 92.558439\n",
      "Train Epoch: 83 [4800/8040 (60%)]\tLoss: 92.461540\n",
      "Train Epoch: 83 [6000/8040 (75%)]\tLoss: 92.702645\n",
      "Train Epoch: 83 [7200/8040 (90%)]\tLoss: 92.616121\n",
      "====> Epoch: 83 Average loss: 92.1485\n",
      "epoch: 83, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 84 [0/8040 (0%)]\tLoss: 92.001066\n",
      "Train Epoch: 84 [1200/8040 (15%)]\tLoss: 92.083301\n",
      "Train Epoch: 84 [2400/8040 (30%)]\tLoss: 92.408911\n",
      "Train Epoch: 84 [3600/8040 (45%)]\tLoss: 92.047843\n",
      "Train Epoch: 84 [4800/8040 (60%)]\tLoss: 92.257935\n",
      "Train Epoch: 84 [6000/8040 (75%)]\tLoss: 91.779134\n",
      "Train Epoch: 84 [7200/8040 (90%)]\tLoss: 92.059977\n",
      "====> Epoch: 84 Average loss: 92.1582\n",
      "epoch: 84, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 85 [0/8040 (0%)]\tLoss: 91.828263\n",
      "Train Epoch: 85 [1200/8040 (15%)]\tLoss: 91.934652\n",
      "Train Epoch: 85 [2400/8040 (30%)]\tLoss: 92.405444\n",
      "Train Epoch: 85 [3600/8040 (45%)]\tLoss: 91.926473\n",
      "Train Epoch: 85 [4800/8040 (60%)]\tLoss: 92.037142\n",
      "Train Epoch: 85 [6000/8040 (75%)]\tLoss: 92.254142\n",
      "Train Epoch: 85 [7200/8040 (90%)]\tLoss: 92.173942\n",
      "====> Epoch: 85 Average loss: 92.1555\n",
      "epoch: 85, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 86 [0/8040 (0%)]\tLoss: 92.209953\n",
      "Train Epoch: 86 [1200/8040 (15%)]\tLoss: 92.383952\n",
      "Train Epoch: 86 [2400/8040 (30%)]\tLoss: 91.782528\n",
      "Train Epoch: 86 [3600/8040 (45%)]\tLoss: 92.277620\n",
      "Train Epoch: 86 [4800/8040 (60%)]\tLoss: 92.015731\n",
      "Train Epoch: 86 [6000/8040 (75%)]\tLoss: 92.353516\n",
      "Train Epoch: 86 [7200/8040 (90%)]\tLoss: 92.043726\n",
      "====> Epoch: 86 Average loss: 92.1651\n",
      "epoch: 86, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 87 [0/8040 (0%)]\tLoss: 92.305941\n",
      "Train Epoch: 87 [1200/8040 (15%)]\tLoss: 91.850309\n",
      "Train Epoch: 87 [2400/8040 (30%)]\tLoss: 92.330664\n",
      "Train Epoch: 87 [3600/8040 (45%)]\tLoss: 92.118465\n",
      "Train Epoch: 87 [4800/8040 (60%)]\tLoss: 91.922689\n",
      "Train Epoch: 87 [6000/8040 (75%)]\tLoss: 91.704924\n",
      "Train Epoch: 87 [7200/8040 (90%)]\tLoss: 92.151742\n",
      "====> Epoch: 87 Average loss: 92.1639\n",
      "epoch: 87, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 88 [0/8040 (0%)]\tLoss: 91.765885\n",
      "Train Epoch: 88 [1200/8040 (15%)]\tLoss: 91.821452\n",
      "Train Epoch: 88 [2400/8040 (30%)]\tLoss: 92.079631\n",
      "Train Epoch: 88 [3600/8040 (45%)]\tLoss: 92.204842\n",
      "Train Epoch: 88 [4800/8040 (60%)]\tLoss: 91.481071\n",
      "Train Epoch: 88 [6000/8040 (75%)]\tLoss: 91.952734\n",
      "Train Epoch: 88 [7200/8040 (90%)]\tLoss: 92.220239\n",
      "====> Epoch: 88 Average loss: 92.1594\n",
      "epoch: 88, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 89 [0/8040 (0%)]\tLoss: 91.452588\n",
      "Train Epoch: 89 [1200/8040 (15%)]\tLoss: 91.577319\n",
      "Train Epoch: 89 [2400/8040 (30%)]\tLoss: 92.438070\n",
      "Train Epoch: 89 [3600/8040 (45%)]\tLoss: 91.740381\n",
      "Train Epoch: 89 [4800/8040 (60%)]\tLoss: 92.076807\n",
      "Train Epoch: 89 [6000/8040 (75%)]\tLoss: 92.388192\n",
      "Train Epoch: 89 [7200/8040 (90%)]\tLoss: 91.260970\n",
      "====> Epoch: 89 Average loss: 92.1397\n",
      "epoch: 89, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 90 [0/8040 (0%)]\tLoss: 91.790885\n",
      "Train Epoch: 90 [1200/8040 (15%)]\tLoss: 91.867155\n",
      "Train Epoch: 90 [2400/8040 (30%)]\tLoss: 91.918066\n",
      "Train Epoch: 90 [3600/8040 (45%)]\tLoss: 92.369442\n",
      "Train Epoch: 90 [4800/8040 (60%)]\tLoss: 92.216927\n",
      "Train Epoch: 90 [6000/8040 (75%)]\tLoss: 92.033341\n",
      "Train Epoch: 90 [7200/8040 (90%)]\tLoss: 91.869132\n",
      "====> Epoch: 90 Average loss: 92.1397\n",
      "epoch: 90, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 91 [0/8040 (0%)]\tLoss: 92.355827\n",
      "Train Epoch: 91 [1200/8040 (15%)]\tLoss: 91.975480\n",
      "Train Epoch: 91 [2400/8040 (30%)]\tLoss: 92.028166\n",
      "Train Epoch: 91 [3600/8040 (45%)]\tLoss: 91.808545\n",
      "Train Epoch: 91 [4800/8040 (60%)]\tLoss: 92.395687\n",
      "Train Epoch: 91 [6000/8040 (75%)]\tLoss: 92.697420\n",
      "Train Epoch: 91 [7200/8040 (90%)]\tLoss: 92.003874\n",
      "====> Epoch: 91 Average loss: 92.1644\n",
      "epoch: 91, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 92 [0/8040 (0%)]\tLoss: 91.950130\n",
      "Train Epoch: 92 [1200/8040 (15%)]\tLoss: 91.868384\n",
      "Train Epoch: 92 [2400/8040 (30%)]\tLoss: 92.610685\n",
      "Train Epoch: 92 [3600/8040 (45%)]\tLoss: 91.925586\n",
      "Train Epoch: 92 [4800/8040 (60%)]\tLoss: 92.615389\n",
      "Train Epoch: 92 [6000/8040 (75%)]\tLoss: 91.799951\n",
      "Train Epoch: 92 [7200/8040 (90%)]\tLoss: 92.050480\n",
      "====> Epoch: 92 Average loss: 92.1401\n",
      "epoch: 92, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 93 [0/8040 (0%)]\tLoss: 92.039258\n",
      "Train Epoch: 93 [1200/8040 (15%)]\tLoss: 92.491610\n",
      "Train Epoch: 93 [2400/8040 (30%)]\tLoss: 92.303394\n",
      "Train Epoch: 93 [3600/8040 (45%)]\tLoss: 92.406356\n",
      "Train Epoch: 93 [4800/8040 (60%)]\tLoss: 92.129264\n",
      "Train Epoch: 93 [6000/8040 (75%)]\tLoss: 91.255518\n",
      "Train Epoch: 93 [7200/8040 (90%)]\tLoss: 92.326937\n",
      "====> Epoch: 93 Average loss: 92.1479\n",
      "epoch: 93, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 94 [0/8040 (0%)]\tLoss: 92.200065\n",
      "Train Epoch: 94 [1200/8040 (15%)]\tLoss: 91.861263\n",
      "Train Epoch: 94 [2400/8040 (30%)]\tLoss: 92.392757\n",
      "Train Epoch: 94 [3600/8040 (45%)]\tLoss: 91.772331\n",
      "Train Epoch: 94 [4800/8040 (60%)]\tLoss: 92.175594\n",
      "Train Epoch: 94 [6000/8040 (75%)]\tLoss: 91.582764\n",
      "Train Epoch: 94 [7200/8040 (90%)]\tLoss: 91.930200\n",
      "====> Epoch: 94 Average loss: 92.1656\n",
      "epoch: 94, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 95 [0/8040 (0%)]\tLoss: 92.388534\n",
      "Train Epoch: 95 [1200/8040 (15%)]\tLoss: 92.325684\n",
      "Train Epoch: 95 [2400/8040 (30%)]\tLoss: 91.691138\n",
      "Train Epoch: 95 [3600/8040 (45%)]\tLoss: 92.144808\n",
      "Train Epoch: 95 [4800/8040 (60%)]\tLoss: 91.982503\n",
      "Train Epoch: 95 [6000/8040 (75%)]\tLoss: 91.914478\n",
      "Train Epoch: 95 [7200/8040 (90%)]\tLoss: 92.215104\n",
      "====> Epoch: 95 Average loss: 92.1485\n",
      "epoch: 95, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 96 [0/8040 (0%)]\tLoss: 92.053320\n",
      "Train Epoch: 96 [1200/8040 (15%)]\tLoss: 92.038501\n",
      "Train Epoch: 96 [2400/8040 (30%)]\tLoss: 92.268921\n",
      "Train Epoch: 96 [3600/8040 (45%)]\tLoss: 92.214404\n",
      "Train Epoch: 96 [4800/8040 (60%)]\tLoss: 91.836279\n",
      "Train Epoch: 96 [6000/8040 (75%)]\tLoss: 92.072835\n",
      "Train Epoch: 96 [7200/8040 (90%)]\tLoss: 92.325985\n",
      "====> Epoch: 96 Average loss: 92.1157\n",
      "epoch: 96, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 97 [0/8040 (0%)]\tLoss: 91.934814\n",
      "Train Epoch: 97 [1200/8040 (15%)]\tLoss: 92.189160\n",
      "Train Epoch: 97 [2400/8040 (30%)]\tLoss: 91.896313\n",
      "Train Epoch: 97 [3600/8040 (45%)]\tLoss: 92.228109\n",
      "Train Epoch: 97 [4800/8040 (60%)]\tLoss: 91.691927\n",
      "Train Epoch: 97 [6000/8040 (75%)]\tLoss: 92.440576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 97 [7200/8040 (90%)]\tLoss: 92.074674\n",
      "====> Epoch: 97 Average loss: 92.1456\n",
      "epoch: 97, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 98 [0/8040 (0%)]\tLoss: 92.676758\n",
      "Train Epoch: 98 [1200/8040 (15%)]\tLoss: 92.345337\n",
      "Train Epoch: 98 [2400/8040 (30%)]\tLoss: 92.188656\n",
      "Train Epoch: 98 [3600/8040 (45%)]\tLoss: 91.976742\n",
      "Train Epoch: 98 [4800/8040 (60%)]\tLoss: 92.150326\n",
      "Train Epoch: 98 [6000/8040 (75%)]\tLoss: 92.078052\n",
      "Train Epoch: 98 [7200/8040 (90%)]\tLoss: 92.002661\n",
      "====> Epoch: 98 Average loss: 92.1444\n",
      "epoch: 98, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 99 [0/8040 (0%)]\tLoss: 92.187126\n",
      "Train Epoch: 99 [1200/8040 (15%)]\tLoss: 91.927604\n",
      "Train Epoch: 99 [2400/8040 (30%)]\tLoss: 92.114185\n",
      "Train Epoch: 99 [3600/8040 (45%)]\tLoss: 91.908398\n",
      "Train Epoch: 99 [4800/8040 (60%)]\tLoss: 92.083415\n",
      "Train Epoch: 99 [6000/8040 (75%)]\tLoss: 92.041333\n",
      "Train Epoch: 99 [7200/8040 (90%)]\tLoss: 91.917537\n",
      "====> Epoch: 99 Average loss: 92.1500\n",
      "epoch: 99, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 100 [0/8040 (0%)]\tLoss: 92.201733\n",
      "Train Epoch: 100 [1200/8040 (15%)]\tLoss: 92.056657\n",
      "Train Epoch: 100 [2400/8040 (30%)]\tLoss: 91.730908\n",
      "Train Epoch: 100 [3600/8040 (45%)]\tLoss: 92.226221\n",
      "Train Epoch: 100 [4800/8040 (60%)]\tLoss: 92.403247\n",
      "Train Epoch: 100 [6000/8040 (75%)]\tLoss: 92.386165\n",
      "Train Epoch: 100 [7200/8040 (90%)]\tLoss: 92.467847\n",
      "====> Epoch: 100 Average loss: 92.1376\n",
      "epoch: 100, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 101 [0/8040 (0%)]\tLoss: 92.519198\n",
      "Train Epoch: 101 [1200/8040 (15%)]\tLoss: 92.048389\n",
      "Train Epoch: 101 [2400/8040 (30%)]\tLoss: 92.406877\n",
      "Train Epoch: 101 [3600/8040 (45%)]\tLoss: 92.283781\n",
      "Train Epoch: 101 [4800/8040 (60%)]\tLoss: 92.110164\n",
      "Train Epoch: 101 [6000/8040 (75%)]\tLoss: 91.710197\n",
      "Train Epoch: 101 [7200/8040 (90%)]\tLoss: 92.111165\n",
      "====> Epoch: 101 Average loss: 92.1480\n",
      "epoch: 101, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 102 [0/8040 (0%)]\tLoss: 91.724105\n",
      "Train Epoch: 102 [1200/8040 (15%)]\tLoss: 92.478727\n",
      "Train Epoch: 102 [2400/8040 (30%)]\tLoss: 92.187899\n",
      "Train Epoch: 102 [3600/8040 (45%)]\tLoss: 92.389315\n",
      "Train Epoch: 102 [4800/8040 (60%)]\tLoss: 92.165023\n",
      "Train Epoch: 102 [6000/8040 (75%)]\tLoss: 91.581380\n",
      "Train Epoch: 102 [7200/8040 (90%)]\tLoss: 92.218742\n",
      "====> Epoch: 102 Average loss: 92.1525\n",
      "epoch: 102, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 103 [0/8040 (0%)]\tLoss: 92.254590\n",
      "Train Epoch: 103 [1200/8040 (15%)]\tLoss: 92.232015\n",
      "Train Epoch: 103 [2400/8040 (30%)]\tLoss: 92.190145\n",
      "Train Epoch: 103 [3600/8040 (45%)]\tLoss: 92.250822\n",
      "Train Epoch: 103 [4800/8040 (60%)]\tLoss: 92.429801\n",
      "Train Epoch: 103 [6000/8040 (75%)]\tLoss: 91.887606\n",
      "Train Epoch: 103 [7200/8040 (90%)]\tLoss: 92.308960\n",
      "====> Epoch: 103 Average loss: 92.1501\n",
      "epoch: 103, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 104 [0/8040 (0%)]\tLoss: 92.684749\n",
      "Train Epoch: 104 [1200/8040 (15%)]\tLoss: 92.142342\n",
      "Train Epoch: 104 [2400/8040 (30%)]\tLoss: 92.223356\n",
      "Train Epoch: 104 [3600/8040 (45%)]\tLoss: 92.277295\n",
      "Train Epoch: 104 [4800/8040 (60%)]\tLoss: 91.996224\n",
      "Train Epoch: 104 [6000/8040 (75%)]\tLoss: 92.506120\n",
      "Train Epoch: 104 [7200/8040 (90%)]\tLoss: 91.947152\n",
      "====> Epoch: 104 Average loss: 92.1402\n",
      "epoch: 104, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 105 [0/8040 (0%)]\tLoss: 92.412142\n",
      "Train Epoch: 105 [1200/8040 (15%)]\tLoss: 92.008529\n",
      "Train Epoch: 105 [2400/8040 (30%)]\tLoss: 92.183049\n",
      "Train Epoch: 105 [3600/8040 (45%)]\tLoss: 91.736035\n",
      "Train Epoch: 105 [4800/8040 (60%)]\tLoss: 91.929346\n",
      "Train Epoch: 105 [6000/8040 (75%)]\tLoss: 92.365137\n",
      "Train Epoch: 105 [7200/8040 (90%)]\tLoss: 92.784611\n",
      "====> Epoch: 105 Average loss: 92.1202\n",
      "epoch: 105, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 106 [0/8040 (0%)]\tLoss: 92.413525\n",
      "Train Epoch: 106 [1200/8040 (15%)]\tLoss: 92.184554\n",
      "Train Epoch: 106 [2400/8040 (30%)]\tLoss: 92.361076\n",
      "Train Epoch: 106 [3600/8040 (45%)]\tLoss: 91.776994\n",
      "Train Epoch: 106 [4800/8040 (60%)]\tLoss: 92.144002\n",
      "Train Epoch: 106 [6000/8040 (75%)]\tLoss: 92.408643\n",
      "Train Epoch: 106 [7200/8040 (90%)]\tLoss: 92.369971\n",
      "====> Epoch: 106 Average loss: 92.1318\n",
      "epoch: 106, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 107 [0/8040 (0%)]\tLoss: 91.973934\n",
      "Train Epoch: 107 [1200/8040 (15%)]\tLoss: 92.079793\n",
      "Train Epoch: 107 [2400/8040 (30%)]\tLoss: 92.221362\n",
      "Train Epoch: 107 [3600/8040 (45%)]\tLoss: 92.159798\n",
      "Train Epoch: 107 [4800/8040 (60%)]\tLoss: 91.856462\n",
      "Train Epoch: 107 [6000/8040 (75%)]\tLoss: 92.407186\n",
      "Train Epoch: 107 [7200/8040 (90%)]\tLoss: 92.495874\n",
      "====> Epoch: 107 Average loss: 92.1243\n",
      "epoch: 107, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 108 [0/8040 (0%)]\tLoss: 91.821265\n",
      "Train Epoch: 108 [1200/8040 (15%)]\tLoss: 92.073250\n",
      "Train Epoch: 108 [2400/8040 (30%)]\tLoss: 92.259814\n",
      "Train Epoch: 108 [3600/8040 (45%)]\tLoss: 92.017806\n",
      "Train Epoch: 108 [4800/8040 (60%)]\tLoss: 91.988981\n",
      "Train Epoch: 108 [6000/8040 (75%)]\tLoss: 92.174520\n",
      "Train Epoch: 108 [7200/8040 (90%)]\tLoss: 92.312427\n",
      "====> Epoch: 108 Average loss: 92.1272\n",
      "epoch: 108, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 109 [0/8040 (0%)]\tLoss: 92.566252\n",
      "Train Epoch: 109 [1200/8040 (15%)]\tLoss: 92.561515\n",
      "Train Epoch: 109 [2400/8040 (30%)]\tLoss: 92.497371\n",
      "Train Epoch: 109 [3600/8040 (45%)]\tLoss: 91.863721\n",
      "Train Epoch: 109 [4800/8040 (60%)]\tLoss: 92.146427\n",
      "Train Epoch: 109 [6000/8040 (75%)]\tLoss: 92.506510\n",
      "Train Epoch: 109 [7200/8040 (90%)]\tLoss: 91.819279\n",
      "====> Epoch: 109 Average loss: 92.1454\n",
      "epoch: 109, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 110 [0/8040 (0%)]\tLoss: 91.976725\n",
      "Train Epoch: 110 [1200/8040 (15%)]\tLoss: 92.683301\n",
      "Train Epoch: 110 [2400/8040 (30%)]\tLoss: 91.895353\n",
      "Train Epoch: 110 [3600/8040 (45%)]\tLoss: 92.159798\n",
      "Train Epoch: 110 [4800/8040 (60%)]\tLoss: 91.859074\n",
      "Train Epoch: 110 [6000/8040 (75%)]\tLoss: 92.299194\n",
      "Train Epoch: 110 [7200/8040 (90%)]\tLoss: 92.123267\n",
      "====> Epoch: 110 Average loss: 92.1366\n",
      "epoch: 110, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 111 [0/8040 (0%)]\tLoss: 92.559774\n",
      "Train Epoch: 111 [1200/8040 (15%)]\tLoss: 92.140088\n",
      "Train Epoch: 111 [2400/8040 (30%)]\tLoss: 92.683984\n",
      "Train Epoch: 111 [3600/8040 (45%)]\tLoss: 91.962834\n",
      "Train Epoch: 111 [4800/8040 (60%)]\tLoss: 92.124935\n",
      "Train Epoch: 111 [6000/8040 (75%)]\tLoss: 92.204720\n",
      "Train Epoch: 111 [7200/8040 (90%)]\tLoss: 91.564193\n",
      "====> Epoch: 111 Average loss: 92.1279\n",
      "epoch: 111, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 112 [0/8040 (0%)]\tLoss: 92.258081\n",
      "Train Epoch: 112 [1200/8040 (15%)]\tLoss: 92.288582\n",
      "Train Epoch: 112 [2400/8040 (30%)]\tLoss: 92.184106\n",
      "Train Epoch: 112 [3600/8040 (45%)]\tLoss: 91.852002\n",
      "Train Epoch: 112 [4800/8040 (60%)]\tLoss: 92.392912\n",
      "Train Epoch: 112 [6000/8040 (75%)]\tLoss: 91.901619\n",
      "Train Epoch: 112 [7200/8040 (90%)]\tLoss: 91.595199\n",
      "====> Epoch: 112 Average loss: 92.1238\n",
      "epoch: 112, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 113 [0/8040 (0%)]\tLoss: 92.361548\n",
      "Train Epoch: 113 [1200/8040 (15%)]\tLoss: 92.231250\n",
      "Train Epoch: 113 [2400/8040 (30%)]\tLoss: 92.545410\n",
      "Train Epoch: 113 [3600/8040 (45%)]\tLoss: 91.820386\n",
      "Train Epoch: 113 [4800/8040 (60%)]\tLoss: 91.663729\n",
      "Train Epoch: 113 [6000/8040 (75%)]\tLoss: 92.291260\n",
      "Train Epoch: 113 [7200/8040 (90%)]\tLoss: 92.473356\n",
      "====> Epoch: 113 Average loss: 92.1401\n",
      "epoch: 113, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 114 [0/8040 (0%)]\tLoss: 92.048950\n",
      "Train Epoch: 114 [1200/8040 (15%)]\tLoss: 92.147843\n",
      "Train Epoch: 114 [2400/8040 (30%)]\tLoss: 92.510409\n",
      "Train Epoch: 114 [3600/8040 (45%)]\tLoss: 92.221802\n",
      "Train Epoch: 114 [4800/8040 (60%)]\tLoss: 92.526929\n",
      "Train Epoch: 114 [6000/8040 (75%)]\tLoss: 92.109285\n",
      "Train Epoch: 114 [7200/8040 (90%)]\tLoss: 92.076245\n",
      "====> Epoch: 114 Average loss: 92.1477\n",
      "epoch: 114, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 115 [0/8040 (0%)]\tLoss: 91.887345\n",
      "Train Epoch: 115 [1200/8040 (15%)]\tLoss: 92.150505\n",
      "Train Epoch: 115 [2400/8040 (30%)]\tLoss: 91.531820\n",
      "Train Epoch: 115 [3600/8040 (45%)]\tLoss: 92.047111\n",
      "Train Epoch: 115 [4800/8040 (60%)]\tLoss: 92.034920\n",
      "Train Epoch: 115 [6000/8040 (75%)]\tLoss: 92.435636\n",
      "Train Epoch: 115 [7200/8040 (90%)]\tLoss: 91.813452\n",
      "====> Epoch: 115 Average loss: 92.1350\n",
      "epoch: 115, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 116 [0/8040 (0%)]\tLoss: 92.204997\n",
      "Train Epoch: 116 [1200/8040 (15%)]\tLoss: 92.463306\n",
      "Train Epoch: 116 [2400/8040 (30%)]\tLoss: 91.866764\n",
      "Train Epoch: 116 [3600/8040 (45%)]\tLoss: 91.700098\n",
      "Train Epoch: 116 [4800/8040 (60%)]\tLoss: 92.049666\n",
      "Train Epoch: 116 [6000/8040 (75%)]\tLoss: 92.033887\n",
      "Train Epoch: 116 [7200/8040 (90%)]\tLoss: 91.998755\n",
      "====> Epoch: 116 Average loss: 92.1409\n",
      "epoch: 116, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 117 [0/8040 (0%)]\tLoss: 91.842798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 117 [1200/8040 (15%)]\tLoss: 92.115479\n",
      "Train Epoch: 117 [2400/8040 (30%)]\tLoss: 91.862297\n",
      "Train Epoch: 117 [3600/8040 (45%)]\tLoss: 92.111507\n",
      "Train Epoch: 117 [4800/8040 (60%)]\tLoss: 92.253923\n",
      "Train Epoch: 117 [6000/8040 (75%)]\tLoss: 92.051978\n",
      "Train Epoch: 117 [7200/8040 (90%)]\tLoss: 91.694466\n",
      "====> Epoch: 117 Average loss: 92.1522\n",
      "epoch: 117, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 118 [0/8040 (0%)]\tLoss: 92.308073\n",
      "Train Epoch: 118 [1200/8040 (15%)]\tLoss: 91.894067\n",
      "Train Epoch: 118 [2400/8040 (30%)]\tLoss: 92.427743\n",
      "Train Epoch: 118 [3600/8040 (45%)]\tLoss: 92.092261\n",
      "Train Epoch: 118 [4800/8040 (60%)]\tLoss: 92.052612\n",
      "Train Epoch: 118 [6000/8040 (75%)]\tLoss: 92.519840\n",
      "Train Epoch: 118 [7200/8040 (90%)]\tLoss: 92.657723\n",
      "====> Epoch: 118 Average loss: 92.1154\n",
      "epoch: 118, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 119 [0/8040 (0%)]\tLoss: 92.375773\n",
      "Train Epoch: 119 [1200/8040 (15%)]\tLoss: 92.191838\n",
      "Train Epoch: 119 [2400/8040 (30%)]\tLoss: 92.234188\n",
      "Train Epoch: 119 [3600/8040 (45%)]\tLoss: 92.337557\n",
      "Train Epoch: 119 [4800/8040 (60%)]\tLoss: 92.351375\n",
      "Train Epoch: 119 [6000/8040 (75%)]\tLoss: 92.305884\n",
      "Train Epoch: 119 [7200/8040 (90%)]\tLoss: 91.969385\n",
      "====> Epoch: 119 Average loss: 92.1318\n",
      "epoch: 119, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 120 [0/8040 (0%)]\tLoss: 91.985612\n",
      "Train Epoch: 120 [1200/8040 (15%)]\tLoss: 91.974943\n",
      "Train Epoch: 120 [2400/8040 (30%)]\tLoss: 92.106641\n",
      "Train Epoch: 120 [3600/8040 (45%)]\tLoss: 92.131055\n",
      "Train Epoch: 120 [4800/8040 (60%)]\tLoss: 92.315430\n",
      "Train Epoch: 120 [6000/8040 (75%)]\tLoss: 92.300675\n",
      "Train Epoch: 120 [7200/8040 (90%)]\tLoss: 92.561873\n",
      "====> Epoch: 120 Average loss: 92.1504\n",
      "epoch: 120, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 121 [0/8040 (0%)]\tLoss: 91.295638\n",
      "Train Epoch: 121 [1200/8040 (15%)]\tLoss: 91.720174\n",
      "Train Epoch: 121 [2400/8040 (30%)]\tLoss: 91.597201\n",
      "Train Epoch: 121 [3600/8040 (45%)]\tLoss: 92.376595\n",
      "Train Epoch: 121 [4800/8040 (60%)]\tLoss: 92.101839\n",
      "Train Epoch: 121 [6000/8040 (75%)]\tLoss: 91.762557\n",
      "Train Epoch: 121 [7200/8040 (90%)]\tLoss: 91.807414\n",
      "====> Epoch: 121 Average loss: 92.1100\n",
      "epoch: 121, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 122 [0/8040 (0%)]\tLoss: 92.056445\n",
      "Train Epoch: 122 [1200/8040 (15%)]\tLoss: 92.129012\n",
      "Train Epoch: 122 [2400/8040 (30%)]\tLoss: 91.745435\n",
      "Train Epoch: 122 [3600/8040 (45%)]\tLoss: 92.187679\n",
      "Train Epoch: 122 [4800/8040 (60%)]\tLoss: 92.549162\n",
      "Train Epoch: 122 [6000/8040 (75%)]\tLoss: 92.456592\n",
      "Train Epoch: 122 [7200/8040 (90%)]\tLoss: 92.391496\n",
      "====> Epoch: 122 Average loss: 92.1390\n",
      "epoch: 122, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 123 [0/8040 (0%)]\tLoss: 92.361410\n",
      "Train Epoch: 123 [1200/8040 (15%)]\tLoss: 91.405412\n",
      "Train Epoch: 123 [2400/8040 (30%)]\tLoss: 91.618115\n",
      "Train Epoch: 123 [3600/8040 (45%)]\tLoss: 92.505176\n",
      "Train Epoch: 123 [4800/8040 (60%)]\tLoss: 91.779468\n",
      "Train Epoch: 123 [6000/8040 (75%)]\tLoss: 91.711621\n",
      "Train Epoch: 123 [7200/8040 (90%)]\tLoss: 92.359017\n",
      "====> Epoch: 123 Average loss: 92.1356\n",
      "epoch: 123, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 124 [0/8040 (0%)]\tLoss: 91.872021\n",
      "Train Epoch: 124 [1200/8040 (15%)]\tLoss: 92.561165\n",
      "Train Epoch: 124 [2400/8040 (30%)]\tLoss: 91.791935\n",
      "Train Epoch: 124 [3600/8040 (45%)]\tLoss: 91.485905\n",
      "Train Epoch: 124 [4800/8040 (60%)]\tLoss: 92.098226\n",
      "Train Epoch: 124 [6000/8040 (75%)]\tLoss: 92.078402\n",
      "Train Epoch: 124 [7200/8040 (90%)]\tLoss: 91.692163\n",
      "====> Epoch: 124 Average loss: 92.1270\n",
      "epoch: 124, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 125 [0/8040 (0%)]\tLoss: 92.718986\n",
      "Train Epoch: 125 [1200/8040 (15%)]\tLoss: 91.722860\n",
      "Train Epoch: 125 [2400/8040 (30%)]\tLoss: 91.559310\n",
      "Train Epoch: 125 [3600/8040 (45%)]\tLoss: 92.083154\n",
      "Train Epoch: 125 [4800/8040 (60%)]\tLoss: 92.179435\n",
      "Train Epoch: 125 [6000/8040 (75%)]\tLoss: 92.102222\n",
      "Train Epoch: 125 [7200/8040 (90%)]\tLoss: 92.004614\n",
      "====> Epoch: 125 Average loss: 92.1265\n",
      "epoch: 125, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 126 [0/8040 (0%)]\tLoss: 91.956388\n",
      "Train Epoch: 126 [1200/8040 (15%)]\tLoss: 91.835506\n",
      "Train Epoch: 126 [2400/8040 (30%)]\tLoss: 92.006958\n",
      "Train Epoch: 126 [3600/8040 (45%)]\tLoss: 92.261336\n",
      "Train Epoch: 126 [4800/8040 (60%)]\tLoss: 92.378988\n",
      "Train Epoch: 126 [6000/8040 (75%)]\tLoss: 92.108285\n",
      "Train Epoch: 126 [7200/8040 (90%)]\tLoss: 91.849715\n",
      "====> Epoch: 126 Average loss: 92.1384\n",
      "epoch: 126, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 127 [0/8040 (0%)]\tLoss: 91.849609\n",
      "Train Epoch: 127 [1200/8040 (15%)]\tLoss: 92.415706\n",
      "Train Epoch: 127 [2400/8040 (30%)]\tLoss: 92.374032\n",
      "Train Epoch: 127 [3600/8040 (45%)]\tLoss: 92.285156\n",
      "Train Epoch: 127 [4800/8040 (60%)]\tLoss: 92.062777\n",
      "Train Epoch: 127 [6000/8040 (75%)]\tLoss: 91.978695\n",
      "Train Epoch: 127 [7200/8040 (90%)]\tLoss: 92.300472\n",
      "====> Epoch: 127 Average loss: 92.1406\n",
      "epoch: 127, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 128 [0/8040 (0%)]\tLoss: 92.424438\n",
      "Train Epoch: 128 [1200/8040 (15%)]\tLoss: 92.605656\n",
      "Train Epoch: 128 [2400/8040 (30%)]\tLoss: 91.359790\n",
      "Train Epoch: 128 [3600/8040 (45%)]\tLoss: 91.795980\n",
      "Train Epoch: 128 [4800/8040 (60%)]\tLoss: 91.990690\n",
      "Train Epoch: 128 [6000/8040 (75%)]\tLoss: 92.527759\n",
      "Train Epoch: 128 [7200/8040 (90%)]\tLoss: 92.328776\n",
      "====> Epoch: 128 Average loss: 92.1258\n",
      "epoch: 128, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 129 [0/8040 (0%)]\tLoss: 92.013118\n",
      "Train Epoch: 129 [1200/8040 (15%)]\tLoss: 91.906510\n",
      "Train Epoch: 129 [2400/8040 (30%)]\tLoss: 91.834823\n",
      "Train Epoch: 129 [3600/8040 (45%)]\tLoss: 92.291463\n",
      "Train Epoch: 129 [4800/8040 (60%)]\tLoss: 92.381567\n",
      "Train Epoch: 129 [6000/8040 (75%)]\tLoss: 92.057284\n",
      "Train Epoch: 129 [7200/8040 (90%)]\tLoss: 91.994092\n",
      "====> Epoch: 129 Average loss: 92.1437\n",
      "epoch: 129, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 130 [0/8040 (0%)]\tLoss: 92.169425\n",
      "Train Epoch: 130 [1200/8040 (15%)]\tLoss: 91.874211\n",
      "Train Epoch: 130 [2400/8040 (30%)]\tLoss: 92.116423\n",
      "Train Epoch: 130 [3600/8040 (45%)]\tLoss: 92.318449\n",
      "Train Epoch: 130 [4800/8040 (60%)]\tLoss: 92.469084\n",
      "Train Epoch: 130 [6000/8040 (75%)]\tLoss: 92.214429\n",
      "Train Epoch: 130 [7200/8040 (90%)]\tLoss: 92.144043\n",
      "====> Epoch: 130 Average loss: 92.1430\n",
      "epoch: 130, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 131 [0/8040 (0%)]\tLoss: 91.652417\n",
      "Train Epoch: 131 [1200/8040 (15%)]\tLoss: 92.168107\n",
      "Train Epoch: 131 [2400/8040 (30%)]\tLoss: 92.426156\n",
      "Train Epoch: 131 [3600/8040 (45%)]\tLoss: 92.421696\n",
      "Train Epoch: 131 [4800/8040 (60%)]\tLoss: 91.856510\n",
      "Train Epoch: 131 [6000/8040 (75%)]\tLoss: 91.815763\n",
      "Train Epoch: 131 [7200/8040 (90%)]\tLoss: 92.454370\n",
      "====> Epoch: 131 Average loss: 92.1366\n",
      "epoch: 131, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 132 [0/8040 (0%)]\tLoss: 92.249495\n",
      "Train Epoch: 132 [1200/8040 (15%)]\tLoss: 92.093791\n",
      "Train Epoch: 132 [2400/8040 (30%)]\tLoss: 91.893473\n",
      "Train Epoch: 132 [3600/8040 (45%)]\tLoss: 92.553695\n",
      "Train Epoch: 132 [4800/8040 (60%)]\tLoss: 92.252360\n",
      "Train Epoch: 132 [6000/8040 (75%)]\tLoss: 92.077954\n",
      "Train Epoch: 132 [7200/8040 (90%)]\tLoss: 92.258691\n",
      "====> Epoch: 132 Average loss: 92.1326\n",
      "epoch: 132, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 133 [0/8040 (0%)]\tLoss: 92.083667\n",
      "Train Epoch: 133 [1200/8040 (15%)]\tLoss: 91.781934\n",
      "Train Epoch: 133 [2400/8040 (30%)]\tLoss: 92.073364\n",
      "Train Epoch: 133 [3600/8040 (45%)]\tLoss: 92.387988\n",
      "Train Epoch: 133 [4800/8040 (60%)]\tLoss: 91.898291\n",
      "Train Epoch: 133 [6000/8040 (75%)]\tLoss: 92.323600\n",
      "Train Epoch: 133 [7200/8040 (90%)]\tLoss: 91.848275\n",
      "====> Epoch: 133 Average loss: 92.1265\n",
      "epoch: 133, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 134 [0/8040 (0%)]\tLoss: 92.519678\n",
      "Train Epoch: 134 [1200/8040 (15%)]\tLoss: 92.270500\n",
      "Train Epoch: 134 [2400/8040 (30%)]\tLoss: 91.785742\n",
      "Train Epoch: 134 [3600/8040 (45%)]\tLoss: 92.183976\n",
      "Train Epoch: 134 [4800/8040 (60%)]\tLoss: 91.777783\n",
      "Train Epoch: 134 [6000/8040 (75%)]\tLoss: 91.562801\n",
      "Train Epoch: 134 [7200/8040 (90%)]\tLoss: 92.376929\n",
      "====> Epoch: 134 Average loss: 92.1289\n",
      "epoch: 134, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 135 [0/8040 (0%)]\tLoss: 92.366895\n",
      "Train Epoch: 135 [1200/8040 (15%)]\tLoss: 91.984294\n",
      "Train Epoch: 135 [2400/8040 (30%)]\tLoss: 92.495459\n",
      "Train Epoch: 135 [3600/8040 (45%)]\tLoss: 92.300163\n",
      "Train Epoch: 135 [4800/8040 (60%)]\tLoss: 92.366097\n",
      "Train Epoch: 135 [6000/8040 (75%)]\tLoss: 91.790617\n",
      "Train Epoch: 135 [7200/8040 (90%)]\tLoss: 92.135262\n",
      "====> Epoch: 135 Average loss: 92.1385\n",
      "epoch: 135, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 136 [0/8040 (0%)]\tLoss: 92.741016\n",
      "Train Epoch: 136 [1200/8040 (15%)]\tLoss: 91.938940\n",
      "Train Epoch: 136 [2400/8040 (30%)]\tLoss: 92.051188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 136 [3600/8040 (45%)]\tLoss: 91.876172\n",
      "Train Epoch: 136 [4800/8040 (60%)]\tLoss: 92.037134\n",
      "Train Epoch: 136 [6000/8040 (75%)]\tLoss: 92.504403\n",
      "Train Epoch: 136 [7200/8040 (90%)]\tLoss: 92.514421\n",
      "====> Epoch: 136 Average loss: 92.1149\n",
      "epoch: 136, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 137 [0/8040 (0%)]\tLoss: 92.434725\n",
      "Train Epoch: 137 [1200/8040 (15%)]\tLoss: 91.913118\n",
      "Train Epoch: 137 [2400/8040 (30%)]\tLoss: 91.911678\n",
      "Train Epoch: 137 [3600/8040 (45%)]\tLoss: 92.505037\n",
      "Train Epoch: 137 [4800/8040 (60%)]\tLoss: 92.600627\n",
      "Train Epoch: 137 [6000/8040 (75%)]\tLoss: 92.207878\n",
      "Train Epoch: 137 [7200/8040 (90%)]\tLoss: 92.021281\n",
      "====> Epoch: 137 Average loss: 92.1226\n",
      "epoch: 137, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 138 [0/8040 (0%)]\tLoss: 92.286011\n",
      "Train Epoch: 138 [1200/8040 (15%)]\tLoss: 92.562492\n",
      "Train Epoch: 138 [2400/8040 (30%)]\tLoss: 92.190747\n",
      "Train Epoch: 138 [3600/8040 (45%)]\tLoss: 92.160083\n",
      "Train Epoch: 138 [4800/8040 (60%)]\tLoss: 92.013118\n",
      "Train Epoch: 138 [6000/8040 (75%)]\tLoss: 92.048389\n",
      "Train Epoch: 138 [7200/8040 (90%)]\tLoss: 92.280892\n",
      "====> Epoch: 138 Average loss: 92.1337\n",
      "epoch: 138, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 139 [0/8040 (0%)]\tLoss: 91.913908\n",
      "Train Epoch: 139 [1200/8040 (15%)]\tLoss: 92.021370\n",
      "Train Epoch: 139 [2400/8040 (30%)]\tLoss: 91.944417\n",
      "Train Epoch: 139 [3600/8040 (45%)]\tLoss: 92.096460\n",
      "Train Epoch: 139 [4800/8040 (60%)]\tLoss: 92.286637\n",
      "Train Epoch: 139 [6000/8040 (75%)]\tLoss: 92.090763\n",
      "Train Epoch: 139 [7200/8040 (90%)]\tLoss: 91.824764\n",
      "====> Epoch: 139 Average loss: 92.1385\n",
      "epoch: 139, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 140 [0/8040 (0%)]\tLoss: 92.400952\n",
      "Train Epoch: 140 [1200/8040 (15%)]\tLoss: 92.609619\n",
      "Train Epoch: 140 [2400/8040 (30%)]\tLoss: 92.096696\n",
      "Train Epoch: 140 [3600/8040 (45%)]\tLoss: 92.070020\n",
      "Train Epoch: 140 [4800/8040 (60%)]\tLoss: 92.834928\n",
      "Train Epoch: 140 [6000/8040 (75%)]\tLoss: 92.503581\n",
      "Train Epoch: 140 [7200/8040 (90%)]\tLoss: 92.198022\n",
      "====> Epoch: 140 Average loss: 92.1429\n",
      "epoch: 140, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 141 [0/8040 (0%)]\tLoss: 92.005705\n",
      "Train Epoch: 141 [1200/8040 (15%)]\tLoss: 92.103434\n",
      "Train Epoch: 141 [2400/8040 (30%)]\tLoss: 92.038314\n",
      "Train Epoch: 141 [3600/8040 (45%)]\tLoss: 92.560970\n",
      "Train Epoch: 141 [4800/8040 (60%)]\tLoss: 91.986279\n",
      "Train Epoch: 141 [6000/8040 (75%)]\tLoss: 92.101774\n",
      "Train Epoch: 141 [7200/8040 (90%)]\tLoss: 91.794141\n",
      "====> Epoch: 141 Average loss: 92.1300\n",
      "epoch: 141, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 142 [0/8040 (0%)]\tLoss: 92.223527\n",
      "Train Epoch: 142 [1200/8040 (15%)]\tLoss: 92.460400\n",
      "Train Epoch: 142 [2400/8040 (30%)]\tLoss: 92.089779\n",
      "Train Epoch: 142 [3600/8040 (45%)]\tLoss: 92.018953\n",
      "Train Epoch: 142 [4800/8040 (60%)]\tLoss: 92.100594\n",
      "Train Epoch: 142 [6000/8040 (75%)]\tLoss: 92.075724\n",
      "Train Epoch: 142 [7200/8040 (90%)]\tLoss: 92.460522\n",
      "====> Epoch: 142 Average loss: 92.1340\n",
      "epoch: 142, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 143 [0/8040 (0%)]\tLoss: 91.593465\n",
      "Train Epoch: 143 [1200/8040 (15%)]\tLoss: 92.389128\n",
      "Train Epoch: 143 [2400/8040 (30%)]\tLoss: 91.700317\n",
      "Train Epoch: 143 [3600/8040 (45%)]\tLoss: 92.036572\n",
      "Train Epoch: 143 [4800/8040 (60%)]\tLoss: 92.463078\n",
      "Train Epoch: 143 [6000/8040 (75%)]\tLoss: 92.098421\n",
      "Train Epoch: 143 [7200/8040 (90%)]\tLoss: 92.544832\n",
      "====> Epoch: 143 Average loss: 92.1058\n",
      "epoch: 143, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 144 [0/8040 (0%)]\tLoss: 92.002563\n",
      "Train Epoch: 144 [1200/8040 (15%)]\tLoss: 92.451042\n",
      "Train Epoch: 144 [2400/8040 (30%)]\tLoss: 92.285807\n",
      "Train Epoch: 144 [3600/8040 (45%)]\tLoss: 92.284847\n",
      "Train Epoch: 144 [4800/8040 (60%)]\tLoss: 91.927677\n",
      "Train Epoch: 144 [6000/8040 (75%)]\tLoss: 92.249390\n",
      "Train Epoch: 144 [7200/8040 (90%)]\tLoss: 92.304435\n",
      "====> Epoch: 144 Average loss: 92.1356\n",
      "epoch: 144, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 145 [0/8040 (0%)]\tLoss: 91.742668\n",
      "Train Epoch: 145 [1200/8040 (15%)]\tLoss: 91.809155\n",
      "Train Epoch: 145 [2400/8040 (30%)]\tLoss: 92.327669\n",
      "Train Epoch: 145 [3600/8040 (45%)]\tLoss: 91.767993\n",
      "Train Epoch: 145 [4800/8040 (60%)]\tLoss: 92.539461\n",
      "Train Epoch: 145 [6000/8040 (75%)]\tLoss: 92.059245\n",
      "Train Epoch: 145 [7200/8040 (90%)]\tLoss: 91.914250\n",
      "====> Epoch: 145 Average loss: 92.1417\n",
      "epoch: 145, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 146 [0/8040 (0%)]\tLoss: 91.843913\n",
      "Train Epoch: 146 [1200/8040 (15%)]\tLoss: 92.110384\n",
      "Train Epoch: 146 [2400/8040 (30%)]\tLoss: 91.990804\n",
      "Train Epoch: 146 [3600/8040 (45%)]\tLoss: 92.206828\n",
      "Train Epoch: 146 [4800/8040 (60%)]\tLoss: 91.991300\n",
      "Train Epoch: 146 [6000/8040 (75%)]\tLoss: 92.105225\n",
      "Train Epoch: 146 [7200/8040 (90%)]\tLoss: 92.103979\n",
      "====> Epoch: 146 Average loss: 92.1209\n",
      "epoch: 146, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 147 [0/8040 (0%)]\tLoss: 92.079606\n",
      "Train Epoch: 147 [1200/8040 (15%)]\tLoss: 92.431340\n",
      "Train Epoch: 147 [2400/8040 (30%)]\tLoss: 92.445093\n",
      "Train Epoch: 147 [3600/8040 (45%)]\tLoss: 92.346118\n",
      "Train Epoch: 147 [4800/8040 (60%)]\tLoss: 92.238192\n",
      "Train Epoch: 147 [6000/8040 (75%)]\tLoss: 92.722323\n",
      "Train Epoch: 147 [7200/8040 (90%)]\tLoss: 92.493229\n",
      "====> Epoch: 147 Average loss: 92.1232\n",
      "epoch: 147, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 148 [0/8040 (0%)]\tLoss: 92.084928\n",
      "Train Epoch: 148 [1200/8040 (15%)]\tLoss: 91.478784\n",
      "Train Epoch: 148 [2400/8040 (30%)]\tLoss: 92.033822\n",
      "Train Epoch: 148 [3600/8040 (45%)]\tLoss: 92.260335\n",
      "Train Epoch: 148 [4800/8040 (60%)]\tLoss: 92.337378\n",
      "Train Epoch: 148 [6000/8040 (75%)]\tLoss: 92.486792\n",
      "Train Epoch: 148 [7200/8040 (90%)]\tLoss: 92.470085\n",
      "====> Epoch: 148 Average loss: 92.1387\n",
      "epoch: 148, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 149 [0/8040 (0%)]\tLoss: 92.119946\n",
      "Train Epoch: 149 [1200/8040 (15%)]\tLoss: 92.363411\n",
      "Train Epoch: 149 [2400/8040 (30%)]\tLoss: 92.329329\n",
      "Train Epoch: 149 [3600/8040 (45%)]\tLoss: 92.163851\n",
      "Train Epoch: 149 [4800/8040 (60%)]\tLoss: 91.433968\n",
      "Train Epoch: 149 [6000/8040 (75%)]\tLoss: 91.861271\n",
      "Train Epoch: 149 [7200/8040 (90%)]\tLoss: 92.960661\n",
      "====> Epoch: 149 Average loss: 92.1257\n",
      "epoch: 149, beta=0.01, frac_anom=0.1\n",
      "Train Epoch: 150 [0/8040 (0%)]\tLoss: 92.110758\n",
      "Train Epoch: 150 [1200/8040 (15%)]\tLoss: 91.958545\n",
      "Train Epoch: 150 [2400/8040 (30%)]\tLoss: 91.763664\n",
      "Train Epoch: 150 [3600/8040 (45%)]\tLoss: 92.015804\n",
      "Train Epoch: 150 [4800/8040 (60%)]\tLoss: 91.564323\n",
      "Train Epoch: 150 [6000/8040 (75%)]\tLoss: 91.997640\n",
      "Train Epoch: 150 [7200/8040 (90%)]\tLoss: 92.235832\n",
      "====> Epoch: 150 Average loss: 92.1330\n",
      "epoch: 150, beta=0.01, frac_anom=0.1\n",
      "====> Test set loss: 32.1261\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    brange=[0,0.01]\n",
    "    erange = range(1, epochs + 1)\n",
    "    anrange = np.array([0.01,0.05,0.1])\n",
    "    \n",
    "    test_loss_total = np.zeros((len(anrange), len(brange)))\n",
    "    test_loss_anom = np.zeros((len(anrange), len(brange)))\n",
    "    test_loss_normals = np.zeros((len(anrange), len(brange)))\n",
    "\n",
    "    for b, betaval in enumerate(brange):\n",
    "\n",
    "        for a, frac_anom in enumerate(anrange):\n",
    "            train_loader, test_loader = create_data(frac_anom)\n",
    "            model_reset()\n",
    "            for epoch in erange:\n",
    "\n",
    "                train(epoch, beta_val=betaval)\n",
    "\n",
    "                print('epoch: %d, beta=%g, frac_anom=%g' %\n",
    "                      (epoch, betaval, frac_anom))\n",
    "\n",
    "            # save the model\n",
    "            torch.save(model, '/results/fashion_mnist_beta_shallow_' + str(betaval) + '_frac_anom_' + str(frac_anom))\n",
    "\n",
    "            test_loss_total[a, b], test_loss_anom[a, b], test_loss_normals[\n",
    "                a, b] = test(frac_anom, beta_val=betaval)\n",
    "\n",
    "\n",
    "\n",
    "        np.savez('/results/test_loss_fashionmnist_beta_shallow' + str(b) + '.npz',\n",
    "                 test_loss_total=test_loss_total,\n",
    "                 test_loss_anom=test_loss_anom,\n",
    "                 test_loss_normals=test_loss_normals,\n",
    "                 brange=brange,\n",
    "                 anrange=anrange)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colors and legends for ROC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "\n",
    "FPRs = dict()\n",
    "TPRs = dict()\n",
    "AUC = dict()\n",
    "\n",
    "lgd = {\n",
    "    0: 'VAE-1%',\n",
    "    1: 'VAE-5%',\n",
    "    2: 'VAE-10%',\n",
    "    3: 'RVAE-1%',\n",
    "    4: 'RVAE-5%',\n",
    "    5: 'RVAE-10%'\n",
    "}\n",
    "colors = {0: 'r', 1: 'b', 2: 'k', 3: 'r', 4: 'b', 5: 'k'}\n",
    "lsty = {0: '--', 1: '--', 2: '--', 3: '-', 4: '-', 5: '-'}\n",
    "c = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate ROC and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for beta in brange:\n",
    "    for frac in anrange:\n",
    "  \n",
    "        if beta == 0:\n",
    "            beta_str = f\"{beta:.0f}\"\n",
    "        else:\n",
    "            beta_str = str(beta)\n",
    "\n",
    "        filename = cwd + '/results/fashion_mnist_' + beta_str + '_' + str(\n",
    "            frac) + '.npz'\n",
    "   \n",
    "\n",
    "        s = np.load(filename)\n",
    "\n",
    "        y = s['recon']\n",
    "        x = s['data']\n",
    "        L = s['anom_lab']\n",
    "\n",
    "        y = y.reshape(y.shape[0], -1)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        L = L.reshape(L.shape[0], -1)\n",
    "\n",
    "        mse = np.linalg.norm(x - y, 2, 1, True)\n",
    "        fpr, tpr, _ = roc_curve(L, mse)\n",
    "        auc = roc_auc_score(L,mse)\n",
    "\n",
    "        FPRs[c] = fpr\n",
    "        TPRs[c] = tpr\n",
    "        AUC[c] = auc\n",
    "\n",
    "        c = c + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2400x1800 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.92639122251842, 1: 0.8354053410989443, 2: 0.7802035181377807, 3: 0.9786737595810748, 4: 0.958488292100077, 5: 0.9514830914872521}\n"
     ]
    }
   ],
   "source": [
    "lw = 2\n",
    "fig = plt.figure(figsize=(8, 6), dpi=300)\n",
    "for c in np.arange(0, len(FPRs)):\n",
    "    fpr = FPRs[c]\n",
    "    tpr = TPRs[c]\n",
    "\n",
    "    plt.plot(fpr, tpr, lsty[c], color=colors[c], lw=lw, label=lgd[c])\n",
    "\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.5, 1.05])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig.savefig('results/ROCn_fashion_mnist.png')\n",
    "\n",
    "print(AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
